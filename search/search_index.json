{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Softjax","text":""},{"location":"#disclaimer","title":"Disclaimer","text":"<p>Softjax is not yet fully released! We are currently finalizing the library, and are planning on releasing it (alongside a similar \"Softtorch\" library) officially until the end of the year. If you somehow stumbled upon this library already, feel free to use and test the github code, and please reach out if you encounter any issues or have suggestions for improvement. Thanks!</p> <p>Note also that some of the API and internals are still subject to potentially bigger changes until the official release. The pip install will also only be available after official release.</p>"},{"location":"#in-a-nutshell","title":"In a nutshell","text":"<p>Softjax provides soft differentiable drop-in replacements for traditionally non-differentiable functions in JAX, including</p> <ul> <li>elementwise operators: <code>abs</code>, <code>relu</code>, <code>clip</code>, <code>sign</code>, <code>round</code> and <code>heaviside</code>;</li> <li>array-valued operators: <code>(arg)max</code>, <code>(arg)min</code>, <code>(arg)median</code>, <code>(arg)sort</code>, <code>(arg)top_k</code> and <code>ranking</code>;</li> <li>comparison operators such as: <code>greater</code>, <code>equal</code> or <code>isclose</code>;</li> <li>logical operators such as: <code>logical_and</code>, <code>all</code> or <code>where</code>;</li> <li>selection operators such as: <code>where</code>, <code>take_along_axis</code>, <code>dynamic_index_in_dim</code> or <code>choose</code>.</li> </ul> <p>All operators offer multiple modes and adjustable strength of softening, allowing for e.g. smoothness of the soft function or boundedness of the softened region, depending on the user needs.</p> <p>Moreover, we tightly integrate functionality for deploying functions using straight-through-estimation, where we use non-differentiable functions in the forward pass and their differentiable replacements in the backward pass.</p> <p>The Softjax library is designed to require minimal user effort, by simply replacing the non-differentiable JAX function with the Softjax counterparts. However, keep in mind that special care needs to be taken when using functions operating on indices, as we relax the notion of an index into a distribution over indices, thereby modifying the shape of returned/accepted values.</p>"},{"location":"#installation","title":"Installation","text":"<p>Requires Python 3.10+. <pre><code>pip install softjax\n</code></pre></p>"},{"location":"#quick-example","title":"Quick example","text":"<p><pre><code>import jax\nimport jax.numpy as jnp\nimport softjax as sj\n\nx = jnp.array([-0.2, -1.0, 0.3, 1.0])\n\n# Elementwise operators\nprint(\"\\nJAX absolute:\", jnp.abs(x))\nprint(\"SoftJAX absolute (hard mode):\", sj.abs(x, mode=\"hard\"))\nprint(\"SoftJAX absolute (soft mode):\", sj.abs(x))\n\nprint(\"\\nJAX clip:\", jnp.clip(x, -0.5, 0.5))\nprint(\"SoftJAX clip (hard mode):\", sj.clip(x, -0.5, 0.5, mode=\"hard\"))\nprint(\"SoftJAX clip (soft mode):\", sj.clip(x, -0.5, 0.5))\n\nprint(\"\\nJAX heaviside:\", jnp.heaviside(x, 0.5))\nprint(\"SoftJAX heaviside (hard mode):\", sj.heaviside(x, mode=\"hard\"))\nprint(\"SoftJAX heaviside (soft mode):\", sj.heaviside(x))\n\nprint(\"\\nJAX ReLU:\", jax.nn.relu(x))\nprint(\"SoftJAX ReLU (hard mode):\", sj.relu(x, mode=\"hard\"))\nprint(\"SoftJAX ReLU (soft mode):\", sj.relu(x))\n\nprint(\"\\nJAX round:\", jnp.round(x))\nprint(\"SoftJAX round (hard mode):\", sj.round(x, mode=\"hard\"))\nprint(\"SoftJAX round (soft mode):\", sj.round(x))\n\nprint(\"\\nJAX sign:\", jnp.sign(x))\nprint(\"SoftJAX sign (hard mode):\", sj.sign(x, mode=\"hard\"))\nprint(\"SoftJAX sign (soft mode):\", sj.sign(x))\n</code></pre> <pre><code>JAX absolute: [0.2 1.  0.3 1. ]\nSoftJAX absolute (hard mode): [0.2 1.  0.3 1. ]\nSoftJAX absolute (soft mode): [0.15231883 0.9999092  0.27154448 0.9999092 ]\n\nJAX clip: [-0.2 -0.5  0.3  0.5]\nSoftJAX clip (hard mode): [-0.2 -0.5  0.3  0.5]\nSoftJAX clip (soft mode): [-0.19523241 -0.4993285   0.28734074  0.4993285 ]\n\nJAX heaviside: [0. 0. 1. 1.]\nSoftJAX heaviside (hard mode): [0. 0. 1. 1.]\nSoftJAX heaviside (soft mode): [1.19202922e-01 4.53978687e-05 9.52574127e-01 9.99954602e-01]\n\nJAX ReLU: [0.  0.  0.3 1. ]\nSoftJAX ReLU (hard mode): [0.  0.  0.3 1. ]\nSoftJAX ReLU (soft mode): [1.26928011e-02 4.53988992e-06 3.04858735e-01 1.00000454e+00]\n\nJAX round: [-0. -1.  0.  1.]\nSoftJAX round (hard mode): [-0. -1.  0.  1.]\nSoftJAX round (soft mode): [-0.04651704 -1.          0.1188737   1.        ]\n\nJAX sign: [-1. -1.  1.  1.]\nSoftJAX sign (hard mode): [-1. -1.  1.  1.]\nSoftJAX sign (soft mode): [-0.76159416 -0.9999092   0.90514825  0.9999092 ]\n</code></pre></p> <p><pre><code># Array-valued operators\nprint(\"\\nJAX max:\", jnp.max(x))\nprint(\"SoftJAX max (hard mode):\", sj.max(x, mode=\"hard\"))\nprint(\"SoftJAX max (soft mode):\", sj.max(x))\n\nprint(\"\\nJAX min:\", jnp.min(x))\nprint(\"SoftJAX min (hard mode):\", sj.min(x, mode=\"hard\"))\nprint(\"SoftJAX min (soft mode):\", sj.min(x))\n\nprint(\"\\nJAX sort:\", jnp.sort(x))\nprint(\"SoftJAX sort (hard mode):\", sj.sort(x, mode=\"hard\"))\nprint(\"SoftJAX sort (soft mode):\", sj.sort(x))\n\nprint(\"\\nJAX median:\", jnp.median(x))\nprint(\"SoftJAX median (hard mode):\", sj.median(x, mode=\"hard\"))\nprint(\"SoftJAX median (soft mode):\", sj.median(x))\n\nprint(\"\\nJAX top_k:\", jax.lax.top_k(x, k=3)[0])\nprint(\"SoftJAX top_k (hard mode):\", sj.top_k(x, k=3, mode=\"hard\")[0])\nprint(\"SoftJAX top_k (soft mode):\", sj.top_k(x, k=3)[0])\n\nprint(\"\\nJAX ranking:\", jnp.argsort(jnp.argsort(x)))\nprint(\"SoftJAX ranking (hard mode):\", sj.ranking(x, mode=\"hard\", descending=False))\nprint(\"SoftJAX ranking (soft mode):\", sj.ranking(x, descending=False))\n</code></pre> <pre><code>JAX max: 1.0\nSoftJAX max (hard mode): 1.0\nSoftJAX max (soft mode): 0.9993548976691374\n\nJAX min: -1.0\nSoftJAX min (hard mode): -1.0\nSoftJAX min (soft mode): -0.9997287789452775\n\nJAX sort: [-1.  -0.2  0.3  1. ]\nSoftJAX sort (hard mode): [-1.  -0.2  0.3  1. ]\nSoftJAX sort (soft mode): [-0.99972878 -0.19691387  0.29728716  0.9993549 ]\n\nJAX median: 0.04999999999999999\nSoftJAX median (hard mode): 0.04999999999999999\nSoftJAX median (soft mode): 0.05000033589501627\n\nJAX top_k: [ 1.   0.3 -0.2]\nSoftJAX top_k (hard mode): [ 1.   0.3 -0.2]\nSoftJAX top_k (soft mode): [ 0.9993549   0.29728716 -0.19691387]\n\nJAX ranking: [1 0 2 3]\nSoftJAX ranking (hard mode): [1. 0. 2. 3.]\nSoftJAX ranking (soft mode): [1.00636968e+00 3.39874686e-04 1.99421369e+00 2.99907667e+00]\n</code></pre></p> <p><pre><code># Operators returning indices\nprint(\"\\nJAX argmax:\", jnp.argmax(x))\nprint(\"SoftJAX argmax (hard mode):\", sj.argmax(x, mode=\"hard\"))\nprint(\"SoftJAX argmax (soft mode):\", sj.argmax(x))\n\nprint(\"\\nJAX argmin:\", jnp.argmin(x))\nprint(\"SoftJAX argmin (hard mode):\", sj.argmin(x, mode=\"hard\"))\nprint(\"SoftJAX argmin (soft mode):\", sj.argmin(x))\n\nprint(\"\\nJAX argmedian:\", \"Not implemented in standard JAX\")\nprint(\"SoftJAX argmedian (hard mode):\", sj.argmedian(x, mode=\"hard\"))\nprint(\"SoftJAX argmedian (soft mode):\", sj.argmedian(x))\n\nprint(\"\\nJAX argsort:\", jnp.argsort(x))\nprint(\"SoftJAX argsort (hard mode):\", sj.argsort(x, mode=\"hard\"))\nprint(\"SoftJAX argsort (soft mode):\", sj.argsort(x))\n\nprint(\"\\nJAX argtop_k:\", jax.lax.top_k(x, k=3)[1])\nprint(\"SoftJAX argtop_k (hard mode):\", sj.top_k(x, k=3, mode=\"hard\")[1])\nprint(\"SoftJAX argtop_k (soft mode):\", sj.top_k(x, k=3)[1])\n</code></pre> <pre><code>JAX argmax: 3\nSoftJAX argmax (hard mode): [0. 0. 0. 1.]\nSoftJAX argmax (soft mode): [6.13857697e-06 2.05926316e-09 9.11045600e-04 9.99082814e-01]\n\nJAX argmin: 1\nSoftJAX argmin (hard mode): [0. 1. 0. 0.]\nSoftJAX argmin (soft mode): [3.35349372e-04 9.99662389e-01 2.25956629e-06 2.06045775e-09]\n\nJAX argmedian: Not implemented in standard JAX\nSoftJAX argmedian (hard mode): [0.5 0.  0.5 0. ]\nSoftJAX argmedian (soft mode): [4.99999764e-01 5.62675608e-08 4.99999764e-01 4.15764163e-07]\n\nJAX argsort: [1 0 2 3]\nSoftJAX argsort (hard mode): [[0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]]\nSoftJAX argsort (soft mode): [[3.35349372e-04 9.99662389e-01 2.25956629e-06 2.06045775e-09]\n [9.92970214e-01 3.33104397e-04 6.69058067e-03 6.10101985e-06]\n [6.68677917e-03 2.24316451e-06 9.92406021e-01 9.04957153e-04]\n [6.13857697e-06 2.05926316e-09 9.11045600e-04 9.99082814e-01]]\n\nJAX argtop_k: [3 2 0]\nSoftJAX argtop_k (hard mode): [[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]]\nSoftJAX argtop_k (soft mode): [[6.13857697e-06 2.05926316e-09 9.11045600e-04 9.99082814e-01]\n [6.68677917e-03 2.24316451e-06 9.92406021e-01 9.04957153e-04]\n [9.92970214e-01 3.33104397e-04 6.69058067e-03 6.10101985e-06]]\n</code></pre></p> <p><pre><code>y = jnp.array([0.2, -0.5, 0.5, -1.0])\n\n# Comparison operators\nprint(\"\\nJAX greater:\", jnp.greater(x, y))\nprint(\"SoftJAX greater (hard mode):\", sj.greater(x, y, mode=\"hard\"))\nprint(\"SoftJAX greater (soft mode):\", sj.greater(x, y))\n\nprint(\"\\nJAX greater equal:\", jnp.greater_equal(x, y))\nprint(\"SoftJAX greater equal (hard mode):\", sj.greater_equal(x, y, mode=\"hard\"))\nprint(\"SoftJAX greater equal (soft mode):\", sj.greater_equal(x, y))\n\nprint(\"\\nJAX less:\", jnp.less(x, y))\nprint(\"SoftJAX less (hard mode):\", sj.less(x, y, mode=\"hard\"))\nprint(\"SoftJAX less (soft mode):\", sj.less(x, y))\n\nprint(\"\\nJAX less equal:\", jnp.less_equal(x, y))\nprint(\"SoftJAX less equal (hard mode):\", sj.less_equal(x, y, mode=\"hard\"))\nprint(\"SoftJAX less equal (soft mode):\", sj.less_equal(x, y))\n\nprint(\"\\nJAX equal:\", jnp.equal(x, y))\nprint(\"SoftJAX equal (hard mode):\", sj.equal(x, y, mode=\"hard\"))\nprint(\"SoftJAX equal (soft mode):\", sj.equal(x, y))\n\nprint(\"\\nJAX not equal:\", jnp.not_equal(x, y))\nprint(\"SoftJAX not equal (hard mode):\", sj.not_equal(x, y, mode=\"hard\"))\nprint(\"SoftJAX not equal (soft mode):\", sj.not_equal(x, y))\n\nprint(\"\\nJAX isclose:\", jnp.isclose(x, y))\nprint(\"SoftJAX isclose (hard mode):\", sj.isclose(x, y, mode=\"hard\"))\nprint(\"SoftJAX isclose (soft mode):\", sj.isclose(x, y))\n</code></pre> <pre><code>JAX greater: [False False False  True]\nSoftJAX greater (hard mode): [0. 0. 0. 1.]\nSoftJAX greater (soft mode): [0.01798621 0.00669285 0.11920292 1.        ]\n\nJAX greater equal: [False False False  True]\nSoftJAX greater equal (hard mode): [0. 0. 0. 1.]\nSoftJAX greater equal (soft mode): [0.01798621 0.00669285 0.11920292 1.        ]\n\nJAX less: [ True  True  True False]\nSoftJAX less (hard mode): [1. 1. 1. 0.]\nSoftJAX less (soft mode): [9.82013790e-01 9.93307149e-01 8.80797078e-01 2.06115369e-09]\n\nJAX less equal: [ True  True  True False]\nSoftJAX less equal (hard mode): [1. 1. 1. 0.]\nSoftJAX less equal (soft mode): [9.82013790e-01 9.93307149e-01 8.80797078e-01 2.06115369e-09]\n\nJAX equal: [False False False False]\nSoftJAX equal (hard mode): [0. 0. 0. 0.]\nSoftJAX equal (soft mode): [1.79862100e-02 6.69285093e-03 1.19202922e-01 2.06115369e-09]\n\nJAX not equal: [ True  True  True  True]\nSoftJAX not equal (hard mode): [1. 1. 1. 1.]\nSoftJAX not equal (soft mode): [0.98201379 0.99330715 0.88079708 1.        ]\n\nJAX isclose: [False False False False]\nSoftJAX isclose (hard mode): [0. 0. 0. 0.]\nSoftJAX isclose (soft mode): [1.79865650e-02 6.69318401e-03 1.19208182e-01 2.06135997e-09]\n</code></pre></p> <p><pre><code># Logical operators\nfuzzy_a = jnp.array([0.1, 0.2, 0.8, 1.0])\nfuzzy_b = jnp.array([0.7, 0.3, 0.1, 0.9])\nbool_a = fuzzy_a &gt;= 0.5\nbool_b = fuzzy_b &gt;= 0.5\n\nprint(\"\\nJAX AND:\", jnp.logical_and(bool_a, bool_b))\nprint(\"SoftJAX AND:\", sj.logical_and(fuzzy_a, fuzzy_b))\n\nprint(\"\\nJAX OR:\", jnp.logical_or(bool_a, bool_b))\nprint(\"SoftJAX OR:\", sj.logical_or(fuzzy_a, fuzzy_b))\n\nprint(\"\\nJAX NOT:\", jnp.logical_not(bool_a))\nprint(\"SoftJAX NOT:\", sj.logical_not(fuzzy_a))\n\nprint(\"\\nJAX XOR:\", jnp.logical_xor(bool_a, bool_b))\nprint(\"SoftJAX XOR:\", sj.logical_xor(fuzzy_a, fuzzy_b))\n\nprint(\"\\nJAX ALL:\", jnp.all(bool_a))\nprint(\"SoftJAX ALL:\", sj.all(fuzzy_a))\n\nprint(\"\\nJAX ANY:\", jnp.any(bool_a))\nprint(\"SoftJAX ANY:\", sj.any(fuzzy_a))\n\n# Selection operators\nprint(\"\\nJAX Where:\", jnp.where(bool_a, x, y))\nprint(\"SoftJAX Where:\", sj.where(fuzzy_a, x, y))\n</code></pre> <pre><code>JAX AND: [False False False  True]\nSoftJAX AND: [0.26457513 0.24494897 0.28284271 0.9486833 ]\n\nJAX OR: [ True False  True  True]\nSoftJAX OR: [0.48038476 0.25166852 0.57573593 0.99999684]\n\nJAX NOT: [ True  True False False]\nSoftJAX NOT: [0.9 0.8 0.2 0. ]\n\nJAX XOR: [ True False  True False]\nSoftJAX XOR: [0.58702688 0.43498731 0.63937484 0.17309871]\n\nJAX ALL: False\nSoftJAX ALL: 0.35565588200778464\n\nJAX ANY: True\nSoftJAX ANY: 0.9980519925071494\n\nJAX Where: [ 0.2 -0.5  0.3  1. ]\nSoftJAX Where: [ 0.16 -0.6   0.34  1.  ]\n</code></pre></p> <p><pre><code># Straight-through operators: Use hard function on forward and soft on backward\nprint(\"Straight-through ReLU:\", sj.relu_st(x))\nprint(\"Straight-through sort:\", sj.sort_st(x))\nprint(\"Straight-through argtop_k:\", sj.top_k_st(x, k=3)[1])\nprint(\"Straight-through greater:\", sj.greater_st(x, y))\n# And many more...\n</code></pre> <pre><code>Straight-through ReLU: [0.  0.  0.3 1. ]\nStraight-through sort: [-1.  -0.2  0.3  1. ]\nStraight-through argtop_k: [[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]]\nStraight-through greater: [0. 0. 0. 1.]\n</code></pre></p> <p>The outputs were generated in this notebook.</p>"},{"location":"#citation","title":"Citation","text":"<p>If this library helped your academic work, please consider citing:</p> <pre><code>@misc{Softjax2025,\n  author = {Paulus, Anselm and Geist, Ren\\'e and Martius, Georg},\n  title = {Softjax},\n  year = {2025},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/a-paulus/softjax}}\n}\n</code></pre> <p>Also consider starring the project on GitHub!</p> <p>Special thanks and credit go to Patrick Kidger for the awesome JAX repositories that served as the basis for the documentation of this project.</p>"},{"location":"#next-steps","title":"Next steps","text":"<p>Have a look at the All of Softjax page.</p>"},{"location":"#feedback","title":"Feedback","text":"<p>This project is still relatively young, if you have any suggestions for improvement or other feedback, please reach out or raise a GitHub issue!</p>"},{"location":"#see-also","title":"See also","text":""},{"location":"#other-libraries-in-the-jax-ecosystem","title":"Other libraries in the JAX ecosystem","text":"<p>Always useful Equinox: neural networks and everything not already in core JAX! jaxtyping: type annotations for shape/dtype of arrays.  </p> <p>Deep learning Optax: first-order gradient (SGD, Adam, ...) optimisers. Orbax: checkpointing (async/multi-host/multi-device). Levanter: scalable+reliable training of foundation models (e.g. LLMs). paramax: parameterizations and constraints for PyTrees.  </p> <p>Scientific computing Optax: first-order gradient (SGD, Adam, ...) optimisers. Optimistix: root finding, minimisation, fixed points, and least squares. Lineax: linear solvers. BlackJAX: probabilistic+Bayesian sampling. sympy2jax: SymPy&lt;-&gt;JAX conversion; train symbolic expressions via gradient descent. PySR: symbolic regression. (Non-JAX honourable mention!)  </p> <p>Awesome JAX Awesome JAX: a longer list of other JAX projects.  </p>"},{"location":"#other-libraries-on-differentiable-programming","title":"Other libraries on differentiable programming","text":"<p>Differentiable sorting, top-k and ranking DiffSort: Differentiable sorting networks in PyTorch. DiffTopK: Differentiable top-k in PyTorch. FastSoftSort: Fast differentiable sorting and ranking in JAX. Differentiable Top-k with Optimal Transport in JAX. SoftSort: Differentiable argsort in PyTorch and TensorFlow.  </p> <p>Other DiffLogic: Differentiable logic gate networks in PyTorch. SmoothOT: Smooth and Sparse Optimal Transport. JaxOpt: Differentiable optimization in JAX.  </p>"},{"location":"#papers-on-differentiable-algorithms","title":"Papers on differentiable algorithms","text":"<p>Softjax builds on / implements various different algoithms for e.g. differentiable <code>argtop_k</code>, <code>sorting</code> and <code>ranking</code>, including:</p> <p>Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application Fast Differentiable Sorting and Ranking. Differentiable Ranks and Sorting using Optimal Transport Differentiable Top-k with Optimal Transport SoftSort: A Continuous Relaxation for the argsort Operator Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances Smooth and Sparse Optimal Transport Smooth Approximations of the Rounding Function </p> <p>Please check the API Documentation for implementation details.</p>"},{"location":"all-of-softjax/","title":"All of Softjax","text":"In\u00a0[2]: Copied! <pre>import softjax as sj\n\nplot(sj.relu, modes=[\"entropic\", \"gated_quintic\"])\n</pre> import softjax as sj  plot(sj.relu, modes=[\"entropic\", \"gated_quintic\"]) <p>SoftJAX provides function surrogates for many other function such as the absolute function.</p> In\u00a0[3]: Copied! <pre>plot(sj.abs, modes=[\"entropic\", \"quintic\"])\n</pre> plot(sj.abs, modes=[\"entropic\", \"quintic\"]) In\u00a0[4]: Copied! <pre>plot(sj.round, modes=[\"entropic\", \"quintic\"])\n</pre> plot(sj.round, modes=[\"entropic\", \"quintic\"]) In\u00a0[5]: Copied! <pre>soft_relu = sj.st(sj.relu)\n\nx = jnp.arange(-1, 1, 0.01)\nvalues, grads = jax.vmap(jax.value_and_grad(soft_relu))(x)\nplot_value_and_grad(x, values, grads, label_func=\"ReLU\", label_grad=\"Soft gradient\")\n</pre> soft_relu = sj.st(sj.relu)  x = jnp.arange(-1, 1, 0.01) values, grads = jax.vmap(jax.value_and_grad(soft_relu))(x) plot_value_and_grad(x, values, grads, label_func=\"ReLU\", label_grad=\"Soft gradient\") In\u00a0[6]: Copied! <pre>def relu_prod(x, y):\n    # Standard ReLU product, no softening\n    return jax.nn.relu(x) * jax.nn.relu(y)\n\n\nplot_value_grad_2D(relu_prod)\n</pre> def relu_prod(x, y):     # Standard ReLU product, no softening     return jax.nn.relu(x) * jax.nn.relu(y)   plot_value_grad_2D(relu_prod) <p>A naive approach would be to replace each relu with <code>sj.relu_st</code> independently. However, the resulting function will not provide informative gradients for every input. This is due to the chain rule, in which the gradient flowing through one relu is multiplid by the (forward) output of the other relu. As the forward pass is not smoothed, the gradient will sometimes be multiplied by zero, resulting in no informative gradient.</p> In\u00a0[7]: Copied! <pre>def soft_relu_prod_naive(x, y, mode=\"entropic\", softness=1.0):\n    # Naive straight-through implementation\n    return sj.relu_st(x, mode=mode, softness=softness) * sj.relu_st(\n        y, mode=mode, softness=softness\n    )\n\n\nplot_value_grad_2D(soft_relu_prod_naive)\n</pre> def soft_relu_prod_naive(x, y, mode=\"entropic\", softness=1.0):     # Naive straight-through implementation     return sj.relu_st(x, mode=mode, softness=softness) * sj.relu_st(         y, mode=mode, softness=softness     )   plot_value_grad_2D(soft_relu_prod_naive) <p>An alternative approach for softening this function is to apply the straight through trick on the outer level as illustrated below. When applied on the outer level, the forward pass computes the hard product of ReLUs as before, whereas the backward pass differentiates through the product of smooth relus.</p> <p>Notice that we use sj.relu instead of sj.relu_st here.</p> In\u00a0[8]: Copied! <pre>@sj.st\ndef soft_relu_prod_custom_st(x, y, mode=\"entropic\", softness=1.0):\n    # Custom straight-through implementation\n    return sj.relu(x, mode=mode, softness=softness) * sj.relu(\n        y, mode=mode, softness=softness\n    )\n\n\nplot_value_grad_2D(soft_relu_prod_custom_st)\n</pre> @sj.st def soft_relu_prod_custom_st(x, y, mode=\"entropic\", softness=1.0):     # Custom straight-through implementation     return sj.relu(x, mode=mode, softness=softness) * sj.relu(         y, mode=mode, softness=softness     )   plot_value_grad_2D(soft_relu_prod_custom_st) <p>We observe that as expected, this version of the function now also produces informative gradients in the third quadrant. In the simple above example, the only both <code>sj.relu</code> functions take the same parameters, therefore it was easy to just apply the <code>sj.st</code> decorator again.</p> <p>In general, we might want to define custom behavior. This can be implemented by using the <code>@sj.grad_replace</code> decorator, which allows custom control flow conditioned on a <code>forward</code> boolean variable. The function will then execute with <code>forward=True</code> on the forward pass and <code>forward=False</code> on the backward pass.</p> <pre>def grad_replace(fn: Callable) -&gt; Callable:\n    def wrapped(*args, **kwargs):\n        fw_y = fn(*args, **kwargs, forward=True)\n        bw_y = fn(*args, **kwargs, forward=False)\n        return jtu.tree_map(lambda fw, bw: jax.lax.stop_gradient(fw - bw) + bw, fw_y, bw_y)\n    return wrapped\n</pre> In\u00a0[9]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(2, 10))\nbool_array = jax.numpy.greater(x, 0.5)\n\n\ndef boolean_loss(x):\n    return jax.numpy.greater(x, 0.5).sum().astype(\"float32\")\n\n\nboolean_grads = jax.grad(boolean_loss)(x)\n\nplot_array(x, title=\"x\")\nplot_array(bool_array, title=\"jax.numpy.greater(x, 0.5)\")\nplot_array(boolean_grads, title=\"jax.grad(jax.numpy.greater(x, 0.5).sum())(x)\")\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(2, 10)) bool_array = jax.numpy.greater(x, 0.5)   def boolean_loss(x):     return jax.numpy.greater(x, 0.5).sum().astype(\"float32\")   boolean_grads = jax.grad(boolean_loss)(x)  plot_array(x, title=\"x\") plot_array(bool_array, title=\"jax.numpy.greater(x, 0.5)\") plot_array(boolean_grads, title=\"jax.grad(jax.numpy.greater(x, 0.5).sum())(x)\") <p>Example - <code>softjax.greater_st</code> yields useful gradients: Instead of <code>jax.numpy.greater</code>, let's use <code>softjax.greater_st</code> (straight_through variant of <code>softjax.greater</code>). As shown below, thanks to straight-through estimation, <code>softjax.greater_st</code> yields exact Booleans while the gradient of the Boolean loss points in informative directions.</p> In\u00a0[10]: Copied! <pre>def soft_boolean_loss(x):\n    return sj.greater_st(x, 0.5).sum()\n\n\nx = jax.random.uniform(jax.random.key(0), shape=(2, 10))\nbool_array = sj.greater_st(x, 0.5)\nboolean_grads = jax.grad(soft_boolean_loss)(x)\n\nplot_array(x, title=\"x\")\nplot_array(bool_array, title=\"soft_greater_st(x, 0.5)\")\nplot_array(boolean_grads, title=\"jax.grad(soft_greater_st(x, 0.5).sum())(x)\")\n</pre> def soft_boolean_loss(x):     return sj.greater_st(x, 0.5).sum()   x = jax.random.uniform(jax.random.key(0), shape=(2, 10)) bool_array = sj.greater_st(x, 0.5) boolean_grads = jax.grad(soft_boolean_loss)(x)  plot_array(x, title=\"x\") plot_array(bool_array, title=\"soft_greater_st(x, 0.5)\") plot_array(boolean_grads, title=\"jax.grad(soft_greater_st(x, 0.5).sum())(x)\") In\u00a0[11]: Copied! <pre>plot(sj.heaviside, modes=[\"entropic\", \"euclidean\", \"quintic\"])\n</pre> plot(sj.heaviside, modes=[\"entropic\", \"euclidean\", \"quintic\"]) <p>In the above case, the <code>linear</code> and <code>quintic</code> relaxations have the advantage of altering the original function only in a bounded region, a property that can be desireble in some cases.</p> <p>Given the concept of a <code>SoftBool</code>, a probabilistic surrogate for binary logical operations such as <code>jax.numpy.equal</code> and <code>jax.numpy.greater</code> is obtained by simply shifting the sigmoid.</p> <p>Example - Greater operator: <code>sj.greater(x,y)</code> corresponds to shifting <code>sj.heaviside</code> by <code>y</code> to the right. The output can be interpreted as the probability $P(x \\geq y)\\in[0,1]$ with $x\\in\\mathbb{R}$ and $y\\in\\mathbb{R}$.</p> In\u00a0[12]: Copied! <pre>def greater_than_1(x, mode=\"entropic\", softness=1.0):\n    return sj.greater(x, y=jnp.array(1.0), mode=mode, softness=softness)\n\n\nplot(greater_than_1, modes=[\"entropic\", \"quintic\"])\n</pre> def greater_than_1(x, mode=\"entropic\", softness=1.0):     return sj.greater(x, y=jnp.array(1.0), mode=mode, softness=softness)   plot(greater_than_1, modes=[\"entropic\", \"quintic\"]) In\u00a0[13]: Copied! <pre>def not_greater_st(x):\n    return sj.logical_not(sj.greater_st(x, y=0.5, mode=\"entropic\", softness=1.0))\n\n\nx = jnp.arange(-1, 1, 0.01)\nvalues, grads = jax.vmap(jax.value_and_grad(not_greater_st))(x)\nplot_value_and_grad(x, values, grads, label_func=\"not_greater_st\")\n</pre> def not_greater_st(x):     return sj.logical_not(sj.greater_st(x, y=0.5, mode=\"entropic\", softness=1.0))   x = jnp.arange(-1, 1, 0.01) values, grads = jax.vmap(jax.value_and_grad(not_greater_st))(x) plot_value_and_grad(x, values, grads, label_func=\"not_greater_st\") <p>Example - Logical AND: Given two <code>SoftBools</code> $P(A)$ and $P(B)$, the probability that both independent events occur is $P(A \\wedge B) = P(A) \\cdot P(B)$.</p> <pre>def logical_and(x: SoftBool, y: SoftBool) -&gt; SoftBool:\n    return x * y\n</pre> In\u00a0[14]: Copied! <pre>plot_softbool_operation(sj.logical_and)\n</pre> plot_softbool_operation(sj.logical_and) <p>Example - Logical XOR: Softjax computes other soft logic operators such as <code>sj.logical_xor</code> by combining <code>sj.logical_not</code> and <code>sj.logical_and</code>.</p> <pre>def sj.logical_xor(x: SoftBool, y: SoftBool) -&gt; SoftBool:\n    return logical_or(logical_and(x, logical_not(y)), logical_and(logical_not(x), y))\n</pre> In\u00a0[15]: Copied! <pre>plot_softbool_operation(sj.logical_xor)\n</pre> plot_softbool_operation(sj.logical_xor) In\u00a0[16]: Copied! <pre>greater = lambda x, y: sj.greater(x, y, mode=\"entropic\", softness=1.0)\nsoft_where = lambda x, y: sj.where(greater(x, y), x, y)\n\nx = jax.random.uniform(jax.random.key(0), shape=(2, 10))\ny = jax.random.uniform(jax.random.key(1), shape=(2, 10))\n\nplot_array(x, title=\"x\")\nplot_array(y, title=\"y\")\nplot_array(soft_where(x, y), title=\"soft_where(x&gt;y, x, y)\")\n</pre> greater = lambda x, y: sj.greater(x, y, mode=\"entropic\", softness=1.0) soft_where = lambda x, y: sj.where(greater(x, y), x, y)  x = jax.random.uniform(jax.random.key(0), shape=(2, 10)) y = jax.random.uniform(jax.random.key(1), shape=(2, 10))  plot_array(x, title=\"x\") plot_array(y, title=\"y\") plot_array(soft_where(x, y), title=\"soft_where(x&gt;y, x, y)\") In\u00a0[17]: Copied! <pre>x = jnp.array([1, 2, 3])\nprint(\"jnp.argmax(x):\", jnp.argmax(x))\n</pre> x = jnp.array([1, 2, 3]) print(\"jnp.argmax(x):\", jnp.argmax(x)) <pre>jnp.argmax(x): 2\n</pre> <p>In comparison, SoftJAX computes a <code>SoftIndex</code> array. Each entry of a <code>SoftIndex</code> array contains the probability that the index is being selected.</p> In\u00a0[18]: Copied! <pre>x = jnp.array([1, 2, 3])\nprint(\"sj.argmax(x):\", sj.argmax(x))\n</pre> x = jnp.array([1, 2, 3]) print(\"sj.argmax(x):\", sj.argmax(x)) <pre>sj.argmax(x): [2.06106005e-09 4.53978686e-05 9.99954600e-01]\n</pre> <p>Example - Softmax: The \"softmax\" (or more precisely \"softargmax\") is a commonly used  differentiable surrogate for the <code>argmax</code> function (it is also the default softening mode in <code>sj.argmax</code>). The $\\text{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j\\exp(x_j)}$ returns a discrete probability distribution over indices (aka a <code>SoftIndex</code>). As shown in the plots below, the softmax is fully differentiable. It is commonly used for multi-class classification and in transformer networks.</p> <p>When <code>softness</code> is low, <code>sj.argmax</code> concentrates probability on the true maximum index (e.g., <code>[1.0, 0.0, 0.0]</code>), recovering the hard maximum. When <code>softness</code> is higher, the result smoothly interpolates between values, providing useful gradients for optimization.</p> In\u00a0[19]: Copied! <pre>def cross_entropy(x, class_target=5):\n    probs = sj.argmax(x, softness=10.0)\n    target_one_hot = jax.nn.one_hot(class_target, num_classes=x.shape[0])\n    log_probs = jnp.log(probs)\n    return -(target_one_hot * log_probs).mean()\n\n\nx = jax.random.normal(jax.random.key(0), shape=(10,))\nprobs = sj.argmax(x, softness=10.0)\nlossgrads = jax.grad(cross_entropy)(x)\n\nplot_softindices_1D(x, title=\"logits\")\nplot_softindices_1D(probs, title=\"index probabilities (softmax)\")\nplot_softindices_1D(lossgrads, title=\"gradients of cross entropy loss\")\n</pre> def cross_entropy(x, class_target=5):     probs = sj.argmax(x, softness=10.0)     target_one_hot = jax.nn.one_hot(class_target, num_classes=x.shape[0])     log_probs = jnp.log(probs)     return -(target_one_hot * log_probs).mean()   x = jax.random.normal(jax.random.key(0), shape=(10,)) probs = sj.argmax(x, softness=10.0) lossgrads = jax.grad(cross_entropy)(x)  plot_softindices_1D(x, title=\"logits\") plot_softindices_1D(probs, title=\"index probabilities (softmax)\") plot_softindices_1D(lossgrads, title=\"gradients of cross entropy loss\") <p>Note that while in a conventional array of indices, the index information is stored in the integer values, a <code>SoftIndex</code> stores the probabilities over possible indices in an extra dimension. By convention, we always put this additional dimension into the final axis. Except for this additional final dimension, the shape of the returned soft index matches that of the indices returned by standard JAX. Here are a few examples of this:</p> In\u00a0[20]: Copied! <pre>x = jnp.arange(12).reshape((3, 4))\nprint(\"x.shape:\", x.shape)\nprint(\"jnp.argmax(x, axis=1).shape:\", jnp.argmax(x, axis=1).shape)\nprint(\"sj.argmax(x, axis=1).shape:\", sj.argmax(x, axis=1).shape)\nprint(\"jnp.argmax(x, axis=0).shape:\", jnp.argmax(x, axis=0).shape)\nprint(\"sj.argmax(x, axis=0).shape:\", sj.argmax(x, axis=0).shape)\nprint(\n    \"jnp.argmax(x, axis=1, keepdims=True).shape:\",\n    jnp.argmax(x, axis=1, keepdims=True).shape,\n)\nprint(\n    \"sj.argmax(x, axis=1, keepdims=True).shape:\",\n    sj.argmax(x, axis=1, keepdims=True).shape,\n)\nprint(\n    \"jnp.argmax(x, axis=0, keepdims=True).shape:\",\n    jnp.argmax(x, axis=0, keepdims=True).shape,\n)\nprint(\n    \"sj.argmax(x, axis=0, keepdims=True).shape:\",\n    sj.argmax(x, axis=0, keepdims=True).shape,\n)\n</pre> x = jnp.arange(12).reshape((3, 4)) print(\"x.shape:\", x.shape) print(\"jnp.argmax(x, axis=1).shape:\", jnp.argmax(x, axis=1).shape) print(\"sj.argmax(x, axis=1).shape:\", sj.argmax(x, axis=1).shape) print(\"jnp.argmax(x, axis=0).shape:\", jnp.argmax(x, axis=0).shape) print(\"sj.argmax(x, axis=0).shape:\", sj.argmax(x, axis=0).shape) print(     \"jnp.argmax(x, axis=1, keepdims=True).shape:\",     jnp.argmax(x, axis=1, keepdims=True).shape, ) print(     \"sj.argmax(x, axis=1, keepdims=True).shape:\",     sj.argmax(x, axis=1, keepdims=True).shape, ) print(     \"jnp.argmax(x, axis=0, keepdims=True).shape:\",     jnp.argmax(x, axis=0, keepdims=True).shape, ) print(     \"sj.argmax(x, axis=0, keepdims=True).shape:\",     sj.argmax(x, axis=0, keepdims=True).shape, ) <pre>x.shape: (3, 4)\njnp.argmax(x, axis=1).shape: (3,)\nsj.argmax(x, axis=1).shape: (3, 4)\njnp.argmax(x, axis=0).shape: (4,)\nsj.argmax(x, axis=0).shape: (4, 3)\n</pre> <pre>jnp.argmax(x, axis=1, keepdims=True).shape: (3, 1)\nsj.argmax(x, axis=1, keepdims=True).shape: (3, 1, 4)\njnp.argmax(x, axis=0, keepdims=True).shape: (1, 4)\nsj.argmax(x, axis=0, keepdims=True).shape: (1, 4, 3)\n</pre> <p>We also offer soft versions of <code>argmedian</code>, <code>argsort</code> and <code>top_k</code>.</p> In\u00a0[21]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(4,))\nprint(\"x:\", x)\n\nprint(\"\\njnp.argmedian(x):\", \"Not implemented in standard JAX\")\nprint(\"sj.argmedian(x):\", sj.argmedian(x))\n\nprint(\"\\njnp.argsort(x):\", jnp.argsort(x))\nprint(\"sj.argsort(x):\", sj.argsort(x))\n\nprint(\"\\njax.lax.top_k(x, k=3)[1]:\", jax.lax.top_k(x, k=3)[1])\nprint(\"sj.top_k(x, k=3)[1]:\", sj.top_k(x, k=3)[1])\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(4,)) print(\"x:\", x)  print(\"\\njnp.argmedian(x):\", \"Not implemented in standard JAX\") print(\"sj.argmedian(x):\", sj.argmedian(x))  print(\"\\njnp.argsort(x):\", jnp.argsort(x)) print(\"sj.argsort(x):\", sj.argsort(x))  print(\"\\njax.lax.top_k(x, k=3)[1]:\", jax.lax.top_k(x, k=3)[1]) print(\"sj.top_k(x, k=3)[1]:\", sj.top_k(x, k=3)[1]) <pre>x: [0.41845711 0.21629545 0.96532146 0.57450053]\n\njnp.argmedian(x): Not implemented in standard JAX\n</pre> <pre>sj.argmedian(x): [4.95553956e-01 8.69234800e-03 1.99739291e-04 4.95553956e-01]\n\njnp.argsort(x): [1 0 3 2]\nsj.argsort(x): [[1.14092958e-01 8.61461280e-01 4.81124140e-04 2.39646378e-02]\n [7.42554231e-01 9.83447670e-02 3.13131302e-03 1.55969689e-01]\n [1.66975269e-01 2.21144035e-02 1.59597616e-02 7.94950566e-01]\n [4.11469085e-03 5.44954558e-04 9.75750772e-01 1.95895826e-02]]\n\njax.lax.top_k(x, k=3)[1]: [2 3 0]\n</pre> <pre>sj.top_k(x, k=3)[1]: [[4.11469085e-03 5.44954558e-04 9.75750772e-01 1.95895826e-02]\n [1.66975269e-01 2.21144035e-02 1.59597616e-02 7.94950566e-01]\n [7.42554231e-01 9.83447670e-02 3.13131302e-03 1.55969689e-01]]\n</pre> <p>Again, the shape of the returned <code>SoftIndex</code> matches that of the normal index array, except for an additional dimension in the last axis that matches the size of the input array along the specified axis. A few examples:</p> In\u00a0[22]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(3, 4))\nprint(\"x.shape:\", x.shape)\n\n# standard JAX only added support for axis argument in jax.lax.top_k recently, normally uses last axis\nprint(\"\\njax.lax.top_k(x, k=2, axis=1)[1].shape:\", jax.lax.top_k(x, k=2)[1].shape)\nprint(\"sj.top_k(x, k=2, axis=1)[1].shape:\", sj.top_k(x, k=2, axis=1)[1].shape)\nprint(\"sj.top_k(x, k=2, axis=0)[1].shape:\", sj.top_k(x, k=2, axis=0)[1].shape)\n\nprint(\"\\njnp.argsort(x, axis=1).shape:\", jnp.argsort(x, axis=1).shape)\nprint(\"sj.argsort(x, axis=1).shape:\", sj.argsort(x, axis=1).shape)\nprint(\"jnp.argsort(x, axis=0).shape:\", jnp.argsort(x, axis=0).shape)\nprint(\"sj.argsort(x, axis=0).shape:\", sj.argsort(x, axis=0).shape)\n\n# standard JAX does not support argmedian\nprint(\"\\nsj.argmedian(x, axis=1).shape:\", sj.argmedian(x, axis=1).shape)\nprint(\"sj.argmedian(x, axis=0).shape:\", sj.argmedian(x, axis=0).shape)\nprint(\n    \"sj.argmedian(x, axis=1, keepdims=True).shape:\",\n    sj.argmedian(x, axis=1, keepdims=True).shape,\n)\nprint(\n    \"sj.argmedian(x, axis=0, keepdims=True).shape:\",\n    sj.argmedian(x, axis=0, keepdims=True).shape,\n)\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(3, 4)) print(\"x.shape:\", x.shape)  # standard JAX only added support for axis argument in jax.lax.top_k recently, normally uses last axis print(\"\\njax.lax.top_k(x, k=2, axis=1)[1].shape:\", jax.lax.top_k(x, k=2)[1].shape) print(\"sj.top_k(x, k=2, axis=1)[1].shape:\", sj.top_k(x, k=2, axis=1)[1].shape) print(\"sj.top_k(x, k=2, axis=0)[1].shape:\", sj.top_k(x, k=2, axis=0)[1].shape)  print(\"\\njnp.argsort(x, axis=1).shape:\", jnp.argsort(x, axis=1).shape) print(\"sj.argsort(x, axis=1).shape:\", sj.argsort(x, axis=1).shape) print(\"jnp.argsort(x, axis=0).shape:\", jnp.argsort(x, axis=0).shape) print(\"sj.argsort(x, axis=0).shape:\", sj.argsort(x, axis=0).shape)  # standard JAX does not support argmedian print(\"\\nsj.argmedian(x, axis=1).shape:\", sj.argmedian(x, axis=1).shape) print(\"sj.argmedian(x, axis=0).shape:\", sj.argmedian(x, axis=0).shape) print(     \"sj.argmedian(x, axis=1, keepdims=True).shape:\",     sj.argmedian(x, axis=1, keepdims=True).shape, ) print(     \"sj.argmedian(x, axis=0, keepdims=True).shape:\",     sj.argmedian(x, axis=0, keepdims=True).shape, ) <pre>x.shape: (3, 4)\n</pre> <pre>\njax.lax.top_k(x, k=2, axis=1)[1].shape: (3, 2)\n</pre> <pre>sj.top_k(x, k=2, axis=1)[1].shape: (3, 2, 4)\nsj.top_k(x, k=2, axis=0)[1].shape: (2, 3, 4)\n</pre> <pre>\njnp.argsort(x, axis=1).shape: (3, 4)\nsj.argsort(x, axis=1).shape: (3, 4, 4)\njnp.argsort(x, axis=0).shape: (3, 4)\n</pre> <pre>sj.argsort(x, axis=0).shape: (3, 4, 3)\n\nsj.argmedian(x, axis=1).shape: (3, 4)\n</pre> <pre>sj.argmedian(x, axis=0).shape: (4, 3)\nsj.argmedian(x, axis=1, keepdims=True).shape: (3, 1, 4)\nsj.argmedian(x, axis=0, keepdims=True).shape: (1, 4, 3)\n</pre> <p>Note: All of the functions in this section come with the three modes: <code>hard</code>, <code>entropic</code> and <code>euclidean</code>. <code>hard</code> mode produces one-hot soft indices and is mainly used in straight-through estimation. <code>entropic</code> is the recommended soft default and reduces all operations to either a softmax or an entropy-regularized optimal transport problem. Finally, <code>euclidean</code> reduces operations to L2 projection onto the unit simplex or the Birkhoff polytope, which can be used to produce sparse outputs. See the API documentation for details.</p> In\u00a0[23]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(2, 3))\nprint(\"x:\\n\", x)\n\nindices = jnp.argmin(x, axis=1, keepdims=True)\nprint(\"min_jnp:\\n\", jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = sj.argmin(x, axis=1, mode=\"hard\", keepdims=True)\nprint(\"min_sj_hard:\\n\", sj.take_along_axis(x, indices_onehot, axis=1))\n\nindices_soft = sj.argmin(x, axis=1, keepdims=True)\nprint(\"min_sj_soft:\\n\", sj.take_along_axis(x, indices_soft, axis=1))\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(2, 3)) print(\"x:\\n\", x)  indices = jnp.argmin(x, axis=1, keepdims=True) print(\"min_jnp:\\n\", jnp.take_along_axis(x, indices, axis=1))  indices_onehot = sj.argmin(x, axis=1, mode=\"hard\", keepdims=True) print(\"min_sj_hard:\\n\", sj.take_along_axis(x, indices_onehot, axis=1))  indices_soft = sj.argmin(x, axis=1, keepdims=True) print(\"min_sj_soft:\\n\", sj.take_along_axis(x, indices_soft, axis=1)) <pre>x:\n [[0.41845711 0.21629545 0.96532146]\n [0.57450053 0.53222649 0.35490518]]\n</pre> <pre>min_jnp:\n [[0.21629545]\n [0.35490518]]\n</pre> <pre>min_sj_hard:\n [[0.21629545]\n [0.35490518]]\n</pre> <pre>min_sj_soft:\n [[0.24029622]\n [0.39747788]]\n</pre> <p>As a convenience, this combination of <code>sj.take_along_axis</code> with <code>SoftIndex</code>-generataing functions is already implemented ino Softjax's <code>max</code>, <code>median</code>, <code>top_k</code> and <code>sort</code> functions.</p> In\u00a0[24]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(4,))\nprint(\"x:\", x)\n\nprint(\"\\njnp.max(x):\", jnp.max(x))\nprint(\"sj.max(x, mode='hard'):\", sj.max(x, mode=\"hard\"))\nprint(\"sj.max(x):\", sj.max(x))\n\nprint(\"\\njnp.median(x):\", jnp.median(x))\nprint(\"sj.median(x, mode='hard'):\", sj.median(x, mode=\"hard\"))\nprint(\"sj.median(x):\", sj.median(x))\n\nprint(\"\\njax.lax.top_k(x, k=2)[0]:\", jax.lax.top_k(x, k=2)[0])\nprint(\"sj.top_k(x, k=2, mode='hard')[0]:\", sj.top_k(x, k=2, mode=\"hard\")[0])\nprint(\"sj.top_k(x, k=2)[0]:\", sj.top_k(x, k=2)[0])\n\nprint(\"\\njnp.sort(x):\", jnp.sort(x))\nprint(\"sj.sort(x, mode='hard'):\", sj.sort(x, mode=\"hard\"))\nprint(\"sj.sort(x):\", sj.sort(x))\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(4,)) print(\"x:\", x)  print(\"\\njnp.max(x):\", jnp.max(x)) print(\"sj.max(x, mode='hard'):\", sj.max(x, mode=\"hard\")) print(\"sj.max(x):\", sj.max(x))  print(\"\\njnp.median(x):\", jnp.median(x)) print(\"sj.median(x, mode='hard'):\", sj.median(x, mode=\"hard\")) print(\"sj.median(x):\", sj.median(x))  print(\"\\njax.lax.top_k(x, k=2)[0]:\", jax.lax.top_k(x, k=2)[0]) print(\"sj.top_k(x, k=2, mode='hard')[0]:\", sj.top_k(x, k=2, mode=\"hard\")[0]) print(\"sj.top_k(x, k=2)[0]:\", sj.top_k(x, k=2)[0])  print(\"\\njnp.sort(x):\", jnp.sort(x)) print(\"sj.sort(x, mode='hard'):\", sj.sort(x, mode=\"hard\")) print(\"sj.sort(x):\", sj.sort(x)) <pre>x: [0.41845711 0.21629545 0.96532146 0.57450053]\n\njnp.max(x): 0.9653214611189975\nsj.max(x, mode='hard'): 0.9653214611189975\n</pre> <pre>sj.max(x): 0.9550070794223621\n</pre> <pre>\njnp.median(x): 0.49647882272194555\nsj.median(x, mode='hard'): 0.49647882272194555\nsj.median(x): 0.494137017678798\n\njax.lax.top_k(x, k=2)[0]: [0.96532146 0.57450053]\nsj.top_k(x, k=2, mode='hard')[0]: [0.96532146 0.57450053]\n</pre> <pre>sj.top_k(x, k=2)[0]: [0.95500708 0.54676106]\n\njnp.sort(x): [0.21629545 0.41845711 0.57450053 0.96532146]\nsj.sort(x, mode='hard'): [0.21629545 0.41845711 0.57450053 0.96532146]\nsj.sort(x): [0.24830531 0.42462602 0.54676106 0.95500708]\n</pre> <p>Finally, we also offer a soft <code>ranking</code> operation. While it does not return a <code>SoftIndex</code> (because its output is the same shape as the input), it relies on similar computations under the hood as e.g. <code>sort</code>, and also offers the same modes.</p> In\u00a0[25]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(5,))\nprint(\"x:\\n\", x)\n# This computes the ranking operation\nprint(\"jnp.argsort(jnp.argsort(x)):\\n\", jnp.argsort(jnp.argsort(x)))\nprint(\n    \"sj.ranking(x, descending=False, mode='hard'):\\n\",\n    sj.ranking(x, descending=False, mode=\"hard\"),\n)\nprint(\"sj.ranking(x, descending=False):\\n\", sj.ranking(x, descending=False))\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(5,)) print(\"x:\\n\", x) # This computes the ranking operation print(\"jnp.argsort(jnp.argsort(x)):\\n\", jnp.argsort(jnp.argsort(x))) print(     \"sj.ranking(x, descending=False, mode='hard'):\\n\",     sj.ranking(x, descending=False, mode=\"hard\"), ) print(\"sj.ranking(x, descending=False):\\n\", sj.ranking(x, descending=False)) <pre>x:\n [0.41845711 0.21629545 0.96532146 0.57450053 0.53222649]\njnp.argsort(jnp.argsort(x)):\n [1 0 4 3 2]\nsj.ranking(x, descending=False, mode='hard'):\n [1. 0. 4. 3. 2.]\n</pre> <pre>sj.ranking(x, descending=False):\n [1.37238141 0.25184717 3.94097212 2.40480632 2.13591075]\n</pre> <p>The naming of the modes stems from a reduction of simple higher-level functions like <code>sj.abs</code> to more complex lower-level functions like <code>sj.argmax</code>. Starting from the <code>argmax</code> function, we relax it with a projection onto the unit simplex. We offer two modes, <code>entropic</code> and <code>euclidean</code> (in fact, ALL functions in Softjax support at least these two and the <code>hard</code> mode), which determine the regularizer used in the relaxed optimization problem. The solution to the <code>euclidean</code> case is available in closed-form via the classic softmax function, the <code>euclidean</code> case is a simple L2-projection onto the unit simplex which boils down to a sort+cumsum operation. Given the <code>argmax</code> relaxation, we directly get a <code>max</code> relaxation by taking the inner product of the soft indices with the original vector. Now we can define a relaxation of the <code>heaviside</code> function from the softened <code>argmax</code> operation, by observing that $\\text{heaviside}(x)=\\text{argmax}([x,0])[0]$. This results in different S-shaped sigmoid functions, in fact the standard exponential-sigmoid is the closed-solution to the <code>entropic</code> mode, whereas a linear inteprolation between 0 and 1 is the closed-form solution to the <code>euclidean</code> mode. Besides these modes, we define additional heaviside modes like <code>cubic</code>, <code>quintic</code> and <code>pseudohuber</code>, which all define different sigmoidal functions with different properties. Our heaviside relaxation can now be used to define relaxations for e.g. the <code>sign</code>, <code>abs</code> and <code>round</code> function. Most importantly though, we can move up the ladder to even higher-level functions based on the <code>ReLU</code> function. We first observe that we can generate the ReLU function from the heaviside function in two ways:</p> <ul> <li>By integrating $\\text{heaviside}(x)$ from negative infinity to x.</li> <li>By taking $x \\cdot \\text{heaviside}(x)$ (a \"gating\"-mechanism).</li> </ul> <p>Therefore, for each of our heaviside relaxations we can define two ReLU relaxations, some of which are well known. For example, the <code>entropic</code> case leads to the classic <code>softplus</code> function when integrated, and to the <code>SiLU</code> function when \"gated\" (we refer to this relaxation mode as <code>gated_entropic</code>). Similarly, the <code>euclidean</code>, <code>cubic</code> and <code>quintic</code> are simple piecewise-polynomials that we can integrate in closed-form. We can now use the soft ReLU relaxation to define more relaxations like e.g. <code>clip</code>.</p>"},{"location":"all-of-softjax/#all-of-softjax","title":"All of Softjax\u00b6","text":"<p>Softjax provides easy-to-use differentiable function surrogates of non-differentiable functions and discrete logic operations in JAX. Softjax offers soft function surrogates operating on real values, Booleans, and indices as well as wrappers to use these functions for gradient computation via straight-through estimation.</p> <p>This pages guides you through all of Softjax key functionalities.</p>"},{"location":"all-of-softjax/#1-softening","title":"1. Softening\u00b6","text":"<p>Many functions are not particulary well suited for gradient-based optimization as their gradients are either zero or undefined at discontinuities. Therefore, a typical approach in machine learning is to use soft surrogates for these functions to enable gradient-based optimization via automatic differentiation. Softjax provides such soft surrogates for many operations in JAX.</p> <p>As shown below, Softjax provides two hyper-parameters to tune soft function surrogates:</p> <ul> <li><p>mode being the type of function used for obtaining a soft approximation.</p> </li> <li><p>softness defining how close the surrogate function approximates the original function.</p> </li> </ul>"},{"location":"all-of-softjax/#2-straight-through-estimation","title":"2. Straight-through estimation\u00b6","text":"<p>Soft function surrogates can be used to compute soft gradients (or more accurately: soft vector-jacobian-producs) without modifying the forward pass via straight-through estimation. Straight-through estimation uses JAX's automatic differentiation system to replace only the function's gradient with the gradient of the surrogate function.</p> <p>Note: Historically, straight-through estimators refer to the special case of treating a function as the identity operation on the backward pass. We use the term more generally to describe the case of replacing a function with smooth a surrogate on the backward pass.</p> <p>Example - ReLu activation: The rectified linear unit (aka <code>relu</code>)  is commonly used as activation function in neural networks. For $x&lt;0$ the gradient of the <code>relu</code> is zero. In turn, neural networks containing <code>relu</code> activations may suffer from the \"dying ReLu problem\" where the gradients computed via automatic differentiation become zero when a gradient-based optimizer adjusts the inputs of ReLU functions to $x&lt;0$. As pointed out in \"The Resurrection of the ReLU\", you can mitigate these problems by replacing its backward pass with a soft surrogate function. To do this with Softjax, simply replace the <code>relu</code> activation in your code with <code>sj.st(sj.relu)</code>, or equivalently directly use our wrapped primitives via <code>sj.relu_st</code> for convenience.</p>"},{"location":"all-of-softjax/#the-ste-trick","title":"The STE Trick\u00b6","text":"<p>Under the hood <code>sj.st()</code> uses the stop-gradient oepration to replace the gradient of a function.</p> <pre>def st(fn: Callable) -&gt; Callable:\n    sig = inspect.signature(fn)\n    mode_default = sig.parameters.get(\"mode\").default\n    def wrapped(*args, **kwargs):\n        mode = kwargs.pop(\"mode\", mode_default)\n        fw_y = fn(*args, **kwargs, mode=\"hard\")\n        bw_y = fn(*args, **kwargs, mode=mode)\n        return jtu.tree_map(lambda fw, bw: jax.lax.stop_gradient(fw - bw) + bw, fw_y, bw_y)\n    return wrapped\n</pre> <p>By adding and subtracting the backward function <code>bw_y</code> to the function call, it does not alter the function's forward pass <code>fw_y</code>. Due to <code>jax.lax.stop_gradient</code>, only the soft backward function <code>bw_y</code> is used in the gradient computation.</p>"},{"location":"all-of-softjax/#custom-differentiation-rules","title":"Custom differentiation rules\u00b6","text":"<p>The <code>@sj.st</code> decorator can also be used to define custom straight-through operations. This can be useful when combining multiple functions provided by the softjax library. For this, it is important to understand, that simply applying the straight-through trick to every non-smooth function does not always result in the intended behavior. Consider for example the case of multiplying the output of two <code>relu</code> functions together. This function only provides meaningful gradients in the first quadrant, we would like to change it such that we get a meaningful signal in the whole domain, as visualized below.</p> <p>Note: We normalize the plotted gradient vectors for reduced cluttering.</p>"},{"location":"all-of-softjax/#3-soft-bools","title":"3. Soft bools\u00b6","text":"<p>Softjax provides differentiable surrogates of JAX's Boolean operators. A Boolean (aka Bool) is a data type that takes one of two possible values either being <code>false</code> or <code>true</code> (aka 0 or 1). Many operations in JAX such as <code>greater</code> or <code>isclose</code> generate arrays containing Booleans, while other operations such as <code>logical_and</code> or <code>any</code> operate on such arrays.</p> <p>Example - <code>jax.numpy.greater</code> yields zero gradients: As shown below, <code>jax.grad</code> does not raise an error when called on its boolean operations. However, the returned gradients are zero for all array entries.</p>"},{"location":"all-of-softjax/#generating-soft-bools","title":"Generating soft bools\u00b6","text":"<p>How does Softjax make Boolean logic operations differentiable? A real number $x\\in \\mathbb{R}$ could be mapped to a <code>Bool</code> using the Heaviside function $$ H(x) = \\begin{cases} 1, &amp; x &gt; 0 \\\\ 0.5, &amp; x=0\\\\ 0, &amp; x &lt; 0 \\end{cases}. $$ The gradient of the Heaviside function (as implemented in JAX) is zero everywhere and hence unsuited for differentiable optimization. Instead of operating directly on Booleans, Softjax's differentiable logic operators resort to soft Booleans. A soft Boolean aka <code>SoftBool</code> can be interpreted as the probability of a Boolean being <code>True</code>.</p> <p>We replace the heaviside function with differentiable surrogate such as the sigmoid function $\\sigma(x) = \\frac{1}{1+e^{-x}}$. While the <code>sigmoid</code> is the canonical example for mapping a real number to a <code>SoftBool</code>, Softjax provides additional surrogates.</p>"},{"location":"all-of-softjax/#manipulating-soft-bools","title":"Manipulating soft bools\u00b6","text":"<p>Softjax replaces a Boolean with a <code>SoftBool</code>, in turn Boolean logic operators are replaced in Softjax with fuzzy logic operators that effectively compute the probabilities of Boolean events.</p> <p>Example - Logical NOT: Given a <code>SoftBool</code> $P(B)$ (being the probability that a Boolean event $B$ occurs), the probability of the event not occuring is $P(\\bar B) = 1 - P(B)$ as implemented in <code>sj.logical_not</code>.</p> <pre>def logical_not(x: SoftBool) -&gt; SoftBool:\n    return 1 - x\n</pre> <p>Given <code>sj.logical_not</code>, the probability that <code>x is not greater equal 0.5</code> is given by <code>sj.logical_not(sj.greater_st(x, 0.5))</code>. Due to the straight-through trick, the function <code>sj.logical_not(sj.greater_st(x, 0.5))</code> uses exact Boolean logic in the forward pass and the <code>SoftBool</code> probability computation in the backward pass.</p>"},{"location":"all-of-softjax/#selection-with-soft-bools","title":"Selection with soft bools\u00b6","text":"<p>Through the use of Fuzzy logic operators, Softjax provides a toolbox to make many non-differentiable functions of JAX differentiable.</p> <p>Example - sj.where(): The function <code>jax.numpy.where(condition, x, y)</code> selects elements of array <code>x</code> if <code>condition == True</code> and otherwise selects <code>y</code>. Softjax provides a differentiable surrogate for this function via <code>sj.where(P, x, y)</code> which effectively computes the expected value $\\mathbb{E}[X] = P \\cdot x + (1-P) \\cdot y$.</p>"},{"location":"all-of-softjax/#4-soft-indices","title":"4. Soft indices\u00b6","text":"<p>Softjax offers soft surrogates for functions that generate indices as outputs, such as <code>argmax</code>, <code>argmin</code>, <code>top_k</code>, <code>argmedian</code>, and <code>argsort</code>. The main mechanism here is to replace hard indices with distributions over indices (<code>SoftIndex</code>), allowing for informative gradients.</p> <p>Similar to how <code>SoftBool</code> required going from boolean logic to fuzzy logic, this now requires adjusting functions that do selection via indices. As such, we provide new versions of e.g. <code>take_along_axis</code>, <code>dynamic_index_in_dim</code>, and <code>choose</code>. Combining the soft index generation with the selection then allows to define surrogates for the corresponding <code>max</code>, <code>min</code>, <code>top_k</code>, <code>median</code> and <code>sort</code> functions.</p>"},{"location":"all-of-softjax/#generating-soft-indices","title":"Generating soft indices\u00b6","text":"<p>In JAX, functions like <code>jax.argmax</code> return integer indices as outputs, which can take values within {0, ..., len(x)-1}.</p>"},{"location":"all-of-softjax/#selection-with-softindices","title":"Selection with SoftIndices\u00b6","text":"<p>Given a <code>SoftIndex</code>, Softjax provides (differentiable) helper functions for selecting array elements, mirroring the non-differentiable indexing in standard JAX. Put simply, entries of an array are selected by computing the expected value:</p> <p>$$\\mathrm{E}(arr, p) = \\sum_{i} arr[i] \\cdot p[i] = arr^{\\top} \\cdot p$$</p> <p>where $p$ is the <code>SoftIndex</code>.</p> <p>Example <code>sj.take_along_axis</code>: The function <code>sj.take_along_axis</code> is central to this selection mechanism. It generalizes <code>jnp.take_along_axis</code> to work with probability distributions (SoftIndices) instead of just integer indices.</p> <p>The standard <code>jnp.take_along_axis(arr, indices, axis)</code> selects elements from <code>arr</code> using integer indices. Conceptually, it works by:</p> <ol> <li>Slicing along the specified axis to get 1D arrays</li> <li>Using the corresponding indices to select elements</li> <li>Assembling the results into the output array</li> </ol> <p>One of its main uses is to accept the index output of e.g. <code>jnp.argmax</code> and select the maximum values at the indexed locations. While <code>jnp.take_along_axis</code> uses integer indices <code>out_1d[j] = arr_1d[indices_1d[j]]</code>, <code>sj.take_along_axis</code> accepts a <code>SoftIndex</code> to compute the corresponding the weighted sum: <code>out_1d[j] = sum_i(arr_1d[i] * soft_indices_2d[j, i])</code>.</p>"},{"location":"all-of-softjax/#a-note-on-modes","title":"A note on modes\u00b6","text":""},{"location":"manifold_points/","title":"Manifold Points","text":"In\u00a0[1]: Copied! <pre>from dataclasses import dataclass\nfrom typing import Literal\n\nimport jax\nimport jax.numpy as jp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport softjax as sj\n\n\njax.config.update(\"jax_enable_x64\", True)\njax.config.update(\"jax_default_matmul_precision\", \"high\")\njax.config.update(\"jax_platforms\", \"cpu\")\n</pre> from dataclasses import dataclass from typing import Literal  import jax import jax.numpy as jp import matplotlib.pyplot as plt import numpy as np import softjax as sj   jax.config.update(\"jax_enable_x64\", True) jax.config.update(\"jax_default_matmul_precision\", \"high\") jax.config.update(\"jax_platforms\", \"cpu\")  In\u00a0[2]: Copied! <pre># Hard manifold selector mirrored from MJX (collision_convex.py)\ndef manifold_points_mjx(\n    poly: jp.ndarray, poly_mask: jp.ndarray, poly_norm: jp.ndarray\n) -&gt; jp.ndarray:\n    \"\"\"MJX hard manifold heuristic (returns 4 vertex indices).\"\"\"\n    dist_mask = jp.where(poly_mask, 0.0, -1e6)\n    # A: any unmasked vertex (in MJX this corresponds to the most penetrating one)\n    # Note: We add a small tie-breaker to ensure consistent results\n    a_idx = jp.argmax(dist_mask - 0.1 * jp.arange(poly.shape[0]))\n    a = poly[a_idx]\n    # B: farthest from A (largest squared distance)\n    b_idx = (((a - poly) ** 2).sum(axis=1) + dist_mask).argmax()\n    b = poly[b_idx]\n    # C: farthest from the AB line within the plane\n    ab = jp.cross(poly_norm, a - b)\n    ap = a - poly\n    c_idx = (jp.abs(ap.dot(ab)) + dist_mask).argmax()\n    c = poly[c_idx]\n    # D: farthest from edges AC and BC\n    ac = jp.cross(poly_norm, a - c)\n    bc = jp.cross(poly_norm, b - c)\n    bp = b - poly\n    dist_bp = jp.abs(bp.dot(bc)) + dist_mask\n    dist_ap = jp.abs(ap.dot(ac)) + dist_mask\n    d_idx = (dist_bp + dist_ap).argmax() % poly.shape[0]\n    return jp.array([a_idx, b_idx, c_idx, d_idx])\n</pre> # Hard manifold selector mirrored from MJX (collision_convex.py) def manifold_points_mjx(     poly: jp.ndarray, poly_mask: jp.ndarray, poly_norm: jp.ndarray ) -&gt; jp.ndarray:     \"\"\"MJX hard manifold heuristic (returns 4 vertex indices).\"\"\"     dist_mask = jp.where(poly_mask, 0.0, -1e6)     # A: any unmasked vertex (in MJX this corresponds to the most penetrating one)     # Note: We add a small tie-breaker to ensure consistent results     a_idx = jp.argmax(dist_mask - 0.1 * jp.arange(poly.shape[0]))     a = poly[a_idx]     # B: farthest from A (largest squared distance)     b_idx = (((a - poly) ** 2).sum(axis=1) + dist_mask).argmax()     b = poly[b_idx]     # C: farthest from the AB line within the plane     ab = jp.cross(poly_norm, a - b)     ap = a - poly     c_idx = (jp.abs(ap.dot(ab)) + dist_mask).argmax()     c = poly[c_idx]     # D: farthest from edges AC and BC     ac = jp.cross(poly_norm, a - c)     bc = jp.cross(poly_norm, b - c)     bp = b - poly     dist_bp = jp.abs(bp.dot(bc)) + dist_mask     dist_ap = jp.abs(ap.dot(ac)) + dist_mask     d_idx = (dist_bp + dist_ap).argmax() % poly.shape[0]     return jp.array([a_idx, b_idx, c_idx, d_idx])  In\u00a0[3]: Copied! <pre>def manifold_points_softjax(\n    poly: jp.ndarray,\n    poly_mask: jp.ndarray,\n    poly_norm: jp.ndarray,\n    *,\n    softness: float = 1.0,\n    mode: Literal[\"hard\", \"entropic\"] = \"entropic\",\n) -&gt; jp.ndarray:\n    \"\"\"Soft counterpart of `manifold_points_mjx`.\n\n    Returns 4 SoftIndex distributions (shape: 4 x n).\n    \"\"\"\n    dist_mask = jp.where(poly_mask, 0.0, -1e6)\n\n    def argmax_soft(x):\n        return sj.argmax(x, mode=mode, softness=softness, axis=0)\n\n    def abs_soft(x):\n        return sj.abs(x, mode=mode, softness=softness)\n\n    # A: soft argmax over masked distances\n    a_idx = argmax_soft(dist_mask - 0.1 * jp.arange(poly.shape[0]))\n    a = sj.dynamic_index_in_dim(poly, a_idx, axis=0, keepdims=False)\n    # B: soft argmax of distance from A\n    b_logits = (((a - poly) ** 2).sum(axis=1)) + dist_mask\n    b_idx = argmax_soft(b_logits)\n    b = sj.dynamic_index_in_dim(poly, b_idx, axis=0, keepdims=False)\n    # C: soft argmax farthest from AB line\n    ab = jp.cross(poly_norm, a - b)\n    ap = a - poly\n    c_logits = abs_soft(ap.dot(ab)) + dist_mask\n    c_idx = argmax_soft(c_logits)\n    c = sj.dynamic_index_in_dim(poly, c_idx, axis=0, keepdims=False)\n    # D: soft argmax farthest from AC and BC edges\n    ac = jp.cross(poly_norm, a - c)\n    bc = jp.cross(poly_norm, b - c)\n    bp = b - poly\n    dist_bp = abs_soft(bp.dot(bc)) + dist_mask\n    dist_ap = abs_soft(ap.dot(ac)) + dist_mask\n    d_logits = dist_bp + dist_ap\n    d_idx = argmax_soft(d_logits)\n    return jp.stack([a_idx, b_idx, c_idx, d_idx], axis=0)\n</pre> def manifold_points_softjax(     poly: jp.ndarray,     poly_mask: jp.ndarray,     poly_norm: jp.ndarray,     *,     softness: float = 1.0,     mode: Literal[\"hard\", \"entropic\"] = \"entropic\", ) -&gt; jp.ndarray:     \"\"\"Soft counterpart of `manifold_points_mjx`.      Returns 4 SoftIndex distributions (shape: 4 x n).     \"\"\"     dist_mask = jp.where(poly_mask, 0.0, -1e6)      def argmax_soft(x):         return sj.argmax(x, mode=mode, softness=softness, axis=0)      def abs_soft(x):         return sj.abs(x, mode=mode, softness=softness)      # A: soft argmax over masked distances     a_idx = argmax_soft(dist_mask - 0.1 * jp.arange(poly.shape[0]))     a = sj.dynamic_index_in_dim(poly, a_idx, axis=0, keepdims=False)     # B: soft argmax of distance from A     b_logits = (((a - poly) ** 2).sum(axis=1)) + dist_mask     b_idx = argmax_soft(b_logits)     b = sj.dynamic_index_in_dim(poly, b_idx, axis=0, keepdims=False)     # C: soft argmax farthest from AB line     ab = jp.cross(poly_norm, a - b)     ap = a - poly     c_logits = abs_soft(ap.dot(ab)) + dist_mask     c_idx = argmax_soft(c_logits)     c = sj.dynamic_index_in_dim(poly, c_idx, axis=0, keepdims=False)     # D: soft argmax farthest from AC and BC edges     ac = jp.cross(poly_norm, a - c)     bc = jp.cross(poly_norm, b - c)     bp = b - poly     dist_bp = abs_soft(bp.dot(bc)) + dist_mask     dist_ap = abs_soft(ap.dot(ac)) + dist_mask     d_logits = dist_bp + dist_ap     d_idx = argmax_soft(d_logits)     return jp.stack([a_idx, b_idx, c_idx, d_idx], axis=0)  In\u00a0[4]: Copied! <pre>@dataclass\nclass Selection:\n    poly: np.ndarray\n    hard_idx: np.ndarray\n    hard_pts: np.ndarray\n    soft_runs: list[tuple[float, np.ndarray]]\n\n\ndef make_planar_polygon(\n    n: int = 8, radius: float = 1.0, jitter: float = 0.05, seed: int = 0\n):\n    rng = np.random.RandomState(seed)\n    angles = np.linspace(0, 2 * np.pi, n, endpoint=False)\n    r = radius * (1.0 + jitter * rng.randn(n))\n    x = r * np.cos(angles)\n    y = r * np.sin(angles)\n    poly2d = np.stack([x, y], axis=1)\n    center = poly2d.mean(axis=0)\n    angles_ccw = np.arctan2(poly2d[:, 1] - center[1], poly2d[:, 0] - center[0])\n    order = np.argsort(angles_ccw)\n    poly2d = poly2d[order]\n    poly3d = np.concatenate([poly2d, np.zeros((n, 1))], axis=1)\n    return jp.array(poly3d)\n\n\ndef order_quad(points: np.ndarray):\n    center = points.mean(axis=0)\n    angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])\n    order = np.argsort(angles)\n    return points[order]\n\n\ndef run_selection(\n    n: int = 16,\n    softness: list[float] | tuple[float, ...] = (1.0,),\n    mode: str = \"entropic\",\n    seed: int = 0,\n) -&gt; Selection:\n    poly = make_planar_polygon(n=n, seed=seed)\n    mask = jp.ones((n,), dtype=bool)\n    normal = jp.array([0.0, 0.0, 1.0])\n\n    hard_idx = np.array(manifold_points_mjx(poly, mask, normal))\n    hard_pts = np.array(poly[hard_idx])\n\n    soft_runs = []\n    for w in softness:\n        soft_probs = manifold_points_softjax(\n            poly, mask, normal, softness=float(w), mode=mode\n        )\n        soft_pts = soft_probs @ np.array(poly)\n        soft_runs.append((w, soft_pts))\n\n    return Selection(\n        poly=np.array(poly), hard_idx=hard_idx, hard_pts=hard_pts, soft_runs=soft_runs\n    )\n\n\ndef plot_multi(sel: Selection, mode: str):\n    poly2d = sel.poly[:, :2]\n    hull = order_quad(poly2d)\n    hull_closed = np.vstack([hull, hull[0]])\n\n    fig, ax = plt.subplots(figsize=(8, 7))\n    ax.fill(\n        hull_closed[:, 0],\n        hull_closed[:, 1],\n        color=\"#dddddd\",\n        alpha=0.3,\n        label=\"poly hull\",\n    )\n    ax.plot(hull_closed[:, 0], hull_closed[:, 1], color=\"#999999\", lw=1.0)\n    ax.scatter(poly2d[:, 0], poly2d[:, 1], color=\"#444444\", s=40, label=\"verts\")\n\n    hard_xy = order_quad(sel.hard_pts[:, :2])\n    hard_closed = np.vstack([hard_xy, hard_xy[0]])\n    ax.plot(\n        hard_closed[:, 0], hard_closed[:, 1], color=\"#000000\", lw=2.0, label=\"hard_mjx\"\n    )\n    ax.scatter(hard_xy[:, 0], hard_xy[:, 1], color=\"#000000\", s=60, zorder=3)\n\n    cmap = plt.get_cmap(\"tab10\")\n    for i, (w, soft_pts) in enumerate(sel.soft_runs):\n        soft_xy = order_quad(np.array(soft_pts)[:, :2])\n        soft_closed = np.vstack([soft_xy, soft_xy[0]])\n        color = cmap(i % cmap.N)\n        ax.plot(\n            soft_closed[:, 0],\n            soft_closed[:, 1],\n            color=color,\n            lw=2.0,\n            ls=\"--\",\n            label=f\"soft w={w}\",\n        )\n        ax.scatter(soft_xy[:, 0], soft_xy[:, 1], color=color, s=55, zorder=3)\n\n    ax.set_aspect(\"equal\", adjustable=\"datalim\")\n    ax.set_title(f\"Hard vs soft manifold points (mode={mode})\")\n    ax.legend(loc=\"upper right\")\n    ax.grid(True, alpha=0.2)\n    plt.show()\n</pre> @dataclass class Selection:     poly: np.ndarray     hard_idx: np.ndarray     hard_pts: np.ndarray     soft_runs: list[tuple[float, np.ndarray]]   def make_planar_polygon(     n: int = 8, radius: float = 1.0, jitter: float = 0.05, seed: int = 0 ):     rng = np.random.RandomState(seed)     angles = np.linspace(0, 2 * np.pi, n, endpoint=False)     r = radius * (1.0 + jitter * rng.randn(n))     x = r * np.cos(angles)     y = r * np.sin(angles)     poly2d = np.stack([x, y], axis=1)     center = poly2d.mean(axis=0)     angles_ccw = np.arctan2(poly2d[:, 1] - center[1], poly2d[:, 0] - center[0])     order = np.argsort(angles_ccw)     poly2d = poly2d[order]     poly3d = np.concatenate([poly2d, np.zeros((n, 1))], axis=1)     return jp.array(poly3d)   def order_quad(points: np.ndarray):     center = points.mean(axis=0)     angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])     order = np.argsort(angles)     return points[order]   def run_selection(     n: int = 16,     softness: list[float] | tuple[float, ...] = (1.0,),     mode: str = \"entropic\",     seed: int = 0, ) -&gt; Selection:     poly = make_planar_polygon(n=n, seed=seed)     mask = jp.ones((n,), dtype=bool)     normal = jp.array([0.0, 0.0, 1.0])      hard_idx = np.array(manifold_points_mjx(poly, mask, normal))     hard_pts = np.array(poly[hard_idx])      soft_runs = []     for w in softness:         soft_probs = manifold_points_softjax(             poly, mask, normal, softness=float(w), mode=mode         )         soft_pts = soft_probs @ np.array(poly)         soft_runs.append((w, soft_pts))      return Selection(         poly=np.array(poly), hard_idx=hard_idx, hard_pts=hard_pts, soft_runs=soft_runs     )   def plot_multi(sel: Selection, mode: str):     poly2d = sel.poly[:, :2]     hull = order_quad(poly2d)     hull_closed = np.vstack([hull, hull[0]])      fig, ax = plt.subplots(figsize=(8, 7))     ax.fill(         hull_closed[:, 0],         hull_closed[:, 1],         color=\"#dddddd\",         alpha=0.3,         label=\"poly hull\",     )     ax.plot(hull_closed[:, 0], hull_closed[:, 1], color=\"#999999\", lw=1.0)     ax.scatter(poly2d[:, 0], poly2d[:, 1], color=\"#444444\", s=40, label=\"verts\")      hard_xy = order_quad(sel.hard_pts[:, :2])     hard_closed = np.vstack([hard_xy, hard_xy[0]])     ax.plot(         hard_closed[:, 0], hard_closed[:, 1], color=\"#000000\", lw=2.0, label=\"hard_mjx\"     )     ax.scatter(hard_xy[:, 0], hard_xy[:, 1], color=\"#000000\", s=60, zorder=3)      cmap = plt.get_cmap(\"tab10\")     for i, (w, soft_pts) in enumerate(sel.soft_runs):         soft_xy = order_quad(np.array(soft_pts)[:, :2])         soft_closed = np.vstack([soft_xy, soft_xy[0]])         color = cmap(i % cmap.N)         ax.plot(             soft_closed[:, 0],             soft_closed[:, 1],             color=color,             lw=2.0,             ls=\"--\",             label=f\"soft w={w}\",         )         ax.scatter(soft_xy[:, 0], soft_xy[:, 1], color=color, s=55, zorder=3)      ax.set_aspect(\"equal\", adjustable=\"datalim\")     ax.set_title(f\"Hard vs soft manifold points (mode={mode})\")     ax.legend(loc=\"upper right\")     ax.grid(True, alpha=0.2)     plt.show()  <p>We first check, that in <code>hard</code> mode our softjax version matches the original mjx selection.</p> In\u00a0[5]: Copied! <pre>sel = run_selection(softness=(0.0,), mode=\"hard\", seed=0)\nplot_multi(sel, mode=\"hard\")\n</pre> sel = run_selection(softness=(0.0,), mode=\"hard\", seed=0) plot_multi(sel, mode=\"hard\") <p>Next, we visualize the relaxations for two different modes.</p> In\u00a0[6]: Copied! <pre>sel = run_selection(softness=[0.5, 1.0, 1.5, 2.0], mode=\"entropic\", seed=0)\nplot_multi(sel, mode=\"entropic\")\n</pre> sel = run_selection(softness=[0.5, 1.0, 1.5, 2.0], mode=\"entropic\", seed=0) plot_multi(sel, mode=\"entropic\") In\u00a0[7]: Copied! <pre>from softjax.straight_through import st\n\nmanifold_points_softjax_st = st(manifold_points_softjax)\n</pre> from softjax.straight_through import st  manifold_points_softjax_st = st(manifold_points_softjax) <p>The <code>manifold_points_softjax_st</code> now still has the discrete one-hot outputs of the hard function, but provides the gradients of the relaxed version.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"manifold_points/#manifold-points","title":"Manifold Points\u00b6","text":"<p>This notebook provides an example of how to translate a hard jax function into a softjax function in practice.</p> <p>As the worked example, we consider Mujoco MJX's convex collision detection algorithm, which has a subroutine that chooses four vertices (A, B, C, D) that roughly maximize the contact patch area.</p>"},{"location":"manifold_points/#original-function","title":"Original function\u00b6","text":"<p>The steps of the selection algorithm are</p> <ol> <li>Pick A: any unmasked vertex (in MJX this is the most penetrating one).</li> <li>Pick B: the vertex farthest from A (long base edge).</li> <li>Pick C: the vertex farthest from line AB within the plane (opens the area).</li> <li>Pick D: the vertex farthest from both edges AC and BC.</li> </ol>"},{"location":"manifold_points/#softjax-version","title":"SoftJax version\u00b6","text":"<p>To make this algorithm smoothly differentiable we convert it to softjax. For this, we:</p> <ul> <li>replace discrete <code>argmax</code> with <code>sj.argmax</code>, which returns a <code>SoftIndex</code> distribution,</li> <li>replace the hard indexing with <code>sj.dynamic_index_in_dim</code>, which uses the <code>SoftIndex</code> as input,</li> <li>replace <code>abs</code> with <code>sj.abs</code>.</li> </ul> <p>Note that we now return four <code>SoftIndex</code> distributions (shape: 4 x n) instead of the hard indices. The remaining code of the collision detection therefore would need to be adjusted accordingly.</p>"},{"location":"manifold_points/#visualization","title":"Visualization\u00b6","text":"<p>We generate a perturbed planar polygon for visualization.</p>"},{"location":"manifold_points/#straight-through-estimation","title":"Straight-through estimation\u00b6","text":"<p>We could now directly use one of the relaxations as a differentiable proxy. However, sometimes it is desired to not alter the forward pass, e.g. in simulation we do not want to relaxt the forward physics. In these cases, we can resort to the straight-through trick, which means to replace only the gradient of a hard function on forward with the gradeint of a relaxed/soft function on backward pass. <code>softjax.straight_through.st</code> implements this behavior by wrapping a function so that the forward pass uses a hard definition while the backward pass uses a soft surrogate. For convenience, we all ready provide wrapped versions of all our primitives, allowing the user to just use e.g. <code>sj.argmax_st</code> instead of <code>sj.argmax</code>.</p> <p>However, as described in the all-of-softjax notebook, this can still lead to uninformative gradients due to the interaction of the straight-through trick with the chain rule. A way to avoid this issue is to apply the straight-through trick on the outer level of the downstream function, which calls the whole function twice instead of each of the primitives. This is illustrated below.</p>"},{"location":"plots/","title":"Plots","text":"In\u00a0[2]: Copied! <pre>sigmoid_modes = [\"entropic\", \"euclidean\", \"pseudohuber\", \"cubic\", \"quintic\"]\n</pre> sigmoid_modes = [\"entropic\", \"euclidean\", \"pseudohuber\", \"cubic\", \"quintic\"] In\u00a0[3]: Copied! <pre>def median_newton(x, **kwargs):\n    return sj.median_newton(jnp.array([x, -0.5, 0.5]), **kwargs)\n\n\ndef greater_than_1(x, **kwargs):\n    return sj.greater(x, jnp.array(1.0), **kwargs)\n\n\ndef less_than_1(x, **kwargs):\n    return sj.less(x, jnp.array(1.0), **kwargs)\n\n\ndef equal_to_0(x, **kwargs):\n    return sj.equal(x, jnp.array(0.0), **kwargs)\n\n\ndef not_equal_to_0(x, **kwargs):\n    return sj.not_equal(x, jnp.array(0.0), **kwargs)\n\n\ndef isclose_to_0(x, **kwargs):\n    return sj.not_equal(x, jnp.array(0.0), **kwargs)\n\n\nplot(sj.heaviside, modes=sigmoid_modes)\nplot(sj.abs, modes=sigmoid_modes)\nplot(sj.sign, modes=sigmoid_modes)\nplot(sj.round, modes=sigmoid_modes)\nplot(median_newton, modes=sigmoid_modes)\nplot(greater_than_1, modes=sigmoid_modes)\nplot(less_than_1, modes=sigmoid_modes)\nplot(equal_to_0, modes=sigmoid_modes)\nplot(not_equal_to_0, modes=sigmoid_modes)\nplot(isclose_to_0, modes=sigmoid_modes)\n</pre> def median_newton(x, **kwargs):     return sj.median_newton(jnp.array([x, -0.5, 0.5]), **kwargs)   def greater_than_1(x, **kwargs):     return sj.greater(x, jnp.array(1.0), **kwargs)   def less_than_1(x, **kwargs):     return sj.less(x, jnp.array(1.0), **kwargs)   def equal_to_0(x, **kwargs):     return sj.equal(x, jnp.array(0.0), **kwargs)   def not_equal_to_0(x, **kwargs):     return sj.not_equal(x, jnp.array(0.0), **kwargs)   def isclose_to_0(x, **kwargs):     return sj.not_equal(x, jnp.array(0.0), **kwargs)   plot(sj.heaviside, modes=sigmoid_modes) plot(sj.abs, modes=sigmoid_modes) plot(sj.sign, modes=sigmoid_modes) plot(sj.round, modes=sigmoid_modes) plot(median_newton, modes=sigmoid_modes) plot(greater_than_1, modes=sigmoid_modes) plot(less_than_1, modes=sigmoid_modes) plot(equal_to_0, modes=sigmoid_modes) plot(not_equal_to_0, modes=sigmoid_modes) plot(isclose_to_0, modes=sigmoid_modes) In\u00a0[4]: Copied! <pre>softplus_modes = [\"entropic\", \"euclidean\", \"quartic\"]\nsoftplus_modes_gated = [\n    \"gated_entropic\",\n    \"gated_euclidean\",\n    \"gated_cubic\",\n    \"gated_quintic\",\n    \"gated_pseudohuber\",\n]\n</pre> softplus_modes = [\"entropic\", \"euclidean\", \"quartic\"] softplus_modes_gated = [     \"gated_entropic\",     \"gated_euclidean\",     \"gated_cubic\",     \"gated_quintic\",     \"gated_pseudohuber\", ] In\u00a0[5]: Copied! <pre>def clip_between_0_and_1(x, **kwargs):\n    return sj.clip(x, jnp.array(0.0), jnp.array(1.0), **kwargs)\n\n\nplot(sj.relu, modes=softplus_modes)\nplot(sj.relu, modes=softplus_modes_gated)\nplot(clip_between_0_and_1, modes=softplus_modes)\nplot(clip_between_0_and_1, modes=softplus_modes_gated)\n</pre> def clip_between_0_and_1(x, **kwargs):     return sj.clip(x, jnp.array(0.0), jnp.array(1.0), **kwargs)   plot(sj.relu, modes=softplus_modes) plot(sj.relu, modes=softplus_modes_gated) plot(clip_between_0_and_1, modes=softplus_modes) plot(clip_between_0_and_1, modes=softplus_modes_gated) In\u00a0[6]: Copied! <pre>projection_modes = [\"entropic\", \"euclidean\"]\n</pre> projection_modes = [\"entropic\", \"euclidean\"] In\u00a0[7]: Copied! <pre>def sj_max(x, **kwargs):\n    return sj.max(jnp.array([x, 0.5]), **kwargs)\n\n\ndef sj_min(x, **kwargs):\n    return sj.min(jnp.array([x, 0.5]), **kwargs)\n\n\ndef sj_median(x, **kwargs):\n    return sj.median(jnp.array([x, -1.0, 1.0, -2.0, 2.0]), **kwargs)\n\n\ndef sj_sort(x, **kwargs):\n    return sj.sort(jnp.array([x, -1.0, 1.0]), **kwargs)[1]\n\n\ndef sj_top_k(x, **kwargs):\n    return sj.top_k(jnp.array([x, -1.0, 1.0]), k=2, **kwargs)[0][1]\n\n\nplot(sj_max, modes=projection_modes)\nplot(sj_min, modes=projection_modes)\nplot(sj_median, modes=projection_modes)\nplot(sj_sort, modes=projection_modes)\nplot(sj_top_k, modes=projection_modes)\n\n# plot(sj_median, modes=projection_modes, fast=False)\n# plot(sj_sort, modes=projection_modes, fast=False)\n# plot(sj_top_k, modes=projection_modes, fast=False)\n</pre> def sj_max(x, **kwargs):     return sj.max(jnp.array([x, 0.5]), **kwargs)   def sj_min(x, **kwargs):     return sj.min(jnp.array([x, 0.5]), **kwargs)   def sj_median(x, **kwargs):     return sj.median(jnp.array([x, -1.0, 1.0, -2.0, 2.0]), **kwargs)   def sj_sort(x, **kwargs):     return sj.sort(jnp.array([x, -1.0, 1.0]), **kwargs)[1]   def sj_top_k(x, **kwargs):     return sj.top_k(jnp.array([x, -1.0, 1.0]), k=2, **kwargs)[0][1]   plot(sj_max, modes=projection_modes) plot(sj_min, modes=projection_modes) plot(sj_median, modes=projection_modes) plot(sj_sort, modes=projection_modes) plot(sj_top_k, modes=projection_modes)  # plot(sj_median, modes=projection_modes, fast=False) # plot(sj_sort, modes=projection_modes, fast=False) # plot(sj_top_k, modes=projection_modes, fast=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"plots/#plots","title":"Plots\u00b6","text":"<p>Below we plot many of the Softjax functions for different modes and softnesses to give an idea of how they look like. All functions are designed to be soft when used on inputs in an interval [0,1] when setting default softness of 1.0. When inputs in other ranges are used, softness can be scaled accrdingly, e.g. on inputs distributed in the inteval [0, 100] a softness of 100.0 should result in a soft behavior.</p>"},{"location":"plots/#sigmoid-based","title":"Sigmoid-based\u00b6","text":""},{"location":"plots/#softplus-based","title":"Softplus-based\u00b6","text":""},{"location":"plots/#simplex-projection-optimal-transport-based","title":"Simplex-projection / optimal transport based\u00b6","text":""},{"location":"quick-example/","title":"Quick Example","text":"In\u00a0[1]: Copied! <pre>import jax\nimport jax.numpy as jnp\nimport softjax as sj\n\n\njnp.set_printoptions(precision=4, suppress=True)\njax.config.update(\"jax_enable_x64\", True)\njax.config.update(\"jax_default_matmul_precision\", \"high\")\njax.config.update(\"jax_platforms\", \"cpu\")\n</pre> import jax import jax.numpy as jnp import softjax as sj   jnp.set_printoptions(precision=4, suppress=True) jax.config.update(\"jax_enable_x64\", True) jax.config.update(\"jax_default_matmul_precision\", \"high\") jax.config.update(\"jax_platforms\", \"cpu\") <p>The default mode of all our Softjax functions is <code>mode=entropic</code>. This uses some well-known exponential-function-based relaxations, e.g. an exponential sigmoid as a soft heaviside function and the <code>softplus</code> function as a soft ReLU function.</p> <p>We can replicate the original JAX function outputs (plus some one-hot generation for index functions) by setting <code>mode=hard</code>. This is especially useful for straight-through estimators, where we want to easily switch between hard and soft modes.</p> <p>Note that all Softjax functions taking a <code>mode</code> argument also take a <code>softness</code> argument, which determines how soft the relaxation is. Note that while the limit of softness going to zero should always recover the hard function, setting <code>softness=0</code> is not allowed. Instead, use the <code>mode=hard</code> setting.</p> In\u00a0[2]: Copied! <pre>x = jnp.array([-0.2, -1.0, 0.3, 1.0])\n\n# Elementwise operators\nprint(\"\\nJAX absolute:\", jnp.abs(x))\nprint(\"SoftJAX absolute (hard mode):\", sj.abs(x, mode=\"hard\"))\nprint(\"SoftJAX absolute (soft mode):\", sj.abs(x))\n\nprint(\"\\nJAX clip:\", jnp.clip(x, -0.5, 0.5))\nprint(\"SoftJAX clip (hard mode):\", sj.clip(x, -0.5, 0.5, mode=\"hard\"))\nprint(\"SoftJAX clip (soft mode):\", sj.clip(x, -0.5, 0.5))\n\nprint(\"\\nJAX heaviside:\", jnp.heaviside(x, 0.5))\nprint(\"SoftJAX heaviside (hard mode):\", sj.heaviside(x, mode=\"hard\"))\nprint(\"SoftJAX heaviside (soft mode):\", sj.heaviside(x))\n\nprint(\"\\nJAX ReLU:\", jax.nn.relu(x))\nprint(\"SoftJAX ReLU (hard mode):\", sj.relu(x, mode=\"hard\"))\nprint(\"SoftJAX ReLU (soft mode):\", sj.relu(x))\n\nprint(\"\\nJAX round:\", jnp.round(x))\nprint(\"SoftJAX round (hard mode):\", sj.round(x, mode=\"hard\"))\nprint(\"SoftJAX round (soft mode):\", sj.round(x))\n\nprint(\"\\nJAX sign:\", jnp.sign(x))\nprint(\"SoftJAX sign (hard mode):\", sj.sign(x, mode=\"hard\"))\nprint(\"SoftJAX sign (soft mode):\", sj.sign(x))\n</pre> x = jnp.array([-0.2, -1.0, 0.3, 1.0])  # Elementwise operators print(\"\\nJAX absolute:\", jnp.abs(x)) print(\"SoftJAX absolute (hard mode):\", sj.abs(x, mode=\"hard\")) print(\"SoftJAX absolute (soft mode):\", sj.abs(x))  print(\"\\nJAX clip:\", jnp.clip(x, -0.5, 0.5)) print(\"SoftJAX clip (hard mode):\", sj.clip(x, -0.5, 0.5, mode=\"hard\")) print(\"SoftJAX clip (soft mode):\", sj.clip(x, -0.5, 0.5))  print(\"\\nJAX heaviside:\", jnp.heaviside(x, 0.5)) print(\"SoftJAX heaviside (hard mode):\", sj.heaviside(x, mode=\"hard\")) print(\"SoftJAX heaviside (soft mode):\", sj.heaviside(x))  print(\"\\nJAX ReLU:\", jax.nn.relu(x)) print(\"SoftJAX ReLU (hard mode):\", sj.relu(x, mode=\"hard\")) print(\"SoftJAX ReLU (soft mode):\", sj.relu(x))  print(\"\\nJAX round:\", jnp.round(x)) print(\"SoftJAX round (hard mode):\", sj.round(x, mode=\"hard\")) print(\"SoftJAX round (soft mode):\", sj.round(x))  print(\"\\nJAX sign:\", jnp.sign(x)) print(\"SoftJAX sign (hard mode):\", sj.sign(x, mode=\"hard\")) print(\"SoftJAX sign (soft mode):\", sj.sign(x)) <pre>\nJAX absolute: [0.2 1.  0.3 1. ]\nSoftJAX absolute (hard mode): [0.2 1.  0.3 1. ]\nSoftJAX absolute (soft mode): [0.1523 0.9999 0.2715 0.9999]\n\nJAX clip: [-0.2 -0.5  0.3  0.5]\nSoftJAX clip (hard mode): [-0.2 -0.5  0.3  0.5]\n</pre> <pre>SoftJAX clip (soft mode): [-0.1952 -0.4993  0.2873  0.4993]\n\nJAX heaviside: [0. 0. 1. 1.]\nSoftJAX heaviside (hard mode): [0. 0. 1. 1.]\nSoftJAX heaviside (soft mode): [0.1192 0.     0.9526 1.    ]\n\nJAX ReLU: [0.  0.  0.3 1. ]\nSoftJAX ReLU (hard mode): [0.  0.  0.3 1. ]\nSoftJAX ReLU (soft mode): [0.0127 0.     0.3049 1.    ]\n</pre> <pre>\nJAX round: [-0. -1.  0.  1.]\nSoftJAX round (hard mode): [-0. -1.  0.  1.]\n</pre> <pre>SoftJAX round (soft mode): [-0.0465 -1.      0.1189  1.    ]\n\nJAX sign: [-1. -1.  1.  1.]\nSoftJAX sign (hard mode): [-1. -1.  1.  1.]\nSoftJAX sign (soft mode): [-0.7616 -0.9999  0.9051  0.9999]\n</pre> In\u00a0[3]: Copied! <pre># Array-valued operators\nprint(\"\\nJAX max:\", jnp.max(x))\nprint(\"SoftJAX max (hard mode):\", sj.max(x, mode=\"hard\"))\nprint(\"SoftJAX max (soft mode):\", sj.max(x))\n\nprint(\"\\nJAX min:\", jnp.min(x))\nprint(\"SoftJAX min (hard mode):\", sj.min(x, mode=\"hard\"))\nprint(\"SoftJAX min (soft mode):\", sj.min(x))\n\nprint(\"\\nJAX sort:\", jnp.sort(x))\nprint(\"SoftJAX sort (hard mode):\", sj.sort(x, mode=\"hard\"))\nprint(\"SoftJAX sort (soft mode):\", sj.sort(x))\n\nprint(\"\\nJAX median:\", jnp.median(x))\nprint(\"SoftJAX median (hard mode):\", sj.median(x, mode=\"hard\"))\nprint(\"SoftJAX median (soft mode):\", sj.median(x))\n\nprint(\"\\nJAX top_k:\", jax.lax.top_k(x, k=3)[0])\nprint(\"SoftJAX top_k (hard mode):\", sj.top_k(x, k=3, mode=\"hard\")[0])\nprint(\"SoftJAX top_k (soft mode):\", sj.top_k(x, k=3)[0])\n\nprint(\"\\nJAX ranking:\", jnp.argsort(jnp.argsort(x)))\nprint(\"SoftJAX ranking (hard mode):\", sj.ranking(x, mode=\"hard\", descending=False))\nprint(\"SoftJAX ranking (soft mode):\", sj.ranking(x, descending=False))\n</pre> # Array-valued operators print(\"\\nJAX max:\", jnp.max(x)) print(\"SoftJAX max (hard mode):\", sj.max(x, mode=\"hard\")) print(\"SoftJAX max (soft mode):\", sj.max(x))  print(\"\\nJAX min:\", jnp.min(x)) print(\"SoftJAX min (hard mode):\", sj.min(x, mode=\"hard\")) print(\"SoftJAX min (soft mode):\", sj.min(x))  print(\"\\nJAX sort:\", jnp.sort(x)) print(\"SoftJAX sort (hard mode):\", sj.sort(x, mode=\"hard\")) print(\"SoftJAX sort (soft mode):\", sj.sort(x))  print(\"\\nJAX median:\", jnp.median(x)) print(\"SoftJAX median (hard mode):\", sj.median(x, mode=\"hard\")) print(\"SoftJAX median (soft mode):\", sj.median(x))  print(\"\\nJAX top_k:\", jax.lax.top_k(x, k=3)[0]) print(\"SoftJAX top_k (hard mode):\", sj.top_k(x, k=3, mode=\"hard\")[0]) print(\"SoftJAX top_k (soft mode):\", sj.top_k(x, k=3)[0])  print(\"\\nJAX ranking:\", jnp.argsort(jnp.argsort(x))) print(\"SoftJAX ranking (hard mode):\", sj.ranking(x, mode=\"hard\", descending=False)) print(\"SoftJAX ranking (soft mode):\", sj.ranking(x, descending=False)) <pre>\nJAX max: 1.0\nSoftJAX max (hard mode): 1.0\n</pre> <pre>SoftJAX max (soft mode): 0.9993548976691374\n</pre> <pre>\nJAX min: -1.0\nSoftJAX min (hard mode): -1.0\nSoftJAX min (soft mode): -0.9997287789452775\n\nJAX sort: [-1.  -0.2  0.3  1. ]\nSoftJAX sort (hard mode): [-1.  -0.2  0.3  1. ]\n</pre> <pre>SoftJAX sort (soft mode): [-0.9997 -0.1969  0.2973  0.9994]\n\nJAX median: 0.04999999999999999\nSoftJAX median (hard mode): 0.04999999999999999\n</pre> <pre>SoftJAX median (soft mode): 0.05000033589501627\n\nJAX top_k: [ 1.   0.3 -0.2]\nSoftJAX top_k (hard mode): [ 1.   0.3 -0.2]\n</pre> <pre>SoftJAX top_k (soft mode): [ 0.9994  0.2973 -0.1969]\n\nJAX ranking: [1 0 2 3]\nSoftJAX ranking (hard mode): [1. 0. 2. 3.]\n</pre> <pre>SoftJAX ranking (soft mode): [1.0064 0.0003 1.9942 2.9991]\n</pre> In\u00a0[4]: Copied! <pre># Operators returning indices\nprint(\"\\nJAX argmax:\", jnp.argmax(x))\nprint(\"SoftJAX argmax (hard mode):\", sj.argmax(x, mode=\"hard\"))\nprint(\"SoftJAX argmax (soft mode):\", sj.argmax(x))\n\nprint(\"\\nJAX argmin:\", jnp.argmin(x))\nprint(\"SoftJAX argmin (hard mode):\", sj.argmin(x, mode=\"hard\"))\nprint(\"SoftJAX argmin (soft mode):\", sj.argmin(x))\n\nprint(\"\\nJAX argmedian:\", \"Not implemented in standard JAX\")\nprint(\"SoftJAX argmedian (hard mode):\", sj.argmedian(x, mode=\"hard\"))\nprint(\"SoftJAX argmedian (soft mode):\", sj.argmedian(x))\n\nprint(\"\\nJAX argsort:\", jnp.argsort(x))\nprint(\"SoftJAX argsort (hard mode):\", sj.argsort(x, mode=\"hard\"))\nprint(\"SoftJAX argsort (soft mode):\", sj.argsort(x))\n\nprint(\"\\nJAX argtop_k:\", jax.lax.top_k(x, k=3)[1])\nprint(\"SoftJAX argtop_k (hard mode):\", sj.top_k(x, k=3, mode=\"hard\")[1])\nprint(\"SoftJAX argtop_k (soft mode):\", sj.top_k(x, k=3)[1])\n</pre> # Operators returning indices print(\"\\nJAX argmax:\", jnp.argmax(x)) print(\"SoftJAX argmax (hard mode):\", sj.argmax(x, mode=\"hard\")) print(\"SoftJAX argmax (soft mode):\", sj.argmax(x))  print(\"\\nJAX argmin:\", jnp.argmin(x)) print(\"SoftJAX argmin (hard mode):\", sj.argmin(x, mode=\"hard\")) print(\"SoftJAX argmin (soft mode):\", sj.argmin(x))  print(\"\\nJAX argmedian:\", \"Not implemented in standard JAX\") print(\"SoftJAX argmedian (hard mode):\", sj.argmedian(x, mode=\"hard\")) print(\"SoftJAX argmedian (soft mode):\", sj.argmedian(x))  print(\"\\nJAX argsort:\", jnp.argsort(x)) print(\"SoftJAX argsort (hard mode):\", sj.argsort(x, mode=\"hard\")) print(\"SoftJAX argsort (soft mode):\", sj.argsort(x))  print(\"\\nJAX argtop_k:\", jax.lax.top_k(x, k=3)[1]) print(\"SoftJAX argtop_k (hard mode):\", sj.top_k(x, k=3, mode=\"hard\")[1]) print(\"SoftJAX argtop_k (soft mode):\", sj.top_k(x, k=3)[1]) <pre>\nJAX argmax: 3\nSoftJAX argmax (hard mode): [0. 0. 0. 1.]\nSoftJAX argmax (soft mode): [0.     0.     0.0009 0.9991]\n\nJAX argmin: 1\nSoftJAX argmin (hard mode): [0. 1. 0. 0.]\nSoftJAX argmin (soft mode): [0.0003 0.9997 0.     0.    ]\n\nJAX argmedian: Not implemented in standard JAX\n</pre> <pre>SoftJAX argmedian (hard mode): [0.5 0.  0.5 0. ]\nSoftJAX argmedian (soft mode): [0.5 0.  0.5 0. ]\n\nJAX argsort: [1 0 2 3]\nSoftJAX argsort (hard mode): [[0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]]\nSoftJAX argsort (soft mode): [[0.0003 0.9997 0.     0.    ]\n [0.993  0.0003 0.0067 0.    ]\n [0.0067 0.     0.9924 0.0009]\n [0.     0.     0.0009 0.9991]]\n\nJAX argtop_k: [3 2 0]\nSoftJAX argtop_k (hard mode): [[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]]\nSoftJAX argtop_k (soft mode): [[0.     0.     0.0009 0.9991]\n [0.0067 0.     0.9924 0.0009]\n [0.993  0.0003 0.0067 0.    ]]\n</pre> In\u00a0[5]: Copied! <pre>y = jnp.array([0.2, -0.5, 0.5, -1.0])\n\n# Comparison operators\nprint(\"\\nJAX greater:\", jnp.greater(x, y))\nprint(\"SoftJAX greater (hard mode):\", sj.greater(x, y, mode=\"hard\"))\nprint(\"SoftJAX greater (soft mode):\", sj.greater(x, y))\n\nprint(\"\\nJAX greater equal:\", jnp.greater_equal(x, y))\nprint(\"SoftJAX greater equal (hard mode):\", sj.greater_equal(x, y, mode=\"hard\"))\nprint(\"SoftJAX greater equal (soft mode):\", sj.greater_equal(x, y))\n\nprint(\"\\nJAX less:\", jnp.less(x, y))\nprint(\"SoftJAX less (hard mode):\", sj.less(x, y, mode=\"hard\"))\nprint(\"SoftJAX less (soft mode):\", sj.less(x, y))\n\nprint(\"\\nJAX less equal:\", jnp.less_equal(x, y))\nprint(\"SoftJAX less equal (hard mode):\", sj.less_equal(x, y, mode=\"hard\"))\nprint(\"SoftJAX less equal (soft mode):\", sj.less_equal(x, y))\n\nprint(\"\\nJAX equal:\", jnp.equal(x, y))\nprint(\"SoftJAX equal (hard mode):\", sj.equal(x, y, mode=\"hard\"))\nprint(\"SoftJAX equal (soft mode):\", sj.equal(x, y))\n\nprint(\"\\nJAX not equal:\", jnp.not_equal(x, y))\nprint(\"SoftJAX not equal (hard mode):\", sj.not_equal(x, y, mode=\"hard\"))\nprint(\"SoftJAX not equal (soft mode):\", sj.not_equal(x, y))\n\nprint(\"\\nJAX isclose:\", jnp.isclose(x, y))\nprint(\"SoftJAX isclose (hard mode):\", sj.isclose(x, y, mode=\"hard\"))\nprint(\"SoftJAX isclose (soft mode):\", sj.isclose(x, y))\n</pre> y = jnp.array([0.2, -0.5, 0.5, -1.0])  # Comparison operators print(\"\\nJAX greater:\", jnp.greater(x, y)) print(\"SoftJAX greater (hard mode):\", sj.greater(x, y, mode=\"hard\")) print(\"SoftJAX greater (soft mode):\", sj.greater(x, y))  print(\"\\nJAX greater equal:\", jnp.greater_equal(x, y)) print(\"SoftJAX greater equal (hard mode):\", sj.greater_equal(x, y, mode=\"hard\")) print(\"SoftJAX greater equal (soft mode):\", sj.greater_equal(x, y))  print(\"\\nJAX less:\", jnp.less(x, y)) print(\"SoftJAX less (hard mode):\", sj.less(x, y, mode=\"hard\")) print(\"SoftJAX less (soft mode):\", sj.less(x, y))  print(\"\\nJAX less equal:\", jnp.less_equal(x, y)) print(\"SoftJAX less equal (hard mode):\", sj.less_equal(x, y, mode=\"hard\")) print(\"SoftJAX less equal (soft mode):\", sj.less_equal(x, y))  print(\"\\nJAX equal:\", jnp.equal(x, y)) print(\"SoftJAX equal (hard mode):\", sj.equal(x, y, mode=\"hard\")) print(\"SoftJAX equal (soft mode):\", sj.equal(x, y))  print(\"\\nJAX not equal:\", jnp.not_equal(x, y)) print(\"SoftJAX not equal (hard mode):\", sj.not_equal(x, y, mode=\"hard\")) print(\"SoftJAX not equal (soft mode):\", sj.not_equal(x, y))  print(\"\\nJAX isclose:\", jnp.isclose(x, y)) print(\"SoftJAX isclose (hard mode):\", sj.isclose(x, y, mode=\"hard\")) print(\"SoftJAX isclose (soft mode):\", sj.isclose(x, y)) <pre>\nJAX greater: [False False False  True]\nSoftJAX greater (hard mode): [0. 0. 0. 1.]\nSoftJAX greater (soft mode): [0.018  0.0067 0.1192 1.    ]\n\nJAX greater equal: [False False False  True]\nSoftJAX greater equal (hard mode): [0. 0. 0. 1.]\nSoftJAX greater equal (soft mode): [0.018  0.0067 0.1192 1.    ]\n\nJAX less: [ True  True  True False]\nSoftJAX less (hard mode): [1. 1. 1. 0.]\nSoftJAX less (soft mode): [0.982  0.9933 0.8808 0.    ]\n</pre> <pre>\nJAX less equal: [ True  True  True False]\nSoftJAX less equal (hard mode): [1. 1. 1. 0.]\nSoftJAX less equal (soft mode): [0.982  0.9933 0.8808 0.    ]\n\nJAX equal: [False False False False]\nSoftJAX equal (hard mode): [0. 0. 0. 0.]\n</pre> <pre>SoftJAX equal (soft mode): [0.018  0.0067 0.1192 0.    ]\n\nJAX not equal: [ True  True  True  True]\nSoftJAX not equal (hard mode): [1. 1. 1. 1.]\nSoftJAX not equal (soft mode): [0.982  0.9933 0.8808 1.    ]\n\nJAX isclose: [False False False False]\nSoftJAX isclose (hard mode): [0. 0. 0. 0.]\nSoftJAX isclose (soft mode): [0.018  0.0067 0.1192 0.    ]\n</pre> In\u00a0[6]: Copied! <pre># Logical operators\nfuzzy_a = jnp.array([0.1, 0.2, 0.8, 1.0])\nfuzzy_b = jnp.array([0.7, 0.3, 0.1, 0.9])\nbool_a = fuzzy_a &gt;= 0.5\nbool_b = fuzzy_b &gt;= 0.5\n\nprint(\"\\nJAX AND:\", jnp.logical_and(bool_a, bool_b))\nprint(\"SoftJAX AND:\", sj.logical_and(fuzzy_a, fuzzy_b))\n\nprint(\"\\nJAX OR:\", jnp.logical_or(bool_a, bool_b))\nprint(\"SoftJAX OR:\", sj.logical_or(fuzzy_a, fuzzy_b))\n\nprint(\"\\nJAX NOT:\", jnp.logical_not(bool_a))\nprint(\"SoftJAX NOT:\", sj.logical_not(fuzzy_a))\n\nprint(\"\\nJAX XOR:\", jnp.logical_xor(bool_a, bool_b))\nprint(\"SoftJAX XOR:\", sj.logical_xor(fuzzy_a, fuzzy_b))\n\nprint(\"\\nJAX ALL:\", jnp.all(bool_a))\nprint(\"SoftJAX ALL:\", sj.all(fuzzy_a))\n\nprint(\"\\nJAX ANY:\", jnp.any(bool_a))\nprint(\"SoftJAX ANY:\", sj.any(fuzzy_a))\n\n# Selection operators\nprint(\"\\nJAX Where:\", jnp.where(bool_a, x, y))\nprint(\"SoftJAX Where:\", sj.where(fuzzy_a, x, y))\n</pre> # Logical operators fuzzy_a = jnp.array([0.1, 0.2, 0.8, 1.0]) fuzzy_b = jnp.array([0.7, 0.3, 0.1, 0.9]) bool_a = fuzzy_a &gt;= 0.5 bool_b = fuzzy_b &gt;= 0.5  print(\"\\nJAX AND:\", jnp.logical_and(bool_a, bool_b)) print(\"SoftJAX AND:\", sj.logical_and(fuzzy_a, fuzzy_b))  print(\"\\nJAX OR:\", jnp.logical_or(bool_a, bool_b)) print(\"SoftJAX OR:\", sj.logical_or(fuzzy_a, fuzzy_b))  print(\"\\nJAX NOT:\", jnp.logical_not(bool_a)) print(\"SoftJAX NOT:\", sj.logical_not(fuzzy_a))  print(\"\\nJAX XOR:\", jnp.logical_xor(bool_a, bool_b)) print(\"SoftJAX XOR:\", sj.logical_xor(fuzzy_a, fuzzy_b))  print(\"\\nJAX ALL:\", jnp.all(bool_a)) print(\"SoftJAX ALL:\", sj.all(fuzzy_a))  print(\"\\nJAX ANY:\", jnp.any(bool_a)) print(\"SoftJAX ANY:\", sj.any(fuzzy_a))  # Selection operators print(\"\\nJAX Where:\", jnp.where(bool_a, x, y)) print(\"SoftJAX Where:\", sj.where(fuzzy_a, x, y)) <pre>\nJAX AND: [False False False  True]\n</pre> <pre>SoftJAX AND: [0.2646 0.2449 0.2828 0.9487]\n\nJAX OR: [ True False  True  True]\nSoftJAX OR: [0.4804 0.2517 0.5757 1.    ]\n</pre> <pre>\nJAX NOT: [ True  True False False]\nSoftJAX NOT: [0.9 0.8 0.2 0. ]\n\nJAX XOR: [ True False  True False]\nSoftJAX XOR: [0.587  0.435  0.6394 0.1731]\n\nJAX ALL: False\n</pre> <pre>SoftJAX ALL: 0.35565588200778464\n\nJAX ANY: True\n</pre> <pre>SoftJAX ANY: 0.9980519925071494\n\nJAX Where: [ 0.2 -0.5  0.3  1. ]\nSoftJAX Where: [ 0.16 -0.6   0.34  1.  ]\n</pre> In\u00a0[7]: Copied! <pre># Straight-through operators: Use hard function on forward and soft on backward\nprint(\"Straight-through ReLU:\", sj.relu_st(x))\nprint(\"Straight-through sort:\", sj.sort_st(x))\nprint(\"Straight-through argtop_k:\", sj.top_k_st(x, k=3)[1])\nprint(\"Straight-through greater:\", sj.greater_st(x, y))\n# And many more...\n</pre> # Straight-through operators: Use hard function on forward and soft on backward print(\"Straight-through ReLU:\", sj.relu_st(x)) print(\"Straight-through sort:\", sj.sort_st(x)) print(\"Straight-through argtop_k:\", sj.top_k_st(x, k=3)[1]) print(\"Straight-through greater:\", sj.greater_st(x, y)) # And many more... <pre>Straight-through ReLU: [0.  0.  0.3 1. ]\nStraight-through sort: [-1.  -0.2  0.3  1. ]\n</pre> <pre>Straight-through argtop_k: [[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]]\nStraight-through greater: [0. 0. 0. 1.]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"quick-example/#quick-example","title":"Quick Example\u00b6","text":"<p>This notebook contains the quick examples from the Readme.</p>"},{"location":"_static/Readme/","title":"Readme","text":"<p>The favicon is adapted from <code>cosine-wave</code> from https://materialdesignicons.com, found by way of https://pictogrammers.com. Specifically it has been adapted by filling in the integral with black. (Originally it has 100% alpha.)</p>"},{"location":"api/softjax_operators/","title":"Softjax operators","text":""},{"location":"api/softjax_operators/#elementwise-operators","title":"Elementwise operators","text":""},{"location":"api/softjax_operators/#softjax.abs","title":"<code>softjax.abs(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.abs.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: Projection mode. \"hard\" returns the exact absolute value, otherwise uses     \"entropic\", \"pseudohuber\", \"euclidean\", \"cubic\", or \"quintic\" relaxations.     Defaults to \"entropic\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise absolute value to <code>x</code>.</p>"},{"location":"api/softjax_operators/#softjax.clip","title":"<code>softjax.clip(x: jax.Array, a: jax.Array, b: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'quartic', 'gated_entropic', 'gated_euclidean', 'gated_cubic', 'gated_quintic', 'gated_pseudohuber'] = 'entropic') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.clip.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>a</code>: Lower bound scalar.</li> <li><code>b</code>: Upper bound scalar.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", applies <code>jnp.clip</code>. Otherwise uses \"entropic\", \"euclidean\",     \"quartic\", \"gated_entropic\", \"gated_euclidean\", \"gated_cubic\", \"gated_quintic\",     or \"gated_pseudohuber\" relaxations. Defaults to \"entropic\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise clipping to <code>x</code>.</p>"},{"location":"api/softjax_operators/#softjax.heaviside","title":"<code>softjax.heaviside(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic') -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.heaviside(x,0.5).</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact Heaviside step. Otherwise uses     \"entropic\", \"euclidean\", \"cubic\", or \"quintic\" relaxations. Defaults to \"entropic\".</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> (Array with values in [0, 1]), relaxing the elementwise Heaviside step function.</p>"},{"location":"api/softjax_operators/#softjax.relu","title":"<code>softjax.relu(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'quartic', 'gated_entropic', 'gated_euclidean', 'gated_cubic', 'gated_quintic', 'gated_pseudohuber'] = 'entropic') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.nn.relu.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", applies <code>jax.nn.relu</code>. Otherwise uses \"entropic\", \"euclidean\",     \"quartic\", \"gated_entropic\", \"gated_euclidean\", \"gated_cubic\", \"gated_quintic\",     or \"gated_pseudohuber\" relaxations. Defaults to \"entropic\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise ReLU to <code>x</code>.</p>"},{"location":"api/softjax_operators/#softjax.round","title":"<code>softjax.round(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', neighbor_radius: int = 5) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.round.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", applies <code>jnp.round</code>. Otherwise uses a sigmoid-based relaxation     based on the algorithm described in https://arxiv.org/pdf/2504.19026v1.     This function thereby inherits the different sigmoid modes \"entropic\",     \"euclidean\", \"pseudohuber\", \"cubic\", or \"quintic\".     Defaults to \"entropic\".</li> <li><code>neighbor_radius</code>: Number of neighbors on each side of the floor value to     consider for the soft rounding. Defaults to 5.</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise rounding to <code>x</code>.</p>"},{"location":"api/softjax_operators/#softjax.sign","title":"<code>softjax.sign(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.sign.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns <code>jnp.sign</code>. Otherwise smooths via \"entropic\", \"euclidean\",     \"cubic\", or \"quintic\" relaxations. Defaults to \"entropic\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise sign to <code>x</code>.</p>"},{"location":"api/softjax_operators/#array-valued-operators","title":"Array-valued operators","text":""},{"location":"api/softjax_operators/#softjax.argmax","title":"<code>softjax.argmax(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic') -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.argmax of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the argmax. If None, the input Array is     flattened before computing the argmax. Defaults to None.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: Controls the type of softening:<ul> <li><code>hard</code>: Returns the result of jnp.argmax with a one-hot encoding of     the indices.</li> <li><code>entropic</code>: Returns a softmax-based relaxation of the argmax.</li> <li><code>euclidean</code>: Returns an L2-projection-based relaxation of the argmax.</li> </ul> </li> </ul> <p>Returns:</p> <p>A SoftIndex of shape (..., {1}, ..., [n]) (positive Array which sums to 1 over the last dimension). Represents the probability of an index corresponding to the argmax along the specified axis.</p> <p>Usage</p> <p>This function can be used as a differentiable relaxation to jax.numpy.argmax, enabling backpropagation through index selection steps in neural networks or optimization routines. However, note that the output is not a discrete index but a <code>SoftIndex</code>, which is a distribution over indices. Therefore, functions which operate on indices have to be adjusted accordingly to accept a SoftIndex, see e.g. <code>softjax.max</code> for an example of using <code>softjax.take_along_axis</code> to retrieve the soft maximum value via the <code>SoftIndex</code>.</p> <p>Difference to jax.nn.softmax</p> <p>Note that <code>softjax.argmax</code> in <code>entropic</code> mode is not fully equivalent to jax.nn.softmax because it moves the probability dimension into the last axis (this is a convention in the <code>SoftIndex</code> data type).</p>"},{"location":"api/softjax_operators/#softjax.max","title":"<code>softjax.max(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.max of <code>x</code> along the specified axis. Implemented as <code>softjax.argmax</code> followed by <code>softjax.take_along_axis</code>, see respective documentations for details.</p> <p>Returns:</p> <p>Array of shape (..., {1}, ...) representing the soft maximum of <code>x</code> along the specified axis.</p>"},{"location":"api/softjax_operators/#softjax.argmin","title":"<code>softjax.argmin(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic') -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.argmin of <code>x</code> along the specified axis. Implemented as <code>softjax.argmax</code> on <code>-x</code>, see respective documentation for details.</p>"},{"location":"api/softjax_operators/#softjax.min","title":"<code>softjax.min(x: jax.Array, axis: int | None = None, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', keepdims: bool = False) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.min of <code>x</code> along the specified axis. Implemented as -<code>softjax.max</code> on <code>-x</code>, see respective documentation for details.</p> <p>Returns:</p> <p>Array of shape (..., {1}, ...) representing the soft minimum of <code>x</code> along the specified axis.</p>"},{"location":"api/softjax_operators/#softjax.argmedian","title":"<code>softjax.argmedian(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes the soft argmedian of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the median. If None, the input Array is     flattened before computing the median. Defaults to None.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the behavior of the median:<ul> <li><code>mode=\"hard\"</code>: Returns the result of jnp.median with a one-hot encoding of     the indices. On ties, it returns a uniform distribution over all median     indices.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations).     We adapt the approach in Differentiable Ranks and Sorting     using Optimal Transport and     Differentiable Top-k with Optimal Transport     to the median operation by carefully adjusting the cost matrix and     marginals.     Intuition: There are three \"anchors\", the median is transported onto one     anchor, and all the larger and smaller elements are transported to the other     two anchors, respectively.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via projection onto Birkhoff polytope).</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: This formulation a well-known soft median     operation based on the interpretation of the median as the minimizer of     absolute deviations. The softening is then achieved by replacing the argmax     operator with a softmax. Note, that this also has close ties to the     \"SoftSort\" operator from SoftSort: A Continuous Relaxation for the argsort Operator.     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <p>A SoftIndex of shape (..., {1}, ..., [n]) (positive Array which sums to 1 over the last dimension). The elements in (..., 0, ...) represent a distribution over values in x being the median along the specified axis.</p>"},{"location":"api/softjax_operators/#softjax.median","title":"<code>softjax.median(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jnp.median of <code>x</code> along the specified axis. Implemented as <code>softjax.argmedian</code> followed by <code>softjax.take_along_axis</code>, see respective documentations for details.</p> <p>Returns:</p> <p>An Array of shape (..., {1}, ...), representing the soft median values along the specified axis.</p>"},{"location":"api/softjax_operators/#softjax.median_newton","title":"<code>softjax.median_newton(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'pseudohuber', 'euclidean', 'cubic', 'quintic'] = 'entropic', max_iter: int = 8, eps: float = 1e-12) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.median of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: Axis along which to compute the median. If None, the input is flattened.     Defaults to None.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}. Defaults to     False.</li> <li><code>softness</code>: Softness of the score function, should be larger than zero. Defaults     to 1.0.</li> <li><code>mode</code>: Smooth score choice:<ul> <li><code>hard</code>: Returns <code>jnp.median</code>.</li> <li><code>sigmoid</code>, <code>pseudohuber</code>, <code>linear</code>, <code>cubic</code>, <code>quintic</code>: Smooth     relaxations for the M-estimator using Newton steps. Defaults to <code>sigmoid</code>.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of Newton iterations in the M-estimator.</li> <li><code>eps</code>: Small constant added to the derivative to avoid division by zero.</li> </ul> <p>Returns:</p> <p>Array of shape (..., {1}, ...) representing the soft median of <code>x</code> along the specified axis.</p>"},{"location":"api/softjax_operators/#softjax.argsort","title":"<code>softjax.argsort(x: jax.Array, axis: int | None = None, descending: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.argsort of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the argsort operation. If None, the input     Array is flattened before computing the argsort. Defaults to None.</li> <li><code>descending</code>: If True, sorts in descending order. Defaults to False (ascending).</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the type of softening:<ul> <li><code>mode=\"hard\"</code>: Returns the result of jnp.argsort with a one-hot encoding of     the indices.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations) as in     Differentiable Ranks and Sorting using Optimal Transport.     Intuition: The sorted elements are selected by specifying n \"anchors\"     and then transporting the ith-largest value to the ith-largest anchor.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via LBFGS projection onto Birkhoff polytope) as     in Fast Differentiable Sorting and Ranking.</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: Uses the \"SoftSort\" operator proposed in     SoftSort: A Continuous Relaxation for the argsort Operator.     This initializes the cost matrix based on the absolute difference of <code>x</code> to     the sorted values and then applies a single row normalization (instead of     full Sinkhorn in OT).     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax. To the best of our     knowledge this variant is novel.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <p>A SoftIndex of shape (..., n, ..., [n]) (positive Array which sums to 1 over the last dimension). The elements in (..., i, ..., [n]) represent a distribution over values in x for the ith smallest element along the specified axis.</p> <p>Computing the expectation</p> <p>Computing the soft sorted values means taking the expectation of <code>x</code> under the SoftIndex distribution. Similar to how with normal indices you would do     <pre><code>sorted_x = jnp.take_along_axis(x, indices, axis=axis)\n</code></pre> we offer the equivalent soft version via     <pre><code>soft_sorted_x = sj.take_along_axis(x, soft_index, axis=axis)\n</code></pre> This is what is done in <code>softjax.sort</code>.</p>"},{"location":"api/softjax_operators/#softjax.sort","title":"<code>softjax.sort(x: jax.Array, axis: int | None = None, descending: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.sort of <code>x</code> along the specified axis. Implemented as <code>softjax.argsort</code> followed by <code>softjax.take_along_axis</code>, see respective documentations for details.</p> <p>Returns:</p> <p>Array of shape (..., n, ...) representing the soft sorted values of <code>x</code> along the specified axis.</p>"},{"location":"api/softjax_operators/#softjax.top_k","title":"<code>softjax.top_k(x: jax.Array, k: int, axis: int = -1, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000) -&gt; tuple[jax.Array, Float[Array, '...']]</code> <code></code>","text":"<p>Performs a soft version of jax.lax.top_k of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>k</code>: The number of top elements to select.</li> <li><code>axis</code>: The axis along which to compute the top_k operation. Defaults to -1.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the type of softening:<ul> <li><code>mode=\"hard\"</code>: Returns the result of jax.lax.top_k with a one-hot encoding of     the indices.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations) as in     Differentiable Top-k with Optimal Transport.     Intuition: The top-k elements are selected by specifying k+1 \"anchors\"     and then transporting the top_k values to the top k anchors, and the     remaining (n-k) values to the last anchor.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via projection onto Birkhoff polytope).     This version combines the approaches in Fast Differentiable Sorting and Ranking     (L2 regularizer for sorting) and Differentiable Top-k with Optimal Transport     (entropic regularizer for top-k).</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: Uses the \"SoftSort\" operator proposed  in     SoftSort: A Continuous Relaxation for the argsort Operator.     This initializes the cost matrix based on the absolute difference of <code>x</code> to     the sorted values and then applies a single row normalization (instead of     full Sinkhorn in OT).     Because this is very fast we do a full soft argsort and then take the top-k     elements.     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax. To the best of our     knowledge this variant is novel.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>soft_values</code>: Top-k values of <code>x</code>, shape (..., k, ...).</li> <li><code>soft_index</code>: SoftIndex of shape (..., k, ..., [n]) (positive Array which sums     to 1 over the last dimension). Represents the soft indices of the top-k values.</li> </ul>"},{"location":"api/softjax_operators/#softjax.ranking","title":"<code>softjax.ranking(x: jax.Array, axis: int | None = None, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000, descending: bool = True) -&gt; jax.Array</code> <code></code>","text":"<p>Computes the soft rankings of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the ranking operation. If None, the input     Array is flattened before computing the ranking. Defaults to None.</li> <li><code>descending</code>: If True, larger inputs receive smaller ranks (best rank is 0). If     False, ranks increase with the input values.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the behavior of the ranking     operation:<ul> <li><code>mode=\"hard\"</code>: Returns ranking computed as two jnp.argsort calls.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations) as in     Differentiable Ranks and Sorting using Optimal Transport.     Intuition: We can use the transportation plan obtained in soft sorting for     ranking by transporting the sorted ranks (0, 1, ..., n-1) back to the     ranks of the original values.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via projection onto Birkhoff polytope) as in     Fast Differentiable Sorting and Ranking.</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: Uses an adaptation of the \"SoftSort\"     operator proposed in SoftSort: A Continuous Relaxation for the argsort Operator.     This initializes the cost matrix based on the absolute difference of <code>x</code> to     the sorted values and then we crucially apply a single column     normalization (instead of of row normalization in the original paper).     This makes the resulting matrix a unimodal column stochastic matrix which is     better suited for soft ranking.     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax. To the best of our     knowledge this variant is novel.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <p>A positive Array of shape (..., n, ...) with values in [0, n-1]. The elements in (..., i, ...) represent the soft rank of the ith element along the specified axis.</p>"},{"location":"api/softjax_operators/#comparison-operators","title":"Comparison operators","text":""},{"location":"api/softjax_operators/#softjax.greater","title":"<code>softjax.greater(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &gt; y</code>. Uses a Heaviside relaxation so the output approaches 0 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, greater returns 0 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &gt; y</code>.</p>"},{"location":"api/softjax_operators/#softjax.greater_equal","title":"<code>softjax.greater_equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &gt;= y</code>. Uses a Heaviside relaxation so the output approaches 1 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, greater_equal returns 1 at     equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &gt;= y</code>.</p>"},{"location":"api/softjax_operators/#softjax.less","title":"<code>softjax.less(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &lt; y</code>. Uses a Heaviside relaxation so the output approaches 0 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, less returns 0 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &lt; y</code>.</p>"},{"location":"api/softjax_operators/#softjax.less_equal","title":"<code>softjax.less_equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &lt;= y</code>. Uses a Heaviside relaxation so the output approaches 1 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, less_equal returns 1 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &lt;= y</code>.</p>"},{"location":"api/softjax_operators/#softjax.equal","title":"<code>softjax.equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x == y</code>. Implemented as a soft <code>abs(x - y) &lt;= 0</code> comparison.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, equal returns 1 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x == y</code>.</p>"},{"location":"api/softjax_operators/#softjax.not_equal","title":"<code>softjax.not_equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x != y</code>. Implemented as a soft <code>abs(x - y) &gt; 0</code> comparison.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, not_equal returns 0 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x != y</code>.</p>"},{"location":"api/softjax_operators/#softjax.isclose","title":"<code>softjax.isclose(x: jax.Array, y: jax.Array, softness: float = 1.0, rtol: float = 1e-05, atol: float = 1e-08, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to <code>jnp.isclose</code> for elementwise comparison. Implemented as a soft <code>abs(x - y) &lt;= atol + rtol * abs(y)</code> comparison.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>rtol</code>: Relative tolerance. Defaults to 1e-5.</li> <li><code>atol</code>: Absolute tolerance. Defaults to 1e-8.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, isclose returns 1 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>isclose(x, y)</code>.</p>"},{"location":"api/softjax_operators/#logical-operators","title":"Logical operators","text":""},{"location":"api/softjax_operators/#softjax.logical_and","title":"<code>softjax.logical_and(x: Float[Array, '...'], y: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical AND between two SoftBool Arrays. Fuzzy logic implemented as <code>all(stack([x, y], axis=-1), axis=-1)</code>.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First SoftBool input Array.</li> <li><code>y</code>: Second SoftBool input Array.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise logical AND.</p>"},{"location":"api/softjax_operators/#softjax.logical_not","title":"<code>softjax.logical_not(x: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical NOT of a SoftBool Array. Fuzzy logic implemented as <code>1.0 - x</code>.</p> <p>Arguments: - <code>x</code>: SoftBool input Array.</p> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> (Array with values in [0, 1]), relaxing the elementwise logical NOT.</p>"},{"location":"api/softjax_operators/#softjax.logical_or","title":"<code>softjax.logical_or(x: Float[Array, '...'], y: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical OR between two SoftBool Arrays. Fuzzy logic implemented as <code>any(stack([x, y], axis=-1), axis=-1)</code>.</p> <p>Arguments: - <code>x</code>: First SoftBool input Array. - <code>y</code>: Second SoftBool input Array.</p> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise logical OR.</p>"},{"location":"api/softjax_operators/#softjax.logical_xor","title":"<code>softjax.logical_xor(x: Float[Array, '...'], y: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical XOR between two SoftBool Arrays.</p> <p>Arguments: - <code>x</code>: First SoftBool input Array. - <code>y</code>: Second SoftBool input Array.</p> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise logical XOR.</p>"},{"location":"api/softjax_operators/#softjax.all","title":"<code>softjax.all(x: Float[Array, '...'], axis: int = -1, epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical AND across a specified axis. Fuzzy logic implemented as the geometric mean along the axis.</p> <p>Arguments: - <code>x</code>: SoftBool input Array. - <code>axis</code>: Axis along which to compute the logical AND. Default is -1 (last axis). - <code>epsilon</code>: Minimum value for numerical stability inside the log.</p> <p>Returns:</p> <p>SoftBool (Array with values in [0, 1]) with the specified axis reduced, relaxing the logical ALL along that axis.</p>"},{"location":"api/softjax_operators/#softjax.any","title":"<code>softjax.any(x: Float[Array, '...'], axis: int = -1) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical OR across a specified axis. Fuzzy logic implemented as <code>1.0 - all(logical_not(x), axis=axis)</code>.</p> <p>Arguments: - <code>x</code>: SoftBool input Array. - <code>axis</code>: Axis along which to compute the logical OR. Default is -1 (last axis).</p> <p>Returns:</p> <p>SoftBool (Array with values in [0, 1]) with the specified axis reduced, relaxing t he logical ANY along that axis.</p>"},{"location":"api/softjax_operators/#selection-operators","title":"Selection operators","text":""},{"location":"api/softjax_operators/#softjax.where","title":"<code>softjax.where(condition: Float[Array, '...'], x: jax.Array, y: jax.Array) -&gt; jax.Array</code> <code></code>","text":"<p>Computes a soft elementwise selection between two Arrays based on a SoftBool condition. Fuzzy logic implemented as <code>x * condition + y * (1.0 - condition)</code>.</p> <p>Arguments: - <code>condition</code>: SoftBool condition Array, same shape as <code>x</code> and <code>y</code>. - <code>x</code>: First input Array, same shape as <code>condition</code>. - <code>y</code>: Second input Array, same shape as <code>condition</code>.</p> <p>Returns:</p> <p>Array of the same shape as <code>x</code> and <code>y</code>, interpolating between <code>x</code> and <code>y</code> according to <code>condition</code> in [0, 1].</p>"},{"location":"api/softjax_operators/#softjax.take_along_axis","title":"<code>softjax.take_along_axis(x: jax.Array, soft_index: Float[Array, '...'], axis: int = -1) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.take_along_axis via a weighted dot product.</p> <p>Relation to <code>jnp.take_along_axis</code></p> <pre><code>x = jnp.array([[1, 2, 3], [4, 5, 6]])\n\nindices = jnp.array([[0, 2], [1, 0]])\nprint(jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = jax.nn.one_hot(indices, x.shape[1])\nprint(sj.take_along_axis(x, indices_onehot, axis=1))\n</code></pre> <pre><code>[[1. 3.]\n [5. 4.]]\n[[1. 3.]\n [5. 4.]]\n</code></pre> Interaction with <code>softjax.argmax</code> <pre><code>x = jnp.array([[5, 3, 4], [2, 7, 6]])\n\nindices = jnp.argmin(x, axis=1, keepdims=True)\nprint(\"argmin_jnp:\", jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = sj.argmin(x, axis=1, mode=\"hard\", keepdims=True)\nprint(\"argmin_val_onehot:\", sj.take_along_axis(x, indices_onehot, axis=1))\n\nindices_soft = sj.argmin(x, axis=1, mode=\"entropic\", softness=1.0,\n    keepdims=True)\nprint(\"argmin_val_soft:\", sj.take_along_axis(x, indices_soft, axis=1))\n</code></pre> <pre><code>argmin_jnp: [[3]\n             [2]]\nargmin_val_onehot: [[3.]\n                    [2.]]\nargmin_val_soft: [[3.42478962]\n                  [2.10433824]]\n</code></pre> Interaction with <code>softjax.argsort</code> <pre><code>x = jnp.array([[5, 3, 4], [2, 7, 6]])\n\nindices = jnp.argsort(x, axis=1)\nprint(\"sorted_jnp:\", jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = sj.argsort(x, axis=1, mode=\"hard\")\nprint(\"sorted_sj_hard:\", sj.take_along_axis(x, indices_onehot, axis=1))\n\nindices_soft = sj.argsort(x, axis=1, mode=\"entropic\", softness=1.0)\nprint(\"sorted_sj_soft:\", sj.take_along_axis(x, indices_soft, axis=1))\n</code></pre> <pre><code>sorted_jnp: [[3 4 5]\n             [2 6 7]]\nsorted_sj_hard: [[3. 4. 5.]\n                 [2. 6. 7.]]\nsorted_sj_soft: [[3.2918137  4.         4.7081863 ]\n                 [2.00000045 6.26894107 6.73105858]]\n</code></pre> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_index</code>: A SoftIndex of shape (..., k, ..., [n]) (positive Array which     sums to 1 over the last dimension).</li> <li><code>axis</code>: Axis along which to apply the soft index. Defaults to -1.</li> </ul> <p>Returns:</p> <p>Array of shape (..., k, ...), representing the result after soft selection along the specified axis.</p>"},{"location":"api/softjax_operators/#softjax.take","title":"<code>softjax.take(x: jax.Array, soft_index: Float[Array, '...'], axis: int | None = None) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.take via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_index</code>: A SoftIndex of shape (k, [n]) (positive Array which     sums to 1 over the last dimension).</li> <li><code>axis</code>: Axis along which to apply the soft index. If None, the input is     flattened. Defaults to None.</li> </ul> <p>Returns:</p> <p>Array of shape (..., k, ...) after soft selection.</p>"},{"location":"api/softjax_operators/#softjax.choose","title":"<code>softjax.choose(soft_index: Float[Array, '...'], choices: jax.Array) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.choose via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>soft_index</code>: A SoftIndex of shape (..., [n]) (positive Array which     sums to 1 over the last dimension). Represents the weights for each choice.</li> <li><code>choices</code>: Array of shape (n, ...) supplying the values to mix.</li> </ul> <p>Returns:</p> <p>Array of shape (..., ...) after softly selecting among <code>choices</code>.</p>"},{"location":"api/softjax_operators/#softjax.dynamic_index_in_dim","title":"<code>softjax.dynamic_index_in_dim(x: jax.Array, soft_index: Float[Array, '...'], axis: int = 0, keepdims: bool = True) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.lax.dynamic_index_in_dim via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_index</code>: A SoftIndex of shape ([n],) (positive Array which     sums to 1 over the last dimension).</li> <li><code>axis</code>: Axis along which to apply the soft index. Defaults to 0.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}.</li> </ul> <p>Returns:</p> <p>Array after soft indexing, shape (..., {1}, ...).</p>"},{"location":"api/softjax_operators/#softjax.dynamic_slice_in_dim","title":"<code>softjax.dynamic_slice_in_dim(x: jax.Array, soft_start_index: Float[Array, '...'], slice_size: int, axis: int = 0) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.lax.dynamic_slice_in_dim via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_index</code>: A SoftIndex of shape ([n],) (positive Array which     sums to 1 over the last dimension).</li> <li><code>slice_size</code>: Length of the slice to extract.</li> <li><code>axis</code>: Axis along which to apply the soft slice. Defaults to 0.</li> </ul> <p>Returns:</p> <p>Array of shape (..., slice_size, ...) after soft slicing.</p>"},{"location":"api/softjax_operators/#softjax.dynamic_slice","title":"<code>softjax.dynamic_slice(x: jax.Array, soft_start_indices: Sequence[Float[Array, '...']], slice_sizes: Sequence[int]) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.lax.dynamic_slice via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (n_1, n_2, ..., n_k).</li> <li><code>soft_start_indices</code>: A list of SoftIndices of shape ([n_i],) (positive Arrays     which sums to 1). Sequence of SoftIndex distributions of shapes     ([n_1],), ([n_2],), ..., ([n_k]) each summing to 1.</li> <li><code>slice_sizes</code>: Sequence of slice lengths for each dimension.</li> </ul> <p>Returns:</p> <p>Array of shape (l_1, l_2, ..., l_k) after soft slicing.</p>"},{"location":"api/straight_through/","title":"Straight-through operators","text":""},{"location":"api/straight_through/#straight-through-utility","title":"Straight-through utility","text":""},{"location":"api/straight_through/#softjax.st","title":"<code>softjax.st(fn: typing.Callable) -&gt; typing.Callable</code> <code></code>","text":"<p>This decorator calls the decorated function twice: once with <code>mode=\"hard\"</code> and once with the specified <code>mode</code>. It returns the output from the hard forward pass, but uses the output from the soft backward pass to compute gradients.</p> <p>Arguments:</p> <ul> <li><code>fn</code>: The function to be wrapped. It should accept a <code>mode</code> argument.</li> </ul> <p>Returns:     A wrapped function that behaves like the <code>mode=\"hard\"</code> version during the     forward pass, but computes gradients using the specified <code>mode</code> and <code>softness</code>     during the backward pass.</p>"},{"location":"api/straight_through/#softjax.grad_replace","title":"<code>softjax.grad_replace(fn: typing.Callable) -&gt; typing.Callable</code> <code></code>","text":"<p>This decorator calls the decorated function twice: once with <code>forward=True</code> and once with <code>forward=False</code>. It returns the output from the forward pass, but uses the output from the backward pass to compute gradients.</p> <p>Arguments:</p> <ul> <li><code>fn</code>: The function to be wrapped. It should accept a <code>forward</code> argument     that specifies which computation to perform depending on forward or     backward pass.</li> </ul> <p>Returns:     A wrapped function that behaves like the <code>forward=True</code> version during the     forward pass, but computes gradients using the <code>forward=False</code> version     during the backward pass.</p>"},{"location":"api/straight_through/#elementwise-operators","title":"Elementwise operators","text":""},{"location":"api/straight_through/#softjax.abs_st","title":"<code>softjax.abs_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.clip_st","title":"<code>softjax.clip_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.heaviside_st","title":"<code>softjax.heaviside_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.relu_st","title":"<code>softjax.relu_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.round_st","title":"<code>softjax.round_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.sign_st","title":"<code>softjax.sign_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#array-valued-operators","title":"Array-valued operators","text":""},{"location":"api/straight_through/#softjax.argmax_st","title":"<code>softjax.argmax_st(*args, **kwargs)</code> <code></code>","text":"<p>Straight-through version of <code>softjax.argmax</code>.</p> <p>This function returns the hard <code>argmax</code> during the forward pass, but uses a soft relaxation (controlled by the <code>mode</code> argument) for the backward pass (i.e., gradients are computed through the soft version).</p> <p>Implemented using the <code>softjax.st</code> decorator as <code>st(softjax.argmax)</code>.</p>"},{"location":"api/straight_through/#softjax.max_st","title":"<code>softjax.max_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.argmin_st","title":"<code>softjax.argmin_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.min_st","title":"<code>softjax.min_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.argmedian_st","title":"<code>softjax.argmedian_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.median_st","title":"<code>softjax.median_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.median_newton_st","title":"<code>softjax.median_newton_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.argsort_st","title":"<code>softjax.argsort_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.sort_st","title":"<code>softjax.sort_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.top_k_st","title":"<code>softjax.top_k_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.ranking_st","title":"<code>softjax.ranking_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#comparison-operators","title":"Comparison operators","text":""},{"location":"api/straight_through/#softjax.greater_st","title":"<code>softjax.greater_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.greater_equal_st","title":"<code>softjax.greater_equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.less_st","title":"<code>softjax.less_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.less_equal_st","title":"<code>softjax.less_equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.equal_st","title":"<code>softjax.equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.not_equal_st","title":"<code>softjax.not_equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.isclose_st","title":"<code>softjax.isclose_st(*args, **kwargs)</code> <code></code>","text":""}]}