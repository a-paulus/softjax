{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>Softjax provides soft differentiable drop-in replacements for traditionally non-differentiable functions in JAX, including</p> <ul> <li>simple elementwise functions: <code>abs</code>, <code>relu</code>, <code>clip</code>, <code>sign</code> and <code>round</code>;</li> <li>functions operating on arrays: <code>max</code>, <code>min</code>, <code>median</code>, <code>sort</code>, <code>ranking</code> and <code>top_k</code>;</li> <li>functions returning indices: <code>argmax</code>, <code>argmin</code>, <code>argmedian</code>, <code>argsort</code> and <code>argtop_k</code>;</li> <li>functions returning boolean values such as: <code>greater</code>, <code>equal</code> or <code>isclose</code>;</li> <li>functions for selection with indices such as: <code>take_along_axis</code>, <code>dynamic_index_in_dim</code> and <code>choose</code>;</li> <li>functions for logical manipulation such as: <code>logical_and</code>, <code>all</code> and <code>where</code>.</li> </ul> <p>Many functions offer multiple modes for softening, allowing for e.g. smoothness of the soft function or boundedness of the softened region, depending on the user needs. Moreover, we tightly integrate functionality for deploying functions using straight-through-estimation, where we use non-differentiable functions in the forward pass and their differentiable replacements in the backward pass.</p> <p>The Softjax library is designed to require minimal user effort, by simply replacing the non-differentiable JAX function with the Softjax counterparts. However, keep in mind that special care needs to be taken when using functions operating on indices, as we relax the notion of an index into a distribution over indices, thereby modifying the shape of returned/accepted values.</p>"},{"location":"#installation","title":"Installation","text":"<p>Requires Python 3.10+. <pre><code>pip install softjax\n</code></pre></p>"},{"location":"#quick-example","title":"Quick example","text":"<pre><code>import jax.numpy as jnp\nimport softjax as sj\n\nsoftness = 0.1\nx = jnp.array([-0.1, 0.0, 0.7, 1.8])\ny = jnp.array([0.2, -0.5, 0.5, -1.0])\n\n# Elementwise functions\nprint(\"Hard ReLU:\", jax.nn.relu(x))\nprint(\"Soft ReLU:\", sj.relu(x, softness=softness))\nprint(\"Hard Clip:\", jnp.clip(x, -0.5, 0.5))\nprint(\"Soft Clip:\", sj.clip(x, -0.5, 0.5, softness=softness))\nprint(\"Hard Absolute:\", jnp.abs(x))\nprint(\"Soft Absolute:\", sj.abs(x, softness=softness))\nprint(\"Hard Sign:\", jnp.sign(x))\nprint(\"Soft Sign:\", sj.sign(x, softness=softness))\nprint(\"Hard round:\", jnp.round(x))\nprint(\"Soft round:\", sj.round(x, softness=softness))\n\n# Functions on arrays\nprint(\"Hard max:\", jnp.max(x))\nprint(\"Soft max:\", sj.max(x, softness=softness))\nprint(\"Hard min:\", jnp.min(x))\nprint(\"Soft min:\", sj.min(x, softness=softness))\nprint(\"Hard median:\", jnp.median(x))\nprint(\"Soft median:\", sj.median(x, softness=softness))\nprint(\"Hard top_k:\", jax.lax.top_k(x, k=3)[0])\nprint(\"Soft top_k:\", sj.top_k(x, k=3, softness=softness)[0])\nprint(\"Hard sort:\", jnp.sort(x))\nprint(\"Soft sort:\", sj.sort(x, softness=softness))\nprint(\"Hard ranking:\", jnp.argsort(jnp.argsort(x)))\nprint(\"Soft ranking:\", sj.ranking(x, softness=softness))\n\n# Straight-through estimation: Use hard function on forward and soft on backward\nprint(\"Straight-through sort:\", sj.sort_st(x, softness=softness))\n\n# Functions returning indices\nprint(\"Hard argmax:\", jnp.argmax(x))\nprint(\"Soft argmax:\", sj.argmax(x, softness=softness))\nprint(\"Hard argmin:\", jnp.argmin(x))\nprint(\"Soft argmin:\", sj.argmin(x, softness=softness))\nprint(\"Hard argmedian:\", \"Not implemented in standard JAX\")\nprint(\"Soft argmedian:\", sj.argmedian(x, softness=softness))\nprint(\"Hard argtop_k:\", jax.lax.top_k(x, k=3)[1])\nprint(\"Soft argtop_k:\", sj.top_k(x, k=3, softness=softness)[1])\nprint(\"Hard argsort:\", jnp.argsort(x))\nprint(\"Soft argsort:\", sj.argsort(x, softness=softness))\n\n## SoftBool generation\nprint(\"Hard heaviside:\", jnp.heaviside(x, 0.5))\nprint(\"Soft heaviside:\", sj.heaviside(x, softness=softness))\nprint(\"Hard greater:\", x &gt; y)\nprint(\"Soft greater:\", sj.greater(x, y, softness=softness))\nprint(\"Hard greater equal:\", x &gt;= y)\nprint(\"Soft greater equal:\", sj.greater_equal(x, y, softness=softness))\nprint(\"Hard less:\", x &lt; y)\nprint(\"Soft less:\", sj.less(x, y, softness=softness))\nprint(\"Hard less equal:\", x &lt;= y)\nprint(\"Soft less equal:\", sj.less_equal(x, y, softness=softness))\nprint(\"Hard equal:\", x == y)\nprint(\"Soft equal:\", sj.equal(x, y, softness=softness))\nprint(\"Hard not equal:\", x != y)\nprint(\"Soft not equal:\", sj.not_equal(x, y, softness=softness))\nprint(\"Hard isclose:\", jnp.isclose(x, y))\nprint(\"Soft isclose:\", sj.isclose(x, y, softness=softness))\n\n## SoftBool manipulation\nfuzzy_a = jnp.array([0.1, 0.2, 0.8, 1.0])\nfuzzy_b = jnp.array([0.7, 0.3, 0.1, 0.9])\nprint(\"Soft AND:\", sj.logical_and(fuzzy_a, fuzzy_b))\nprint(\"Soft OR:\", sj.logical_or(fuzzy_a, fuzzy_b))\nprint(\"Soft NOT:\", sj.logical_not(fuzzy_a))\nprint(\"Soft XOR:\", sj.logical_xor(fuzzy_a, fuzzy_b))\nprint(\"Soft ALL:\", sj.all(fuzzy_a))\nprint(\"Soft ANY:\", sj.any(fuzzy_a))\n\n## SoftBool selection\nprint(\"Where:\", sj.where(fuzzy_a, x, y))\n</code></pre> <pre><code>Hard ReLU: [0.  0.  0.7 1.8]\nSoft ReLU: [0.03132617 0.06931472 0.70009115 1.8       ]\nHard Clip: [-0.1  0.   0.5  0.5]\nSoft Clip: [-9.84325757e-02 -3.46944695e-18  4.87307813e-01  4.99999774e-01]\nHard Absolute: [0.1 0.  0.7 1.8]\nSoft Absolute: [0.04621172 0.         0.69872453 1.79999995]\nHard Sign: [-1.  0.  1.  1.]\nSoft Sign: [-0.46211716  0.          0.9981779   0.99999997]\nHard round: [-0.  0.  1.  2.]\nSoft round: [-0.09064511  0.          0.71513653  1.81513653]\nHard max: 1.8\nSoft max: 1.7999815903777097\nHard min: -0.1\nSoft min: -0.07291629800981214\nHard median: 0.35\nSoft median: 0.24772037254528773\nHard top_k: [1.8 0.7 0. ]\nSoft top_k: [ 1.79998159  0.69911281 -0.02640987]\nHard sort: [-0.1  0.   0.7  1.8]\nSoft sort: [-0.0729163  -0.02640987  0.69911281  1.79998159]\nHard ranking: [0 1 2 3]\nSoft ranking: [2.73063414e+00 2.26809603e+00 1.00156413e+00 1.67486891e-05]\nStraight-through sort: [-0.1  0.   0.7  1.8]\nHard argmax: 3\nSoft argmax: [5.60270275e-09 1.52297251e-08 1.67014215e-05 9.99983278e-01]\nHard argmin: 0\nSoft argmin: [7.30879333e-01 2.68875480e-01 2.45182702e-04 4.09496812e-09]\nHard argmedian: Not implemented in standard JAX\nSoft argmedian: [0.23233226 0.38305115 0.38305115 0.00156544]\nHard argtop_k: [3 2 1]\nSoft argtop_k: [[5.60270275e-09 1.52297251e-08 1.67014215e-05 9.99983278e-01]\n [3.35039123e-04 9.10730760e-04 9.98737550e-01 1.66806157e-05]\n [2.68762251e-01 7.30571543e-01 6.66195015e-04 1.11265898e-08]]\nHard argsort: [0 1 2 3]\nSoft argsort: [[7.30879333e-01 2.68875480e-01 2.45182702e-04 4.09496812e-09]\n [2.68762251e-01 7.30571543e-01 6.66195015e-04 1.11265898e-08]\n [3.35039123e-04 9.10730760e-04 9.98737550e-01 1.66806157e-05]\n [5.60270275e-09 1.52297251e-08 1.67014215e-05 9.99983278e-01]]\nHard heaviside: [0.  0.5 1.  1. ]\nSoft heaviside: [0.26894142 0.5        0.99908895 0.99999998]\nHard greater: [False  True  True  True]\nSoft greater: [0.04742587 0.99330715 0.88079708 1.        ]\nHard greater equal: [False  True  True  True]\nSoft greater equal: [0.04742587 0.99330715 0.88079708 1.        ]\nHard less: [ True False False False]\nSoft less: [9.52574127e-01 6.69285092e-03 1.19202922e-01 6.91446900e-13]\nHard less equal: [ True False False False]\nSoft less equal: [9.52574127e-01 6.69285093e-03 1.19202922e-01 6.91446900e-13]\nHard equal: [False False False False]\nSoft equal: [4.74258732e-02 6.69285093e-03 1.19202922e-01 6.91446900e-13]\nHard not equal: [ True  True  True  True]\nSoft not equal: [0.95257413 0.99330715 0.88079708 1.        ]\nHard isclose: [False False False False]\nSoft isclose: [4.74267813e-02 6.69318401e-03 1.19208182e-01 6.91446900e-13]\nSoft AND: [0.26457513 0.24494897 0.28284271 0.9486833 ]\nSoft OR: [0.48038476 0.25166852 0.57573593 0.99999684]\nSoft NOT: [0.9 0.8 0.2 0. ]\nSoft XOR: [0.58702688 0.43498731 0.63937484 0.17309871]\nSoft ALL: 0.3556558820077846\nSoft ANY: 0.9980519925071494\nWhere: [ 0.17 -0.4   0.66  1.8 ]\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If this library helped your academic work, please consider citing:</p> <pre><code>@misc{Softjax2025,\n  author = {Paulus, Anselm and Geist, Rene and Martius, Georg},\n  title = {Softjax},\n  year = {2025},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/a-paulus/softjax}}\n}\n</code></pre> <p>Also consider starring the project on GitHub!</p> <p>Special thanks and credit go to Patrick Kidger for the awesome JAX repositories that served as the basis for the documentation of this project.</p>"},{"location":"#next-steps","title":"Next steps","text":"<p>Have a look at the All of Softjax page.</p>"},{"location":"#feedback","title":"Feedback","text":"<p>This project is still relatively young, if you have any suggestions for improvement or other feedback, please reach out or raise a GitHub issue!</p>"},{"location":"#see-also","title":"See also","text":""},{"location":"#other-libraries-in-the-jax-ecosystem","title":"Other libraries in the JAX ecosystem","text":"<p>Always useful Equinox: neural networks and everything not already in core JAX! jaxtyping: type annotations for shape/dtype of arrays.  </p> <p>Deep learning Optax: first-order gradient (SGD, Adam, ...) optimisers. Orbax: checkpointing (async/multi-host/multi-device). Levanter: scalable+reliable training of foundation models (e.g. LLMs). paramax: parameterizations and constraints for PyTrees.  </p> <p>Scientific computing Optimistix: root finding, minimisation, fixed points, and least squares. Lineax: linear solvers. BlackJAX: probabilistic+Bayesian sampling. sympy2jax: SymPy&lt;-&gt;JAX conversion; train symbolic expressions via gradient descent. PySR: symbolic regression. (Non-JAX honourable mention!)  </p> <p>Awesome JAX Awesome JAX: a longer list of other JAX projects.  </p>"},{"location":"#other-libraries-on-differentiable-programming","title":"Other libraries on differentiable programming","text":"<p>Differentiable sorting, top-k and ranking DiffSort: Differentiable sorting networks in PyTorch. DiffTopK: Differentiable top-k in PyTorch. FastSoftSort: Fast differentiable sorting and ranking in JAX. Differentiable Top-k with Optimal Transport in JAX. SoftSort: Differentiable argsort in PyTorch and TensorFlow.  </p> <p>Other DiffLogic: Differentiable logic gate networks in PyTorch. SmoothOT: Smooth and Sparse Optimal Transport. JaxOpt: Differentiable optimization in JAX.  </p>"},{"location":"#papers-on-differentiable-algorithms","title":"Papers on differentiable algorithms","text":"<p>Softjax builds on / implements various different algoithms for e.g. differentiable <code>argtop_k</code>, <code>sorting</code> and <code>ranking</code>, including:</p> <p>Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application Fast Differentiable Sorting and Ranking. Differentiable Ranks and Sorting using Optimal Transport Differentiable Top-k with Optimal Transport SoftSort: A Continuous Relaxation for the argsort Operator Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances Smooth and Sparse Optimal Transport </p> <p>Please check the API Documentation for implementation details.</p>"},{"location":"all-of-softjax/","title":"All of Softjax","text":"In\u00a0[3]: Copied! <pre>import softjax as sj\n\nplot(sj.relu, modes=[\"softplus\", \"silu\"])\n</pre> import softjax as sj  plot(sj.relu, modes=[\"softplus\", \"silu\"]) <p>SoftJAX provides function surrogates for many other function such as the absolute function.</p> In\u00a0[4]: Copied! <pre>plot(sj.abs, modes=[\"sigmoid\", \"quintic\"])\n</pre> plot(sj.abs, modes=[\"sigmoid\", \"quintic\"]) In\u00a0[5]: Copied! <pre>plot(sj.round, modes=[\"cosine\"])\n</pre> plot(sj.round, modes=[\"cosine\"]) In\u00a0[6]: Copied! <pre>soft_relu = lambda xi: sj.st(sj.relu)(xi, mode=\"softplus\", softness=0.1)\n\nx = jnp.arange(-1, 1, 0.01)\nvalues, grads = jax.vmap(jax.value_and_grad(soft_relu))(x)\nplot_value_and_grad(x, values, grads, label_func=\"ReLU\", label_grad=\"Soft gradient\")\n</pre> soft_relu = lambda xi: sj.st(sj.relu)(xi, mode=\"softplus\", softness=0.1)  x = jnp.arange(-1, 1, 0.01) values, grads = jax.vmap(jax.value_and_grad(soft_relu))(x) plot_value_and_grad(x, values, grads, label_func=\"ReLU\", label_grad=\"Soft gradient\") In\u00a0[7]: Copied! <pre>def relu_prod(x, y):\n    # Standard ReLU product, no softening\n    return jax.nn.relu(x) * jax.nn.relu(y)\n\n\nplot_value_grad_2D(relu_prod)\n</pre> def relu_prod(x, y):     # Standard ReLU product, no softening     return jax.nn.relu(x) * jax.nn.relu(y)   plot_value_grad_2D(relu_prod) <p>A naive approach would be to replace each relu with <code>sj.relu_st</code> independently. However, the resulting function will not provide informative gradients for every input. This is due to the chain rule, in which the gradient flowing through one relu is multiplid by the (forward) output of the other relu. As the forward pass is not smoothed, the gradient will sometimes be multiplied by zero, resulting in no informative gradient.</p> In\u00a0[8]: Copied! <pre>def soft_relu_prod_naive(x, y, mode=\"softplus\", softness=0.1):\n    # Naive straight-through implementation\n    return sj.relu_st(x, mode=mode, softness=softness) * sj.relu_st(\n        y, mode=mode, softness=softness\n    )\n\n\nplot_value_grad_2D(soft_relu_prod_naive)\n</pre> def soft_relu_prod_naive(x, y, mode=\"softplus\", softness=0.1):     # Naive straight-through implementation     return sj.relu_st(x, mode=mode, softness=softness) * sj.relu_st(         y, mode=mode, softness=softness     )   plot_value_grad_2D(soft_relu_prod_naive) <p>An alternative approach for softening this function is to apply the straight through trick on the outer level as illustrated below. When applied on the outer level, the forward pass computes the hard product of ReLUs as before, whereas the backward pass differentiates through the product of smooth relus.</p> In\u00a0[9]: Copied! <pre>@sj.st\ndef soft_relu_prod_custom_st(x, y, mode=\"softplus\", softness=0.1):\n    # Custom straight-through implementation\n    return sj.relu(x, mode=mode, softness=softness) * sj.relu(\n        y, mode=mode, softness=softness\n    )\n\n\nplot_value_grad_2D(soft_relu_prod_custom_st)\n</pre> @sj.st def soft_relu_prod_custom_st(x, y, mode=\"softplus\", softness=0.1):     # Custom straight-through implementation     return sj.relu(x, mode=mode, softness=softness) * sj.relu(         y, mode=mode, softness=softness     )   plot_value_grad_2D(soft_relu_prod_custom_st) <p>We observe that as expected, this version of the function now also produces informative gradients in the third quadrant. In the simple above example, the only both <code>sj.relu</code> functions take the same parameters, therefore it was easy to just apply the <code>sj.st</code> decorator again.</p> <p>In general, we might want to define custom behavior. This can be implemented by using the <code>@sj.grad_replace</code> decorator, which allows custom control flow conditioned on a <code>forward</code> boolean variable. The function will then execute with <code>forward=True</code> on the forward pass and <code>forward=False</code> on the backward pass.</p> <pre>def grad_replace(fn: Callable) -&gt; Callable:\n    def wrapped(*args, **kwargs):\n        fw_y = fn(*args, **kwargs, forward=True)\n        bw_y = fn(*args, **kwargs, forward=False)\n        return jtu.tree_map(lambda fw, bw: jax.lax.stop_gradient(fw - bw) + bw, fw_y, bw_y)\n    return wrapped\n</pre> In\u00a0[10]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(2, 10))\nbool_array = jax.numpy.greater_equal(x, 0.5)\n\n\ndef boolean_loss(x):\n    return jax.numpy.greater_equal(x, 0.5).sum().astype(\"float32\")\n\n\nboolean_grads = jax.grad(boolean_loss)(x)\n\nplot_array(x, title=\"x\")\nplot_array(bool_array, title=\"jax.numpy.greater_equal(x, 0.5)\")\nplot_array(boolean_grads, title=\"jax.grad(jax.numpy.greater_equal(x, 0.5).sum())(x)\")\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(2, 10)) bool_array = jax.numpy.greater_equal(x, 0.5)   def boolean_loss(x):     return jax.numpy.greater_equal(x, 0.5).sum().astype(\"float32\")   boolean_grads = jax.grad(boolean_loss)(x)  plot_array(x, title=\"x\") plot_array(bool_array, title=\"jax.numpy.greater_equal(x, 0.5)\") plot_array(boolean_grads, title=\"jax.grad(jax.numpy.greater_equal(x, 0.5).sum())(x)\") <p>Example - <code>softjax.greater_equal_st</code> yields useful gradients: Instead of <code>jax.numpy.greater_equal</code>, let's use <code>softjax.greater_equal_st</code> (straight_through variant of <code>softjax.greater_equal</code>). As shown below, thanks to straight-through estimation, <code>softjax.greater_equal_st</code> yields exact Booleans while the gradient of the Boolean loss points in informative directions.</p> In\u00a0[11]: Copied! <pre>def soft_greater_equal_st(x, y):\n    return sj.greater_equal_st(x, y, mode=\"sigmoid\", softness=0.1)\n\n\ndef soft_boolean_loss(x):\n    return soft_greater_equal_st(x, 0.5).sum()\n\n\nx = jax.random.uniform(jax.random.key(0), shape=(2, 10))\nbool_array = soft_greater_equal_st(x, 0.5)\nboolean_grads = jax.grad(soft_boolean_loss)(x)\n\nplot_array(x, title=\"x\")\nplot_array(bool_array, title=\"soft_greater_equal_st(x, 0.5)\")\nplot_array(boolean_grads, title=\"jax.grad(soft_greater_equal_st(x, 0.5).sum())(x)\")\n</pre> def soft_greater_equal_st(x, y):     return sj.greater_equal_st(x, y, mode=\"sigmoid\", softness=0.1)   def soft_boolean_loss(x):     return soft_greater_equal_st(x, 0.5).sum()   x = jax.random.uniform(jax.random.key(0), shape=(2, 10)) bool_array = soft_greater_equal_st(x, 0.5) boolean_grads = jax.grad(soft_boolean_loss)(x)  plot_array(x, title=\"x\") plot_array(bool_array, title=\"soft_greater_equal_st(x, 0.5)\") plot_array(boolean_grads, title=\"jax.grad(soft_greater_equal_st(x, 0.5).sum())(x)\") <pre>/home/local_apaulus/Projects/softjax/.venv/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:125: UserWarning: Explicitly requested dtype float64 requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n</pre> In\u00a0[12]: Copied! <pre>plot(sj.heaviside, modes=[\"sigmoid\", \"linear\", \"quintic\"])\n</pre> plot(sj.heaviside, modes=[\"sigmoid\", \"linear\", \"quintic\"]) <p>In the above case, the <code>linear</code> and <code>quintic</code> relaxations have the advantage of altering the original function only in a bounded region, a property that can be desireble in some cases.</p> <p>Given the concept of a <code>SoftBool</code>, a probabilistic surrogate for binary logical operations such as <code>jax.numpy.equal</code> and <code>jax.numpy.greater</code> is obtained by simply shifting the sigmoid.</p> <p>Example - Greater operator: <code>sj.greater(x,y)</code> corresponds to shifting <code>sj.heaviside</code> by <code>y</code> to the right. The output can be interpreted as the probability $P(x \\geq y)\\in[0,1]$ with $x\\in\\mathbb{R}$ and $y\\in\\mathbb{R}$.</p> In\u00a0[13]: Copied! <pre>def greater_than_1(x, mode=\"sigmoid\", softness=0.1):\n    return sj.greater(x, y=jnp.array(1.0), mode=mode, softness=softness)\n\n\nplot(greater_than_1, modes=[\"sigmoid\", \"quintic\"])\n</pre> def greater_than_1(x, mode=\"sigmoid\", softness=0.1):     return sj.greater(x, y=jnp.array(1.0), mode=mode, softness=softness)   plot(greater_than_1, modes=[\"sigmoid\", \"quintic\"]) In\u00a0[14]: Copied! <pre>def not_greater_equal_st(x):\n    return sj.logical_not(sj.greater_equal_st(x, y=0.5, mode=\"sigmoid\", softness=0.1))\n\n\nx = jnp.arange(-1, 1, 0.01)\nvalues, grads = jax.vmap(jax.value_and_grad(not_greater_equal_st))(x)\nplot_value_and_grad(x, values, grads, label_func=\"not_greater_equal_st\")\n</pre> def not_greater_equal_st(x):     return sj.logical_not(sj.greater_equal_st(x, y=0.5, mode=\"sigmoid\", softness=0.1))   x = jnp.arange(-1, 1, 0.01) values, grads = jax.vmap(jax.value_and_grad(not_greater_equal_st))(x) plot_value_and_grad(x, values, grads, label_func=\"not_greater_equal_st\") <p>Example - Logical AND: Given two <code>SoftBools</code> $P(A)$ and $P(B)$, the probability that both independent events occur is $P(A \\wedge B) = P(A) \\cdot P(B)$.</p> <pre>def logical_and(x: SoftBool, y: SoftBool) -&gt; SoftBool:\n    return x * y\n</pre> In\u00a0[15]: Copied! <pre>plot_softbool_operation(sj.logical_and)\n</pre> plot_softbool_operation(sj.logical_and) <p>Example - Logical XOR: Softjax computes other soft logic operators such as <code>sj.logical_xor</code> by combining <code>sj.logical_not</code> and <code>sj.logical_and</code>.</p> <pre>def sj.logical_xor(x: SoftBool, y: SoftBool) -&gt; SoftBool:\n    return logical_or(logical_and(x, logical_not(y)), logical_and(logical_not(x), y))\n</pre> In\u00a0[16]: Copied! <pre>plot_softbool_operation(sj.logical_xor)\n</pre> plot_softbool_operation(sj.logical_xor) In\u00a0[17]: Copied! <pre>greater = lambda x, y: sj.greater(x, y, mode=\"sigmoid\", softness=0.1)\nsoft_where = lambda x, y: sj.where(greater(x, y), x, y)\n\nx = jax.random.uniform(jax.random.key(0), shape=(2, 10))\ny = jax.random.uniform(jax.random.key(1), shape=(2, 10))\n\nplot_array(x, title=\"x\")\nplot_array(y, title=\"y\")\nplot_array(soft_where(x, y), title=\"soft_where(x&gt;y, x, y)\")\n</pre> greater = lambda x, y: sj.greater(x, y, mode=\"sigmoid\", softness=0.1) soft_where = lambda x, y: sj.where(greater(x, y), x, y)  x = jax.random.uniform(jax.random.key(0), shape=(2, 10)) y = jax.random.uniform(jax.random.key(1), shape=(2, 10))  plot_array(x, title=\"x\") plot_array(y, title=\"y\") plot_array(soft_where(x, y), title=\"soft_where(x&gt;y, x, y)\") In\u00a0[18]: Copied! <pre>x = jnp.array([1, 2, 3])\nprint(jnp.argmax(x))\n</pre> x = jnp.array([1, 2, 3]) print(jnp.argmax(x)) <pre>2\n</pre> <p>In comparison, SoftJAX computes a <code>SoftIndex</code> array. Each entry of a <code>SoftIndex</code> array contains the probability that the index is being selected.</p> In\u00a0[19]: Copied! <pre>x = jnp.array([1, 2, 3])\nprint(sj.argmax(x))\n</pre> x = jnp.array([1, 2, 3]) print(sj.argmax(x)) <pre>[0.09003057 0.24472848 0.66524094]\n</pre> <p>Example - Softmax: The \"softmax\" (or more precisely \"softargmax\") is a commonly used  differentiable surrogate for the <code>argmax</code> function (it is also the default softening mode in <code>sj.argmax</code>). The $\\text{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j\\exp(x_j)}$ returns a discrete probability distribution over indices (aka a <code>SoftIndex</code>). As shown in the plots below, the softmax is fully differentiable. It is commonly used for multi-class classification and in transformer networks.</p> <p>When <code>softness</code> is low, <code>sj.argmax</code> concentrates probability on the true maximum index (e.g., <code>[1.0, 0.0, 0.0]</code>), recovering the hard maximum. When <code>softness</code> is higher, the result smoothly interpolates between values, providing useful gradients for optimization.</p> In\u00a0[20]: Copied! <pre>def cross_entropy(x, class_target=5):\n    probs = sj.argmax(x)\n    target_one_hot = jax.nn.one_hot(class_target, num_classes=x.shape[0])\n    log_probs = jnp.log(probs)\n    return -(target_one_hot * log_probs).mean()\n\n\nx = jax.random.normal(jax.random.key(0), shape=(10,))\nprobs = sj.argmax(x)\nlossgrads = jax.grad(cross_entropy)(x)\n\nplot_softindices_1D(x, title=\"logits\")\nplot_softindices_1D(probs, title=\"index probabilities (softmax)\")\nplot_softindices_1D(lossgrads, title=\"gradients of cross entropy loss\")\n</pre> def cross_entropy(x, class_target=5):     probs = sj.argmax(x)     target_one_hot = jax.nn.one_hot(class_target, num_classes=x.shape[0])     log_probs = jnp.log(probs)     return -(target_one_hot * log_probs).mean()   x = jax.random.normal(jax.random.key(0), shape=(10,)) probs = sj.argmax(x) lossgrads = jax.grad(cross_entropy)(x)  plot_softindices_1D(x, title=\"logits\") plot_softindices_1D(probs, title=\"index probabilities (softmax)\") plot_softindices_1D(lossgrads, title=\"gradients of cross entropy loss\") <p>Note that while in a conventional array of indices, the index information is stored in the integer values, a <code>SoftIndex</code> stores the probabilities over possible indices in an extra dimension. By convention, we always put this additional dimension into the final axis. Except for this additional final dimension, the shape of the returned soft index matches that of the indices returned by standard JAX. Here are a few examples of this:</p> In\u00a0[21]: Copied! <pre>x = jnp.arange(12).reshape((3, 4))\nprint(\"x.shape:\", x.shape)\nprint(\"jnp.argmax(x, axis=1).shape:\", jnp.argmax(x, axis=1).shape)\nprint(\"sj.argmax(x, axis=1).shape:\", sj.argmax(x, axis=1).shape)\nprint(\"jnp.argmax(x, axis=0).shape:\", jnp.argmax(x, axis=0).shape)\nprint(\"sj.argmax(x, axis=0).shape:\", sj.argmax(x, axis=0).shape)\nprint(\n    \"jnp.argmax(x, axis=1, keepdims=True).shape:\",\n    jnp.argmax(x, axis=1, keepdims=True).shape,\n)\nprint(\n    \"sj.argmax(x, axis=1, keepdims=True).shape:\",\n    sj.argmax(x, axis=1, keepdims=True).shape,\n)\nprint(\n    \"jnp.argmax(x, axis=0, keepdims=True).shape:\",\n    jnp.argmax(x, axis=0, keepdims=True).shape,\n)\nprint(\n    \"sj.argmax(x, axis=0, keepdims=True).shape:\",\n    sj.argmax(x, axis=0, keepdims=True).shape,\n)\n</pre> x = jnp.arange(12).reshape((3, 4)) print(\"x.shape:\", x.shape) print(\"jnp.argmax(x, axis=1).shape:\", jnp.argmax(x, axis=1).shape) print(\"sj.argmax(x, axis=1).shape:\", sj.argmax(x, axis=1).shape) print(\"jnp.argmax(x, axis=0).shape:\", jnp.argmax(x, axis=0).shape) print(\"sj.argmax(x, axis=0).shape:\", sj.argmax(x, axis=0).shape) print(     \"jnp.argmax(x, axis=1, keepdims=True).shape:\",     jnp.argmax(x, axis=1, keepdims=True).shape, ) print(     \"sj.argmax(x, axis=1, keepdims=True).shape:\",     sj.argmax(x, axis=1, keepdims=True).shape, ) print(     \"jnp.argmax(x, axis=0, keepdims=True).shape:\",     jnp.argmax(x, axis=0, keepdims=True).shape, ) print(     \"sj.argmax(x, axis=0, keepdims=True).shape:\",     sj.argmax(x, axis=0, keepdims=True).shape, ) <pre>x.shape: (3, 4)\njnp.argmax(x, axis=1).shape: (3,)\nsj.argmax(x, axis=1).shape: (3, 4)\njnp.argmax(x, axis=0).shape: (4,)\nsj.argmax(x, axis=0).shape: (4, 3)\n</pre> <pre>jnp.argmax(x, axis=1, keepdims=True).shape: (3, 1)\nsj.argmax(x, axis=1, keepdims=True).shape: (3, 1, 4)\njnp.argmax(x, axis=0, keepdims=True).shape: (1, 4)\nsj.argmax(x, axis=0, keepdims=True).shape: (1, 4, 3)\n</pre> <p>We also offer soft versions of <code>argmedian</code>, <code>argtop_k</code> and <code>argsort</code>.</p> In\u00a0[22]: Copied! <pre>x = jnp.arange(5)\nprint(\"x:\\n\", x)\nprint(\"jnp.argmedian(x):\\n\", \"Not implemented in standard JAX\")\nprint(\"sj.argmedian(x):\\n\", sj.argmedian(x))\n\nprint(\"jax.lax.top_k(x, k=2)[1]:\\n\", jax.lax.top_k(x, k=2)[1])\nprint(\"sj.argtop_k(x, k=2):\\n\", sj.argtop_k(x, k=2))\n\nprint(\"jnp.argsort(x):\\n\", jnp.argsort(x))\nprint(\"sj.argsort(x):\\n\", sj.argsort(x))\n</pre> x = jnp.arange(5) print(\"x:\\n\", x) print(\"jnp.argmedian(x):\\n\", \"Not implemented in standard JAX\") print(\"sj.argmedian(x):\\n\", sj.argmedian(x))  print(\"jax.lax.top_k(x, k=2)[1]:\\n\", jax.lax.top_k(x, k=2)[1]) print(\"sj.argtop_k(x, k=2):\\n\", sj.argtop_k(x, k=2))  print(\"jnp.argsort(x):\\n\", jnp.argsort(x)) print(\"sj.argsort(x):\\n\", sj.argsort(x)) <pre>x:\n [0 1 2 3 4]\njnp.argmedian(x):\n Not implemented in standard JAX\n</pre> <pre>sj.argmedian(x):\n [0.12706839 0.2315337  0.28279588 0.2315337  0.12706839]\njax.lax.top_k(x, k=2)[1]:\n [4 3]\n</pre> <pre>sj.argtop_k(x, k=2):\n [[0.01165623 0.03168492 0.08612854 0.23412167 0.6364086 ]\n [0.02591887 0.07045478 0.19151598 0.52059436 0.19151598]]\njnp.argsort(x):\n [0 1 2 3 4]\nsj.argsort(x):\n [[0.6364086  0.23412167 0.08612854 0.03168492 0.01165623]\n [0.19151598 0.5205944  0.19151598 0.07045479 0.02591887]\n [0.06745081 0.18335031 0.4983978  0.18335031 0.06745081]\n [0.02591887 0.07045478 0.19151598 0.52059436 0.19151598]\n [0.01165623 0.03168492 0.08612854 0.23412167 0.6364086 ]]\n</pre> <p>Again, the shape of the returned <code>SoftIndex</code> matches that of the normal index array, except for an additional dimension in the last axis that matches the size of the input array along the specified axis. A few examples:</p> In\u00a0[23]: Copied! <pre>x = jnp.arange(12).reshape((3, 4))\nprint(\"x.shape:\", x.shape)\nprint(\"sj.argtop_k(x, k=2, axis=1).shape:\", sj.argtop_k(x, k=2, axis=1).shape)\nprint(\"sj.argtop_k(x, k=2, axis=0).shape:\", sj.argtop_k(x, k=2, axis=0).shape)\nprint(\"sj.argsort(x, axis=1).shape:\", sj.argsort(x, axis=1).shape)\nprint(\"sj.argsort(x, axis=0).shape:\", sj.argsort(x, axis=0).shape)\nprint(\"sj.argmedian(x, axis=1).shape:\", sj.argmedian(x, axis=1).shape)\nprint(\"sj.argmedian(x, axis=0).shape:\", sj.argmedian(x, axis=0).shape)\nprint(\n    \"sj.argmedian(x, axis=1, keepdims=True).shape:\",\n    sj.argmedian(x, axis=1, keepdims=True).shape,\n)\nprint(\n    \"sj.argmedian(x, axis=0, keepdims=True).shape:\",\n    sj.argmedian(x, axis=0, keepdims=True).shape,\n)\n</pre> x = jnp.arange(12).reshape((3, 4)) print(\"x.shape:\", x.shape) print(\"sj.argtop_k(x, k=2, axis=1).shape:\", sj.argtop_k(x, k=2, axis=1).shape) print(\"sj.argtop_k(x, k=2, axis=0).shape:\", sj.argtop_k(x, k=2, axis=0).shape) print(\"sj.argsort(x, axis=1).shape:\", sj.argsort(x, axis=1).shape) print(\"sj.argsort(x, axis=0).shape:\", sj.argsort(x, axis=0).shape) print(\"sj.argmedian(x, axis=1).shape:\", sj.argmedian(x, axis=1).shape) print(\"sj.argmedian(x, axis=0).shape:\", sj.argmedian(x, axis=0).shape) print(     \"sj.argmedian(x, axis=1, keepdims=True).shape:\",     sj.argmedian(x, axis=1, keepdims=True).shape, ) print(     \"sj.argmedian(x, axis=0, keepdims=True).shape:\",     sj.argmedian(x, axis=0, keepdims=True).shape, ) <pre>x.shape: (3, 4)\n</pre> <pre>sj.argtop_k(x, k=2, axis=1).shape: (3, 2, 4)\nsj.argtop_k(x, k=2, axis=0).shape: (2, 3, 4)\nsj.argsort(x, axis=1).shape: (3, 4, 4)\n</pre> <pre>sj.argsort(x, axis=0).shape: (3, 4, 3)\nsj.argmedian(x, axis=1).shape: (3, 4)\n</pre> <pre>sj.argmedian(x, axis=0).shape: (4, 3)\nsj.argmedian(x, axis=1, keepdims=True).shape: (3, 1, 4)\nsj.argmedian(x, axis=0, keepdims=True).shape: (1, 4, 3)\n</pre> <p>Note: All of the functions in this section come with the three modes: <code>hard</code>, <code>entropic</code> and <code>euclidean</code>. <code>hard</code> mode produces one-hot soft indices and is mainly used in straight-through estimation. <code>entropic</code> is the recommended soft default and reduces all operations to either a softmax or an entropy-regularized optimal transport problem. Finally, <code>euclidean</code> reduces operations to L2 projection onto the unit simplex or the Birkhoff polytope, which can be used to produce sparse outputs. See the API documentation for details.</p> In\u00a0[24]: Copied! <pre>x = jnp.array([[5, 3, 4], [2, 7, 6]])\nprint(\"x:\\n\", x)\n\nindices = jnp.argmin(x, axis=1, keepdims=True)\nprint(\"min_jnp:\\n\", jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = sj.argmin(x, axis=1, mode=\"hard\", keepdims=True)\nprint(\"min_sj_hard:\\n\", sj.take_along_axis(x, indices_onehot, axis=1))\n\nindices_soft = sj.argmin(x, axis=1, keepdims=True)\nprint(\"min_sj_soft:\\n\", sj.take_along_axis(x, indices_soft, axis=1))\n</pre> x = jnp.array([[5, 3, 4], [2, 7, 6]]) print(\"x:\\n\", x)  indices = jnp.argmin(x, axis=1, keepdims=True) print(\"min_jnp:\\n\", jnp.take_along_axis(x, indices, axis=1))  indices_onehot = sj.argmin(x, axis=1, mode=\"hard\", keepdims=True) print(\"min_sj_hard:\\n\", sj.take_along_axis(x, indices_onehot, axis=1))  indices_soft = sj.argmin(x, axis=1, keepdims=True) print(\"min_sj_soft:\\n\", sj.take_along_axis(x, indices_soft, axis=1)) <pre>x:\n [[5 3 4]\n [2 7 6]]\nmin_jnp:\n [[3]\n [2]]\n</pre> <pre>min_sj_hard:\n [[3.]\n [2.]]\nmin_sj_soft:\n [[3.4247892]\n [2.1043382]]\n</pre> <p>As a convenience, this combination of <code>sj.take_along_axis</code> with <code>SoftIndex</code>-generataing functions is already implemented ino Softjax's <code>max</code>, <code>median</code>, <code>top_k</code> and <code>sort</code> functions.</p> In\u00a0[25]: Copied! <pre>x = jnp.arange(5)\nprint(\"x:\\n\", x)\n\nprint(\"jnp.max(x):\\n\", jnp.max(x))\nprint(\"sj.max(x):\\n\", sj.max(x))\n\nprint(\"jnp.median(x):\\n\", jnp.median(x))\nprint(\"sj.median(x):\\n\", sj.median(x))\n\nprint(\"jax.lax.top_k(x, k=2)[0]:\\n\", jax.lax.top_k(x, k=2)[0])\nprint(\"sj.top_k(x, k=2)[0]:\\n\", sj.top_k(x, k=2)[0])\n\nprint(\"jnp.sort(x):\\n\", jnp.sort(x))\nprint(\"sj.sort(x):\\n\", sj.sort(x))\n</pre> x = jnp.arange(5) print(\"x:\\n\", x)  print(\"jnp.max(x):\\n\", jnp.max(x)) print(\"sj.max(x):\\n\", sj.max(x))  print(\"jnp.median(x):\\n\", jnp.median(x)) print(\"sj.median(x):\\n\", sj.median(x))  print(\"jax.lax.top_k(x, k=2)[0]:\\n\", jax.lax.top_k(x, k=2)[0]) print(\"sj.top_k(x, k=2)[0]:\\n\", sj.top_k(x, k=2)[0])  print(\"jnp.sort(x):\\n\", jnp.sort(x)) print(\"sj.sort(x):\\n\", sj.sort(x)) <pre>x:\n [0 1 2 3 4]\njnp.max(x):\n 4\n</pre> <pre>sj.max(x):\n 3.4519415\n</pre> <pre>jnp.median(x):\n 2.0\nsj.median(x):\n 2.0000002\njax.lax.top_k(x, k=2)[0]:\n [4 3]\nsj.top_k(x, k=2)[0]:\n [3.4519415 2.7813337]\njnp.sort(x):\n [0 1 2 3 4]\n</pre> <pre>sj.sort(x):\n [0.5480584 1.2186662 2.        2.7813337 3.4519415]\n</pre> <p>Finally, we also offer a soft <code>ranking</code> operation. While it does not return a <code>SoftIndex</code> (because its output is the same shape as the input), it relies on similar computations under the hood as e.g. <code>sort</code>, and also offers the same modes.</p> In\u00a0[26]: Copied! <pre>x = jnp.arange(5)\nprint(\"x:\\n\", x)\n# This computes the ranking operation\nprint(\"jnp.argsort(jnp.argsort(x)):\\n\", jnp.argsort(jnp.argsort(x)))\nprint(\"sj.ranking(x, descending=False):\\n\", sj.ranking(x, descending=False))\n</pre> x = jnp.arange(5) print(\"x:\\n\", x) # This computes the ranking operation print(\"jnp.argsort(jnp.argsort(x)):\\n\", jnp.argsort(jnp.argsort(x))) print(\"sj.ranking(x, descending=False):\\n\", sj.ranking(x, descending=False)) <pre>x:\n [0 1 2 3 4]\njnp.argsort(jnp.argsort(x)):\n [0 1 2 3 4]\n</pre> <pre>sj.ranking(x, descending=False):\n [0.5480584 1.2186662 2.        2.7813337 3.4519415]\n</pre>"},{"location":"all-of-softjax/#all-of-softjax","title":"All of Softjax\u00b6","text":"<p>Softjax provides easy-to-use differentiable function surrogates of non-differentiable functions and discrete logic operations in JAX. Softjax offers soft function surrogates operating on real values, Booleans, and indices as well as wrappers to use these functions for gradient computation via straight-through estimation.</p> <p>This pages guides you through all of Softjax key functionalities.</p>"},{"location":"all-of-softjax/#1-softening","title":"1. Softening\u00b6","text":"<p>Many functions are not particulary well suited for gradient-based optimization as their gradients are either zero or undefined at discontinuities. Therefore, a typical approach in machine learning is to use soft surrogates for these functions to enable gradient-based optimization via automatic differentiation. Softjax provides such soft surrogates for many operations in JAX.</p> <p>As shown below, Softjax provides two hyper-parameters to tune soft function surrogates:</p> <ul> <li><p>mode being the type of function used for obtaining a soft approximation.</p> </li> <li><p>softness defining how close the surrogate function approximates the original function.</p> </li> </ul>"},{"location":"all-of-softjax/#2-straight-through-estimation","title":"2. Straight-through estimation\u00b6","text":"<p>Soft function surrogates can be used to compute soft gradients (or more accurately: soft vector-jacobian-producs) without modifying the forward pass via straight-through estimation. Straight-through estimation uses JAX's automatic differentiation system to replace only the function's gradient with the gradient of the surrogate function.</p> <p>Note: Historically, straight-through estimators refer to the special case of treating a function as the identity operation on the backward pass. We use the term more generally to describe the case of replacing a function with smooth a surrogate on the backward pass.</p> <p>Example - ReLu activation: The rectified linear unit (aka <code>relu</code>)  is commonly used as activation function in neural networks. For $x&lt;0$ the gradient of the <code>relu</code> is zero. In turn, neural networks containing <code>relu</code> activations may suffer from the \"dying ReLu problem\" where the gradients computed via automatic differentiation become zero when a gradient-based optimizer adjusts the inputs of ReLU functions to $x&lt;0$. As pointed out in \"The Resurrection of the ReLU\", you can mitigate these problems by replacing its backward pass with a soft surrogate function. To do this with Softjax, simply replace the <code>relu</code> activation in your code with <code>sj.st(sj.relu)</code>, or equivalently directly use our wrapped primitives via <code>sj.relu_st</code> for convenience.</p>"},{"location":"all-of-softjax/#the-ste-trick","title":"The STE Trick\u00b6","text":"<p>Under the hood <code>sj.st()</code> uses the stop-gradient oepration to replace the gradient of a function.</p> <pre>def st(fn: Callable) -&gt; Callable:\n    sig = inspect.signature(fn)\n    mode_default = sig.parameters.get(\"mode\").default\n    def wrapped(*args, **kwargs):\n        mode = kwargs.pop(\"mode\", mode_default)\n        fw_y = fn(*args, **kwargs, mode=\"hard\")\n        bw_y = fn(*args, **kwargs, mode=mode)\n        return jtu.tree_map(lambda fw, bw: jax.lax.stop_gradient(fw - bw) + bw, fw_y, bw_y)\n    return wrapped\n</pre> <p>By adding and subtracting the backward function <code>bw_y</code> to the function call, it does not alter the function's forward pass <code>fw_y</code>. Due to <code>jax.lax.stop_gradient</code>, only the soft backward function <code>bw_y</code> is used in the gradient computation.</p>"},{"location":"all-of-softjax/#custom-differentiation-rules","title":"Custom differentiation rules\u00b6","text":"<p>The <code>@sj.st</code> decorator can also be used to define custom straight-through operations. This can be useful when combining multiple functions provided by the softjax library. For this, it is important to understand, that simply applying the straight-through trick to every non-smooth function does not always result in the intended behavior. Consider for example the case of multiplying the output of two <code>relu</code> functions together. This function only provides meaningful gradients in the first quadrant, we would like to change it such that we get a meaningful signal in the whole domain, as visualized below.</p> <p>Note: We normalize the plotted gradient vectors for reduced cluttering.</p>"},{"location":"all-of-softjax/#3-soft-bools","title":"3. Soft bools\u00b6","text":"<p>Softjax provides differentiable surrogates of JAX's Boolean operators. A Boolean (aka Bool) is a data type that takes one of two possible values either being <code>false</code> or <code>true</code> (aka 0 or 1). Many operations in JAX such as <code>greater_equal</code> or <code>isclose</code> generate arrays containing Booleans, while other operations such as <code>logical_and</code> or <code>any</code> operate on such arrays.</p> <p>Example - <code>jax.numpy.greater_equal</code> yields zero gradients: As shown below, <code>jax.grad</code> does not raise an error when called on its boolean operations. However, the returned gradients are zero for all array entries.</p>"},{"location":"all-of-softjax/#generating-soft-bools","title":"Generating soft bools\u00b6","text":"<p>How does Softjax make Boolean logic operations differentiable? A real number $x\\in \\mathbb{R}$ could be mapped to a <code>Bool</code> using the Heaviside function $$ H(x) = \\begin{cases} 1, &amp; x &gt; 0 \\\\ 0.5, &amp; x=0\\\\ 0, &amp; x &lt; 0 \\end{cases}. $$ The gradient of the Heaviside function (as implemented in JAX) is zero everywhere and hence unsuited for differentiable optimization. Instead of operating directly on Booleans, Softjax's differentiable logic operators resort to soft Booleans. A soft Boolean aka <code>SoftBool</code> can be interpreted as the probability of a Boolean being <code>True</code>.</p> <p>We replace the heaviside function with differentiable surrogate such as the sigmoid function $\\sigma(x) = \\frac{1}{1+e^{-x}}$. While the <code>sigmoid</code> is the canonical example for mapping a real number to a <code>SoftBool</code>, Softjax provides additional surrogates.</p>"},{"location":"all-of-softjax/#manipulating-soft-bools","title":"Manipulating soft bools\u00b6","text":"<p>Softjax replaces a Boolean with a <code>SoftBool</code>, in turn Boolean logic operators are replaced in Softjax with fuzzy logic operators that effectively compute the probabilities of Boolean events.</p> <p>Example - Logical NOT: Given a <code>SoftBool</code> $P(B)$ (being the probability that a Boolean event $B$ occurs), the probability of the event not occuring is $P(\\bar B) = 1 - P(B)$ as implemented in <code>sj.logical_not</code>.</p> <pre>def logical_not(x: SoftBool) -&gt; SoftBool:\n    return 1 - x\n</pre> <p>Given <code>sj.logical_not</code>, the probability that <code>x is not greater equal 0.5</code> is given by <code>sj.logical_not(sj.greater_equal_st(x, 0.5))</code>. Due to the straight-through trick, the function <code>sj.logical_not(sj.greater_equal_st(x, 0.5))</code> uses exact Boolean logic in the forward pass and the <code>SoftBool</code> probability computation in the backward pass.</p>"},{"location":"all-of-softjax/#selection-with-soft-bools","title":"Selection with soft bools\u00b6","text":"<p>Through the use of Fuzzy logic operators, Softjax provides a toolbox to make many non-differentiable functions of JAX differentiable.</p> <p>Example - sj.where(): The function <code>jax.numpy.where(condition, x, y)</code> selects elements of array <code>x</code> if <code>condition == True</code> and otherwise selects <code>y</code>. Softjax provides a differentiable surroagte for this function via <code>sj.where(P, x, y)</code> which effectively computes the expected value $\\mathbb{E}[X] = P \\cdot x + (1-P) \\cdot y$.</p>"},{"location":"all-of-softjax/#4-soft-indices","title":"4. Soft indices\u00b6","text":"<p>Softjax offers soft surrogates for functions that generate indices as outputs, such as <code>argmax</code>, <code>argmin</code>, <code>argtop_k</code>, <code>argmedian</code>, and <code>argsort</code>. The main mechanism here is to replace hard indices with distributions over indices (<code>SoftIndex</code>), allowing for informative gradients.</p> <p>Similar to how <code>SoftBool</code> required going from boolean logic to fuzzy logic, this now requires adjusting functions that do selection via indices. As such, we provide new versions of e.g. <code>take_along_axis</code>, <code>dynamic_index_in_dim</code>, and <code>choose</code>. Combining the soft index generation with the selection then allows to define surrogates for the corresponding <code>max</code>, <code>min</code>, <code>top_k</code>, <code>median</code> and <code>sort</code> functions.</p>"},{"location":"all-of-softjax/#generating-soft-indices","title":"Generating soft indices\u00b6","text":"<p>In JAX, functions like <code>jax.argmax</code> return integer indices as outputs, which can take values within {0, ..., len(x)-1}.</p>"},{"location":"all-of-softjax/#selection-with-softindices","title":"Selection with SoftIndices\u00b6","text":"<p>Given a <code>SoftIndex</code>, Softjax provides (differentiable) helper functions for selecting array elements, mirroring the non-differentiable indexing in standard JAX. Put simply, entries of an array are selected by computing the expected value:</p> <p>$$\\mathrm{E}(arr, p) = \\sum_{i} arr[i] \\cdot p[i] = arr^{\\top} \\cdot p$$</p> <p>where $p$ is the <code>SoftIndex</code>.</p> <p>Example <code>sj.take_along_axis</code>: The function <code>sj.take_along_axis</code> is central to this selection mechanism. It generalizes <code>jnp.take_along_axis</code> to work with probability distributions (SoftIndices) instead of just integer indices.</p> <p>The standard <code>jnp.take_along_axis(arr, indices, axis)</code> selects elements from <code>arr</code> using integer indices. Conceptually, it works by:</p> <ol> <li>Slicing along the specified axis to get 1D arrays</li> <li>Using the corresponding indices to select elements</li> <li>Assembling the results into the output array</li> </ol> <p>One of its main uses is to accept the index output of e.g. <code>jnp.argmax</code> and select the maximum values at the indexed locations. While <code>jnp.take_along_axis</code> uses integer indices <code>out_1d[j] = arr_1d[indices_1d[j]]</code>, <code>sj.take_along_axis</code> accepts a <code>SoftIndex</code> to compute the corresponding the weighted sum: <code>out_1d[j] = sum_i(arr_1d[i] * soft_indices_2d[j, i])</code>.</p>"},{"location":"manifold_points/","title":"Manifold Points","text":"In\u00a0[1]: Copied! <pre>from dataclasses import dataclass\nfrom typing import Literal\n\nimport jax.numpy as jp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport softjax as sj\n</pre> from dataclasses import dataclass from typing import Literal  import jax.numpy as jp import matplotlib.pyplot as plt import numpy as np import softjax as sj  In\u00a0[2]: Copied! <pre># Hard manifold selector mirrored from MJX (collision_convex.py)\ndef manifold_points_mjx(\n    poly: jp.ndarray, poly_mask: jp.ndarray, poly_norm: jp.ndarray\n) -&gt; jp.ndarray:\n    \"\"\"MJX hard manifold heuristic (returns 4 vertex indices).\"\"\"\n    dist_mask = jp.where(poly_mask, 0.0, -1e6)\n    # A: any unmasked vertex (in MJX this corresponds to the most penetrating one)\n    # Note: We add a small tie-breaker to ensure consistent results\n    a_idx = jp.argmax(dist_mask - 0.1 * jp.arange(poly.shape[0]))\n    a = poly[a_idx]\n    # B: farthest from A (largest squared distance)\n    b_idx = (((a - poly) ** 2).sum(axis=1) + dist_mask).argmax()\n    b = poly[b_idx]\n    # C: farthest from the AB line within the plane\n    ab = jp.cross(poly_norm, a - b)\n    ap = a - poly\n    c_idx = (jp.abs(ap.dot(ab)) + dist_mask).argmax()\n    c = poly[c_idx]\n    # D: farthest from edges AC and BC\n    ac = jp.cross(poly_norm, a - c)\n    bc = jp.cross(poly_norm, b - c)\n    bp = b - poly\n    dist_bp = jp.abs(bp.dot(bc)) + dist_mask\n    dist_ap = jp.abs(ap.dot(ac)) + dist_mask\n    d_idx = (dist_bp + dist_ap).argmax() % poly.shape[0]\n    return jp.array([a_idx, b_idx, c_idx, d_idx])\n</pre> # Hard manifold selector mirrored from MJX (collision_convex.py) def manifold_points_mjx(     poly: jp.ndarray, poly_mask: jp.ndarray, poly_norm: jp.ndarray ) -&gt; jp.ndarray:     \"\"\"MJX hard manifold heuristic (returns 4 vertex indices).\"\"\"     dist_mask = jp.where(poly_mask, 0.0, -1e6)     # A: any unmasked vertex (in MJX this corresponds to the most penetrating one)     # Note: We add a small tie-breaker to ensure consistent results     a_idx = jp.argmax(dist_mask - 0.1 * jp.arange(poly.shape[0]))     a = poly[a_idx]     # B: farthest from A (largest squared distance)     b_idx = (((a - poly) ** 2).sum(axis=1) + dist_mask).argmax()     b = poly[b_idx]     # C: farthest from the AB line within the plane     ab = jp.cross(poly_norm, a - b)     ap = a - poly     c_idx = (jp.abs(ap.dot(ab)) + dist_mask).argmax()     c = poly[c_idx]     # D: farthest from edges AC and BC     ac = jp.cross(poly_norm, a - c)     bc = jp.cross(poly_norm, b - c)     bp = b - poly     dist_bp = jp.abs(bp.dot(bc)) + dist_mask     dist_ap = jp.abs(ap.dot(ac)) + dist_mask     d_idx = (dist_bp + dist_ap).argmax() % poly.shape[0]     return jp.array([a_idx, b_idx, c_idx, d_idx])  In\u00a0[3]: Copied! <pre>def manifold_points_softjax(\n    poly: jp.ndarray,\n    poly_mask: jp.ndarray,\n    poly_norm: jp.ndarray,\n    *,\n    softness: float = 1.0,\n    mode: Literal[\"hard\", \"soft\"] = \"soft\",\n) -&gt; jp.ndarray:\n    \"\"\"Soft counterpart of `manifold_points_mjx`.\n\n    Returns 4 SoftIndex distributions (shape: 4 x n).\n    \"\"\"\n    dist_mask = jp.where(poly_mask, 0.0, -1e6)\n\n    def argmax_soft(x):\n        m = \"entropic\" if mode == \"soft\" else \"hard\"\n        return sj.argmax(x, mode=m, softness=softness, axis=0)\n\n    def abs_soft(x):\n        m = \"sigmoid\" if mode == \"soft\" else \"hard\"\n        return sj.abs(x, mode=m, softness=softness)\n\n    # A: soft argmax over masked distances\n    a_idx = argmax_soft(dist_mask - 0.1 * jp.arange(poly.shape[0]))\n    a = sj.dynamic_index_in_dim(poly, a_idx, axis=0, keepdims=False)\n    # B: soft argmax of distance from A\n    b_logits = (((a - poly) ** 2).sum(axis=1)) + dist_mask\n    b_idx = argmax_soft(b_logits)\n    b = sj.dynamic_index_in_dim(poly, b_idx, axis=0, keepdims=False)\n    # C: soft argmax farthest from AB line\n    ab = jp.cross(poly_norm, a - b)\n    ap = a - poly\n    c_logits = abs_soft(ap.dot(ab)) + dist_mask\n    c_idx = argmax_soft(c_logits)\n    c = sj.dynamic_index_in_dim(poly, c_idx, axis=0, keepdims=False)\n    # D: soft argmax farthest from AC and BC edges\n    ac = jp.cross(poly_norm, a - c)\n    bc = jp.cross(poly_norm, b - c)\n    bp = b - poly\n    dist_bp = abs_soft(bp.dot(bc)) + dist_mask\n    dist_ap = abs_soft(ap.dot(ac)) + dist_mask\n    d_logits = dist_bp + dist_ap\n    d_idx = argmax_soft(d_logits)\n    return jp.stack([a_idx, b_idx, c_idx, d_idx], axis=0)\n</pre> def manifold_points_softjax(     poly: jp.ndarray,     poly_mask: jp.ndarray,     poly_norm: jp.ndarray,     *,     softness: float = 1.0,     mode: Literal[\"hard\", \"soft\"] = \"soft\", ) -&gt; jp.ndarray:     \"\"\"Soft counterpart of `manifold_points_mjx`.      Returns 4 SoftIndex distributions (shape: 4 x n).     \"\"\"     dist_mask = jp.where(poly_mask, 0.0, -1e6)      def argmax_soft(x):         m = \"entropic\" if mode == \"soft\" else \"hard\"         return sj.argmax(x, mode=m, softness=softness, axis=0)      def abs_soft(x):         m = \"sigmoid\" if mode == \"soft\" else \"hard\"         return sj.abs(x, mode=m, softness=softness)      # A: soft argmax over masked distances     a_idx = argmax_soft(dist_mask - 0.1 * jp.arange(poly.shape[0]))     a = sj.dynamic_index_in_dim(poly, a_idx, axis=0, keepdims=False)     # B: soft argmax of distance from A     b_logits = (((a - poly) ** 2).sum(axis=1)) + dist_mask     b_idx = argmax_soft(b_logits)     b = sj.dynamic_index_in_dim(poly, b_idx, axis=0, keepdims=False)     # C: soft argmax farthest from AB line     ab = jp.cross(poly_norm, a - b)     ap = a - poly     c_logits = abs_soft(ap.dot(ab)) + dist_mask     c_idx = argmax_soft(c_logits)     c = sj.dynamic_index_in_dim(poly, c_idx, axis=0, keepdims=False)     # D: soft argmax farthest from AC and BC edges     ac = jp.cross(poly_norm, a - c)     bc = jp.cross(poly_norm, b - c)     bp = b - poly     dist_bp = abs_soft(bp.dot(bc)) + dist_mask     dist_ap = abs_soft(ap.dot(ac)) + dist_mask     d_logits = dist_bp + dist_ap     d_idx = argmax_soft(d_logits)     return jp.stack([a_idx, b_idx, c_idx, d_idx], axis=0)  In\u00a0[4]: Copied! <pre>@dataclass\nclass Selection:\n    poly: np.ndarray\n    hard_idx: np.ndarray\n    hard_pts: np.ndarray\n    soft_runs: list[tuple[float, np.ndarray]]\n\n\ndef make_planar_polygon(\n    n: int = 8, radius: float = 1.0, jitter: float = 0.05, seed: int = 0\n):\n    rng = np.random.RandomState(seed)\n    angles = np.linspace(0, 2 * np.pi, n, endpoint=False)\n    r = radius * (1.0 + jitter * rng.randn(n))\n    x = r * np.cos(angles)\n    y = r * np.sin(angles)\n    poly2d = np.stack([x, y], axis=1)\n    center = poly2d.mean(axis=0)\n    angles_ccw = np.arctan2(poly2d[:, 1] - center[1], poly2d[:, 0] - center[0])\n    order = np.argsort(angles_ccw)\n    poly2d = poly2d[order]\n    poly3d = np.concatenate([poly2d, np.zeros((n, 1))], axis=1)\n    return jp.array(poly3d)\n\n\ndef order_quad(points: np.ndarray):\n    center = points.mean(axis=0)\n    angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])\n    order = np.argsort(angles)\n    return points[order]\n\n\ndef run_selection(\n    n: int = 16,\n    softness: list[float] | tuple[float, ...] = (1.0,),\n    mode: str = \"entropic\",\n    seed: int = 0,\n) -&gt; Selection:\n    poly = make_planar_polygon(n=n, seed=seed)\n    mask = jp.ones((n,), dtype=bool)\n    normal = jp.array([0.0, 0.0, 1.0])\n\n    hard_idx = np.array(manifold_points_mjx(poly, mask, normal))\n    hard_pts = np.array(poly[hard_idx])\n\n    soft_runs = []\n    for w in softness:\n        soft_probs = manifold_points_softjax(\n            poly, mask, normal, softness=float(w), mode=mode\n        )\n        soft_pts = soft_probs @ np.array(poly)\n        soft_runs.append((w, soft_pts))\n\n    return Selection(\n        poly=np.array(poly), hard_idx=hard_idx, hard_pts=hard_pts, soft_runs=soft_runs\n    )\n\n\ndef plot_multi(sel: Selection, mode: str):\n    poly2d = sel.poly[:, :2]\n    hull = order_quad(poly2d)\n    hull_closed = np.vstack([hull, hull[0]])\n\n    fig, ax = plt.subplots(figsize=(8, 7))\n    ax.fill(\n        hull_closed[:, 0],\n        hull_closed[:, 1],\n        color=\"#dddddd\",\n        alpha=0.3,\n        label=\"poly hull\",\n    )\n    ax.plot(hull_closed[:, 0], hull_closed[:, 1], color=\"#999999\", lw=1.0)\n    ax.scatter(poly2d[:, 0], poly2d[:, 1], color=\"#444444\", s=40, label=\"verts\")\n\n    hard_xy = order_quad(sel.hard_pts[:, :2])\n    hard_closed = np.vstack([hard_xy, hard_xy[0]])\n    ax.plot(\n        hard_closed[:, 0], hard_closed[:, 1], color=\"#000000\", lw=2.0, label=\"hard_mjx\"\n    )\n    ax.scatter(hard_xy[:, 0], hard_xy[:, 1], color=\"#000000\", s=60, zorder=3)\n\n    cmap = plt.get_cmap(\"tab10\")\n    for i, (w, soft_pts) in enumerate(sel.soft_runs):\n        soft_xy = order_quad(np.array(soft_pts)[:, :2])\n        soft_closed = np.vstack([soft_xy, soft_xy[0]])\n        color = cmap(i % cmap.N)\n        ax.plot(\n            soft_closed[:, 0],\n            soft_closed[:, 1],\n            color=color,\n            lw=2.0,\n            ls=\"--\",\n            label=f\"soft w={w}\",\n        )\n        ax.scatter(soft_xy[:, 0], soft_xy[:, 1], color=color, s=55, zorder=3)\n\n    ax.set_aspect(\"equal\", adjustable=\"datalim\")\n    ax.set_title(f\"Hard vs soft manifold points (mode={mode})\")\n    ax.legend(loc=\"upper right\")\n    ax.grid(True, alpha=0.2)\n    plt.show()\n</pre> @dataclass class Selection:     poly: np.ndarray     hard_idx: np.ndarray     hard_pts: np.ndarray     soft_runs: list[tuple[float, np.ndarray]]   def make_planar_polygon(     n: int = 8, radius: float = 1.0, jitter: float = 0.05, seed: int = 0 ):     rng = np.random.RandomState(seed)     angles = np.linspace(0, 2 * np.pi, n, endpoint=False)     r = radius * (1.0 + jitter * rng.randn(n))     x = r * np.cos(angles)     y = r * np.sin(angles)     poly2d = np.stack([x, y], axis=1)     center = poly2d.mean(axis=0)     angles_ccw = np.arctan2(poly2d[:, 1] - center[1], poly2d[:, 0] - center[0])     order = np.argsort(angles_ccw)     poly2d = poly2d[order]     poly3d = np.concatenate([poly2d, np.zeros((n, 1))], axis=1)     return jp.array(poly3d)   def order_quad(points: np.ndarray):     center = points.mean(axis=0)     angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])     order = np.argsort(angles)     return points[order]   def run_selection(     n: int = 16,     softness: list[float] | tuple[float, ...] = (1.0,),     mode: str = \"entropic\",     seed: int = 0, ) -&gt; Selection:     poly = make_planar_polygon(n=n, seed=seed)     mask = jp.ones((n,), dtype=bool)     normal = jp.array([0.0, 0.0, 1.0])      hard_idx = np.array(manifold_points_mjx(poly, mask, normal))     hard_pts = np.array(poly[hard_idx])      soft_runs = []     for w in softness:         soft_probs = manifold_points_softjax(             poly, mask, normal, softness=float(w), mode=mode         )         soft_pts = soft_probs @ np.array(poly)         soft_runs.append((w, soft_pts))      return Selection(         poly=np.array(poly), hard_idx=hard_idx, hard_pts=hard_pts, soft_runs=soft_runs     )   def plot_multi(sel: Selection, mode: str):     poly2d = sel.poly[:, :2]     hull = order_quad(poly2d)     hull_closed = np.vstack([hull, hull[0]])      fig, ax = plt.subplots(figsize=(8, 7))     ax.fill(         hull_closed[:, 0],         hull_closed[:, 1],         color=\"#dddddd\",         alpha=0.3,         label=\"poly hull\",     )     ax.plot(hull_closed[:, 0], hull_closed[:, 1], color=\"#999999\", lw=1.0)     ax.scatter(poly2d[:, 0], poly2d[:, 1], color=\"#444444\", s=40, label=\"verts\")      hard_xy = order_quad(sel.hard_pts[:, :2])     hard_closed = np.vstack([hard_xy, hard_xy[0]])     ax.plot(         hard_closed[:, 0], hard_closed[:, 1], color=\"#000000\", lw=2.0, label=\"hard_mjx\"     )     ax.scatter(hard_xy[:, 0], hard_xy[:, 1], color=\"#000000\", s=60, zorder=3)      cmap = plt.get_cmap(\"tab10\")     for i, (w, soft_pts) in enumerate(sel.soft_runs):         soft_xy = order_quad(np.array(soft_pts)[:, :2])         soft_closed = np.vstack([soft_xy, soft_xy[0]])         color = cmap(i % cmap.N)         ax.plot(             soft_closed[:, 0],             soft_closed[:, 1],             color=color,             lw=2.0,             ls=\"--\",             label=f\"soft w={w}\",         )         ax.scatter(soft_xy[:, 0], soft_xy[:, 1], color=color, s=55, zorder=3)      ax.set_aspect(\"equal\", adjustable=\"datalim\")     ax.set_title(f\"Hard vs soft manifold points (mode={mode})\")     ax.legend(loc=\"upper right\")     ax.grid(True, alpha=0.2)     plt.show()  <p>We first check, that in <code>hard</code> mode our softjax version matches the original mjx selection.</p> In\u00a0[5]: Copied! <pre>sel = run_selection(softness=(0.0,), mode=\"hard\", seed=0)\nplot_multi(sel, mode=\"hard\")\n</pre> sel = run_selection(softness=(0.0,), mode=\"hard\", seed=0) plot_multi(sel, mode=\"hard\") <p>Next, we visualize the relaxations for two different modes.</p> In\u00a0[6]: Copied! <pre>sel = run_selection(softness=[0.05, 0.1, 0.15, 0.2], mode=\"soft\", seed=0)\nplot_multi(sel, mode=\"entropic\")\n</pre> sel = run_selection(softness=[0.05, 0.1, 0.15, 0.2], mode=\"soft\", seed=0) plot_multi(sel, mode=\"entropic\") In\u00a0[7]: Copied! <pre>from softjax.straight_through import st\n\nmanifold_points_softjax_st = st(manifold_points_softjax)\n</pre> from softjax.straight_through import st  manifold_points_softjax_st = st(manifold_points_softjax) <p>The <code>manifold_points_softjax_st</code> now still has the discrete one-hot outputs of the hard function, but provides the gradients of the relaxed version.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"manifold_points/#manifold-points","title":"Manifold Points\u00b6","text":"<p>This notebook provides an example of how to translate a hard jax function into a softjax function in practice.</p> <p>As the worked example, we consider Mujoco MJX's convex collision detection algorithm, which has a subroutine that chooses four vertices (A, B, C, D) that roughly maximize the contact patch area.</p>"},{"location":"manifold_points/#original-function","title":"Original function\u00b6","text":"<p>The steps of the selection algorithm are</p> <ol> <li>Pick A: any unmasked vertex (in MJX this is the most penetrating one).</li> <li>Pick B: the vertex farthest from A (long base edge).</li> <li>Pick C: the vertex farthest from line AB within the plane (opens the area).</li> <li>Pick D: the vertex farthest from both edges AC and BC.</li> </ol>"},{"location":"manifold_points/#softjax-version","title":"SoftJax version\u00b6","text":"<p>To make this algorithm smoothly differentiable we convert it to softjax. For this, we:</p> <ul> <li>replace discrete <code>argmax</code> with <code>sj.argmax</code>, which returns a <code>SoftIndex</code> distribution,</li> <li>replace the hard indexing with <code>sj.dynamic_index_in_dim</code>, which uses the <code>SoftIndex</code> as input,</li> <li>replace <code>abs</code> with <code>sj.abs</code>.</li> </ul> <p>Note that we now return four <code>SoftIndex</code> distributions (shape: 4 x n) instead of the hard indices. The remaining code of the collision detection therefore would need to be adjusted accordingly.</p>"},{"location":"manifold_points/#visualization","title":"Visualization\u00b6","text":"<p>We generate a perturbed planar polygon for visualization.</p>"},{"location":"manifold_points/#straight-through-estimation","title":"Straight-through estimation\u00b6","text":"<p>We could now directly use one of the relaxations as a differentiable proxy. However, sometimes it is desired to not alter the forward pass, e.g. in simulation we do not want to relaxt the forward physics. In these cases, we can resort to the straight-through trick, which means to replace only the gradient of a hard function on forward with the gradeint of a relaxed/soft function on backward pass. <code>softjax.straight_through.st</code> implements this behavior by wrapping a function so that the forward pass uses a hard definition while the backward pass uses a soft surrogate. For convenience, we all ready provide wrapped versions of all our primitives, allowing the user to just use e.g. <code>sj.argmax_st</code> instead of <code>sj.argmax</code>.</p> <p>However, as described in the all-of-softjax notebook, this can still lead to uninformative gradients due to the interaction of the straight-through trick with the chain rule. A way to avoid this issue is to apply the straight-through trick on the outer level of the downstream function, which calls the whole function twice instead of each of the primitives. This is illustrated below.</p>"},{"location":"_static/Readme/","title":"Readme","text":"<p>The favicon is adapted from <code>cosine-wave</code> from https://materialdesignicons.com, found by way of https://pictogrammers.com. Specifically it has been adapted by filling in the integral with black. (Originally it has 100% alpha.)</p>"},{"location":"api/soft_bools/","title":"SoftBools","text":""},{"location":"api/soft_bools/#generating-softbools","title":"Generating SoftBools","text":""},{"location":"api/soft_bools/#softjax.heaviside","title":"<code>softjax.heaviside(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'sigmoid', 'pseudohuber', 'tanh', 'linear', 'cubic', 'quintic'] = 'sigmoid') -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.heaviside(x,0.5).</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact Heaviside step. Otherwise uses     \"sigmoid\", \"linear\", \"cubic\", or \"quintic\" relaxations. Defaults to \"sigmoid\".</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> (Array with values in [0, 1]), relaxing the elementwise Heaviside step function.</p>"},{"location":"api/soft_bools/#softjax.greater","title":"<code>softjax.greater(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'sigmoid', 'pseudohuber', 'tanh', 'linear', 'cubic', 'quintic'] = 'sigmoid', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &gt; y</code>. Uses a Heaviside relaxation so the output approaches 0 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"sigmoid\", \"linear\" spline, \"cubic\" spline, or \"quintic\" spline.     Defaults to \"sigmoid\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, greater returns 0 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &gt; y</code>.</p>"},{"location":"api/soft_bools/#softjax.greater_equal","title":"<code>softjax.greater_equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'sigmoid', 'pseudohuber', 'tanh', 'linear', 'cubic', 'quintic'] = 'sigmoid', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &gt;= y</code>. Uses a Heaviside relaxation so the output approaches 1 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"sigmoid\", \"linear\" spline, \"cubic\" spline, or \"quintic\" spline.     Defaults to \"sigmoid\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, greater_equal returns 1 at     equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &gt;= y</code>.</p>"},{"location":"api/soft_bools/#softjax.less","title":"<code>softjax.less(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'sigmoid', 'pseudohuber', 'tanh', 'linear', 'cubic', 'quintic'] = 'sigmoid', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &lt; y</code>. Uses a Heaviside relaxation so the output approaches 0 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"sigmoid\", \"linear\" spline, \"cubic\" spline, or \"quintic\" spline.     Defaults to \"sigmoid\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, less returns 0 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &lt; y</code>.</p>"},{"location":"api/soft_bools/#softjax.less_equal","title":"<code>softjax.less_equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'sigmoid', 'pseudohuber', 'tanh', 'linear', 'cubic', 'quintic'] = 'sigmoid', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &lt;= y</code>. Uses a Heaviside relaxation so the output approaches 1 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"sigmoid\", \"linear\" spline, \"cubic\" spline, or \"quintic\" spline.     Defaults to \"sigmoid\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, less_equal returns 1 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &lt;= y</code>.</p>"},{"location":"api/soft_bools/#softjax.equal","title":"<code>softjax.equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'sigmoid', 'pseudohuber', 'tanh', 'linear', 'cubic', 'quintic'] = 'sigmoid', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x == y</code>. Implemented as a soft <code>abs(x - y) &lt;= 0</code> comparison.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"sigmoid\", \"linear\" spline, \"cubic\" spline, or \"quintic\" spline.     Defaults to \"sigmoid\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, equal returns 1 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x == y</code>.</p>"},{"location":"api/soft_bools/#softjax.not_equal","title":"<code>softjax.not_equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'sigmoid', 'pseudohuber', 'tanh', 'linear', 'cubic', 'quintic'] = 'sigmoid', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x != y</code>. Implemented as a soft <code>abs(x - y) &gt; 0</code> comparison.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"sigmoid\", \"linear\" spline, \"cubic\" spline, or \"quintic\" spline.     Defaults to \"sigmoid\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, not_equal returns 0 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x != y</code>.</p>"},{"location":"api/soft_bools/#softjax.isclose","title":"<code>softjax.isclose(x: jax.Array, y: jax.Array, softness: float = 1.0, rtol: float = 1e-05, atol: float = 1e-08, mode: Literal['hard', 'sigmoid', 'pseudohuber', 'tanh', 'linear', 'cubic', 'quintic'] = 'sigmoid', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to <code>jnp.isclose</code> for elementwise comparison. Implemented as a soft <code>abs(x - y) &lt;= atol + rtol * abs(y)</code> comparison.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>rtol</code>: Relative tolerance. Defaults to 1e-5.</li> <li><code>atol</code>: Absolute tolerance. Defaults to 1e-8.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"sigmoid\", \"linear\" spline, \"cubic\" spline, or \"quintic\" spline.     Defaults to \"sigmoid\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, isclose returns 1 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>isclose(x, y)</code>.</p>"},{"location":"api/soft_bools/#manipulating-softbools","title":"Manipulating SoftBools","text":""},{"location":"api/soft_bools/#softjax.logical_and","title":"<code>softjax.logical_and(x: Float[Array, '...'], y: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical AND between two SoftBool Arrays. Fuzzy logic implemented as <code>all(stack([x, y], axis=-1), axis=-1)</code>.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First SoftBool input Array.</li> <li><code>y</code>: Second SoftBool input Array.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise logical AND.</p>"},{"location":"api/soft_bools/#softjax.logical_not","title":"<code>softjax.logical_not(x: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical NOT of a SoftBool Array. Fuzzy logic implemented as <code>1.0 - x</code>.</p> <p>Arguments: - <code>x</code>: SoftBool input Array.</p> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> (Array with values in [0, 1]), relaxing the elementwise logical NOT.</p>"},{"location":"api/soft_bools/#softjax.logical_or","title":"<code>softjax.logical_or(x: Float[Array, '...'], y: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical OR between two SoftBool Arrays. Fuzzy logic implemented as <code>any(stack([x, y], axis=-1), axis=-1)</code>.</p> <p>Arguments: - <code>x</code>: First SoftBool input Array. - <code>y</code>: Second SoftBool input Array.</p> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise logical OR.</p>"},{"location":"api/soft_bools/#softjax.logical_xor","title":"<code>softjax.logical_xor(x: Float[Array, '...'], y: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical XOR between two SoftBool Arrays.</p> <p>Arguments: - <code>x</code>: First SoftBool input Array. - <code>y</code>: Second SoftBool input Array.</p> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise logical XOR.</p>"},{"location":"api/soft_bools/#softjax.all","title":"<code>softjax.all(x: Float[Array, '...'], axis: int = -1, epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical AND across a specified axis. Fuzzy logic implemented as the geometric mean along the axis.</p> <p>Arguments: - <code>x</code>: SoftBool input Array. - <code>axis</code>: Axis along which to compute the logical AND. Default is -1 (last axis). - <code>epsilon</code>: Minimum value for numerical stability inside the log.</p> <p>Returns:</p> <p>SoftBool (Array with values in [0, 1]) with the specified axis reduced, relaxing the logical ALL along that axis.</p>"},{"location":"api/soft_bools/#softjax.any","title":"<code>softjax.any(x: Float[Array, '...'], axis: int = -1) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical OR across a specified axis. Fuzzy logic implemented as <code>1.0 - all(logical_not(x), axis=axis)</code>.</p> <p>Arguments: - <code>x</code>: SoftBool input Array. - <code>axis</code>: Axis along which to compute the logical OR. Default is -1 (last axis).</p> <p>Returns:</p> <p>SoftBool (Array with values in [0, 1]) with the specified axis reduced, relaxing t he logical ANY along that axis.</p>"},{"location":"api/soft_bools/#selection-with-softbools","title":"Selection with SoftBools","text":""},{"location":"api/soft_bools/#softjax.where","title":"<code>softjax.where(condition: Float[Array, '...'], x: jax.Array, y: jax.Array) -&gt; jax.Array</code> <code></code>","text":"<p>Computes a soft elementwise selection between two Arrays based on a SoftBool condition. Fuzzy logic implemented as <code>x * condition + y * (1.0 - condition)</code>.</p> <p>Arguments: - <code>condition</code>: SoftBool condition Array, same shape as <code>x</code> and <code>y</code>. - <code>x</code>: First input Array, same shape as <code>condition</code>. - <code>y</code>: Second input Array, same shape as <code>condition</code>.</p> <p>Returns:</p> <p>Array of the same shape as <code>x</code> and <code>y</code>, interpolating between <code>x</code> and <code>y</code> according to <code>condition</code> in [0, 1].</p>"},{"location":"api/soft_functions/","title":"SoftFunctions","text":""},{"location":"api/soft_functions/#softjax.relu","title":"<code>softjax.relu(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'softplus', 'silu', 'quadratic', 'quartic'] = 'softplus') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.nn.relu.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", applies <code>jax.nn.relu</code>. Otherwise uses \"softplus\", \"silu\",     \"quadratic\" spline, or \"quartic\" spline smoothing. Defaults to \"softplus\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise ReLU to <code>x</code>.</p>"},{"location":"api/soft_functions/#softjax.clip","title":"<code>softjax.clip(x: jax.Array, a: float, b: float, softness: float = 1.0, mode: Literal['hard', 'softplus', 'silu', 'quadratic', 'quartic'] = 'softplus') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.clip.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>a</code>: Lower bound scalar.</li> <li><code>b</code>: Upper bound scalar.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", applies <code>jnp.clip</code>. Otherwise smooths via \"softplus\", \"silu\",     \"quadratic\" spline, or \"quartic\" spline relaxations. Defaults to \"softplus\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise clipping to <code>x</code>.</p>"},{"location":"api/soft_functions/#softjax.abs","title":"<code>softjax.abs(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'sigmoid', 'pseudohuber', 'tanh', 'linear', 'cubic', 'quintic'] = 'sigmoid') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.abs.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: Projection mode. \"hard\" returns the exact absolute value, otherwise uses     \"sigmoid\", \"pseudohuber\", \"tanh\", \"linear\", \"cubic\", or \"quintic\" relaxations.     Defaults to \"sigmoid\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise absolute value to <code>x</code>.</p>"},{"location":"api/soft_functions/#softjax.sign","title":"<code>softjax.sign(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'sigmoid', 'pseudohuber', 'tanh', 'linear', 'cubic', 'quintic'] = 'sigmoid') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.sign.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns <code>jnp.sign</code>. Otherwise smooths via \"sigmoid\", \"linear\",     \"cubic\", or \"quintic\" relaxations. Defaults to \"sigmoid\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise sign to <code>x</code>.</p>"},{"location":"api/soft_functions/#softjax.round","title":"<code>softjax.round(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'cosine'] = 'cosine') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.round.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", applies <code>jnp.round</code>.     If \"cosine\", uses a cosine-based smoothing. Specifically, it returns a function     that has gradient 1+s*cos(2\u03c0x) for softness &lt;= 1, and (1+cos(2\u03c0x))^s/Z for     softness &gt; 1, where s is the softness parameter and Z is a normalization     constant.     Defaults to \"cosine\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise rounding to <code>x</code>.</p>"},{"location":"api/soft_functions/#softjax.median_newton","title":"<code>softjax.median_newton(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'sigmoid', 'tanh', 'pseudohuber', 'linear', 'cubic', 'quintic'] = 'sigmoid', max_iter: int = 8, eps: float = 1e-12) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.median of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: Axis along which to compute the median. If None, the input is flattened.     Defaults to None.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}. Defaults to     False.</li> <li><code>softness</code>: Softness of the score function, should be larger than zero. Defaults     to 1.0.</li> <li><code>mode</code>: Smooth score choice:<ul> <li><code>hard</code>: Returns <code>jnp.median</code>.</li> <li><code>sigmoid</code>, <code>tanh</code>, <code>pseudohuber</code>, <code>linear</code>, <code>cubic</code>, <code>quintic</code>: Smooth     relaxations for the M-estimator using Newton steps. Defaults to <code>sigmoid</code>.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of Newton iterations in the M-estimator.</li> <li><code>eps</code>: Small constant added to the derivative to avoid division by zero.</li> </ul> <p>Returns:</p> <p>Array of shape (..., {1}, ...) representing the soft median of <code>x</code> along the specified axis.</p>"},{"location":"api/soft_indices/","title":"SoftIndices","text":""},{"location":"api/soft_indices/#generating-softindices","title":"Generating SoftIndices","text":""},{"location":"api/soft_indices/#softjax.argmax","title":"<code>softjax.argmax(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic') -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.argmax of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the argmax. If None, the input Array is     flattened before computing the argmax. Defaults to None.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: Controls the type of softening:<ul> <li><code>hard</code>: Returns the result of jnp.argmax with a one-hot encoding of     the indices.</li> <li><code>entropic</code>: Returns a softmax-based relaxation of the argmax.</li> <li><code>euclidean</code>: Returns an L2-projection-based relaxation of the argmax.</li> </ul> </li> </ul> <p>Returns:</p> <p>A SoftIndex of shape (..., {1}, ..., [n]) (positive Array which sums to 1 over the last dimension). Represents the probability of an index corresponding to the argmax along the specified axis.</p> <p>Usage</p> <p>This function can be used as a differentiable relaxation to <code>softjax.argmax</code>, enabling backpropagation through index selection steps in neural networks or optimization routines. However, note that the output is not a discrete index but a <code>SoftIndex</code>, which is a distribution over indices. Therefore, functions which operate on indices have to be adjusted accordingly to accept a SoftIndex, see e.g. <code>softjax.max</code> for an example of using <code>softjax.take_along_axis</code> to retrieve the soft maximum value via the <code>SoftIndex</code>.</p> <p>Difference to jax.nn.softmax</p> <p>Note that <code>softjax.argmax</code> in <code>entropic</code> mode is not fully equivalent to jax.nn.softmax because it moves the probability dimension into the last axis (this is a convention in the <code>SoftIndex</code> data type).</p> Example <pre><code>x = jnp.array([[5, 3, 4], [2, 7, 6]])\n\n# Hard\nprint(\"jnp:\", jnp.argmax(x, axis=1))\nprint(\"sj_hard:\", sj.argmax(x, mode=\"hard\", axis=1))\n\n# Entropic (Softmax projection)\nprint(\"sj_entropic_low:\", sj.argmax(x, mode=\"entropic\", softness=0.01, axis=1))\nprint(\"sj_entropic_high:\", sj.argmax(x, mode=\"entropic\", softness=1.0, axis=1))\n\n# Euclidean (L2 projection)\nprint(\"sj_euclidean_low:\", sj.argmax(x, mode=\"euclidean\", softness=0.01,\n    axis=1))\nprint(\"sj_euclidean_high:\", sj.argmax(x, mode=\"euclidean\", softness=4.0,\n    axis=1))\n</code></pre> <pre><code>jnp: [0 1]\nsj_hard: [[1. 0. 0.]\n          [0. 1. 0.]]\nsj_entropic_low: [[1.00000000e+000 1.38389653e-087 3.72007598e-044]\n                  [7.12457641e-218 1.00000000e+000 3.72007598e-044]]\nsj_entropic_high: [[0.66524096 0.09003057 0.24472847]\n                   [0.00490169 0.72747516 0.26762315]]\nsj_euclidean_low: [[1. 0. 0.]\n                   [0. 1. 0.]]\nsj_euclidean_high: [[0.58333333 0.08333333 0.33333333]\n                    [0.         0.625      0.375     ]]\n</code></pre>"},{"location":"api/soft_indices/#softjax.argmin","title":"<code>softjax.argmin(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic') -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.argmin of <code>x</code> along the specified axis. Implemented as <code>softjax.argmax</code> on <code>-x</code>, see respective documentation for details.</p>"},{"location":"api/soft_indices/#softjax.argsort","title":"<code>softjax.argsort(x: jax.Array, axis: int | None = None, descending: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 20) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.argsort of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the argsort operation. If None, the input     Array is flattened before computing the argsort. Defaults to None.</li> <li><code>descending</code>: If True, sorts in descending order. Defaults to False (ascending).</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the type of softening:<ul> <li><code>mode=\"hard\"</code>: Returns the result of jnp.argsort with a one-hot encoding of     the indices.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations) as in     Differentiable Ranks and Sorting using Optimal Transport.     Intuition: The sorted elements are selected by specifying n \"anchors\"     and then transporting the ith-largest value to the ith-largest anchor.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via LBFGS projection onto Birkhoff polytope) as     in Fast Differentiable Sorting and Ranking.</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: Uses the \"SoftSort\" operator proposed in     SoftSort: A Continuous Relaxation for the argsort Operator.     This initializes the cost matrix based on the absolute difference of <code>x</code> to     the sorted values and then applies a single row normalization (instead of     full Sinkhorn in OT).     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax. To the best of our     knowledge this variant is novel.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <p>A SoftIndex of shape (..., n, ..., [n]) (positive Array which sums to 1 over the last dimension). The elements in (..., i, ..., [n]) represent a distribution over values in x for the ith smallest element along the specified axis.</p> <p>Computing the expectation</p> <p>Computing the soft sorted values means taking the expectation of <code>x</code> under the SoftIndex distribution. Similar to how with normal indices you would do     <pre><code>sorted_x = jnp.take_along_axis(x, indices, axis=axis)\n</code></pre> we offer the equivalent soft version via     <pre><code>soft_sorted_x = sj.take_along_axis(x, soft_indices, axis=axis)\n</code></pre> This is what is done in <code>softjax.sort</code>.</p>"},{"location":"api/soft_indices/#softjax.argtop_k","title":"<code>softjax.argtop_k(x: jax.Array, k: int, axis: int = -1, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 20) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes the soft argtop_k operation of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>k</code>: The number of top elements to select.</li> <li><code>axis</code>: The axis along which to compute the top_k operation. Defaults to -1.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the type of softening:<ul> <li><code>mode=\"hard\"</code>: Returns the result of jax.lax.top_k with a one-hot encoding of     the indices.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations) as in     Differentiable Top-k with Optimal Transport.     Intuition: The top-k elements are selected by specifying k+1 \"anchors\"     and then transporting the top_k values to the top k anchors, and the     remaining (n-k) values to the last anchor.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via projection onto Birkhoff polytope).     This version combines the approaches in Fast Differentiable Sorting and Ranking     (L2 regularizer for sorting) and Differentiable Top-k with Optimal Transport     (entropic regularizer for top-k).</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: Uses the \"SoftSort\" operator proposed  in     SoftSort: A Continuous Relaxation for the argsort Operator.     This initializes the cost matrix based on the absolute difference of <code>x</code> to     the sorted values and then applies a single row normalization (instead of     full Sinkhorn in OT).     Because this is very fast we do a full soft argsort and then take the top-k     elements.     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax. To the best of our     knowledge this variant is novel.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <p>A SoftIndex of shape (..., k, ..., [n]) (positive Array which sums to 1 over the last dimension). The elements in (..., i, ..., [n]) represent a distribution over values in x for the ith largest element along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.ranking","title":"<code>softjax.ranking(x: jax.Array, axis: int | None = None, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 20, descending: bool = True) -&gt; jax.Array</code> <code></code>","text":"<p>Computes the soft rankings of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the ranking operation. If None, the input     Array is flattened before computing the ranking. Defaults to None.</li> <li><code>descending</code>: If True, larger inputs receive smaller ranks (best rank is 0). If     False, ranks increase with the input values.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the behavior of the ranking     operation:<ul> <li><code>mode=\"hard\"</code>: Returns ranking computed as two jnp.argsort calls.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations) as in     Differentiable Ranks and Sorting using Optimal Transport.     Intuition: We can use the transportation plan obtained in soft sorting for     ranking by transporting the sorted ranks (0, 1, ..., n-1) back to the     ranks of the original values.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via projection onto Birkhoff polytope) as in     Fast Differentiable Sorting and Ranking.</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: Uses an adaptation of the \"SoftSort\"     operator proposed in SoftSort: A Continuous Relaxation for the argsort Operator.     This initializes the cost matrix based on the absolute difference of <code>x</code> to     the sorted values and then we crucially apply a single column     normalization (instead of of row normalization in the original paper).     This makes the resulting matrix a unimodal column stochastic matrix which is     better suited for soft ranking.     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax. To the best of our     knowledge this variant is novel.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <p>A positive Array of shape (..., n, ...) with values in [0, n-1]. The elements in (..., i, ...) represent the soft rank of the ith element along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.argmedian","title":"<code>softjax.argmedian(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 20) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes the soft argmedian of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the median. If None, the input Array is     flattened before computing the median. Defaults to None.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the behavior of the median:<ul> <li><code>mode=\"hard\"</code>: Returns the result of jnp.median with a one-hot encoding of     the indices. On ties, it returns a uniform distribution over all median     indices.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations).     We adapt the approach in Differentiable Ranks and Sorting     using Optimal Transport and     Differentiable Top-k with Optimal Transport     to the median operation by carefully adjusting the cost matrix and     marginals.     Intuition: There are three \"anchors\", the median is transported onto one     anchor, and all the larger and smaller elements are transported to the other     two anchors, respectively.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via projection onto Birkhoff polytope).</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: This formulation a well-known soft median     operation based on the interpretation of the median as the minimizer of     absolute deviations. The softening is then achieved by replacing the argmax     operator with a softmax. Note, that this also has close ties to the     \"SoftSort\" operator from SoftSort: A Continuous Relaxation for the argsort Operator.     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <p>A SoftIndex of shape (..., {1}, ..., [n]) (positive Array which sums to 1 over the last dimension). The elements in (..., 0, ...) represent a distribution over values in x being the median along the specified axis.</p>"},{"location":"api/soft_indices/#selection-with-softindices","title":"Selection with SoftIndices","text":""},{"location":"api/soft_indices/#softjax.take_along_axis","title":"<code>softjax.take_along_axis(x: jax.Array, soft_indices: Float[Array, '...'], axis: int = -1) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.take_along_axis via a weighted dot product.</p> <p>Relation to <code>jnp.take_along_axis</code></p> <pre><code>x = jnp.array([[1, 2, 3], [4, 5, 6]])\n\nindices = jnp.array([[0, 2], [1, 0]])\nprint(jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = jax.nn.one_hot(indices, x.shape[1])\nprint(sj.take_along_axis(x, indices_onehot, axis=1))\n</code></pre> <pre><code>[[1. 3.]\n [5. 4.]]\n[[1. 3.]\n [5. 4.]]\n</code></pre> Interaction with <code>softjax.argmax</code> <pre><code>x = jnp.array([[5, 3, 4], [2, 7, 6]])\n\nindices = jnp.argmin(x, axis=1, keepdims=True)\nprint(\"argmin_jnp:\", jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = sj.argmin(x, axis=1, mode=\"hard\", keepdims=True)\nprint(\"argmin_val_onehot:\", sj.take_along_axis(x, indices_onehot, axis=1))\n\nindices_soft = sj.argmin(x, axis=1, mode=\"entropic\", softness=1.0,\n    keepdims=True)\nprint(\"argmin_val_soft:\", sj.take_along_axis(x, indices_soft, axis=1))\n</code></pre> <pre><code>argmin_jnp: [[3]\n             [2]]\nargmin_val_onehot: [[3.]\n                    [2.]]\nargmin_val_soft: [[3.42478962]\n                  [2.10433824]]\n</code></pre> Interaction with <code>softjax.argsort</code> <pre><code>x = jnp.array([[5, 3, 4], [2, 7, 6]])\n\nindices = jnp.argsort(x, axis=1)\nprint(\"sorted_jnp:\", jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = sj.argsort(x, axis=1, mode=\"hard\")\nprint(\"sorted_sj_hard:\", sj.take_along_axis(x, indices_onehot, axis=1))\n\nindices_soft = sj.argsort(x, axis=1, mode=\"entropic\", softness=1.0)\nprint(\"sorted_sj_soft:\", sj.take_along_axis(x, indices_soft, axis=1))\n</code></pre> <pre><code>sorted_jnp: [[3 4 5]\n             [2 6 7]]\nsorted_sj_hard: [[3. 4. 5.]\n                 [2. 6. 7.]]\nsorted_sj_soft: [[3.2918137  4.         4.7081863 ]\n                 [2.00000045 6.26894107 6.73105858]]\n</code></pre> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_indices</code>: A SoftIndex of shape (..., k, ..., [n]) (positive Array which     sums to 1 over the last dimension).</li> <li><code>axis</code>: Axis along which to apply the soft index. Defaults to -1.</li> </ul> <p>Returns:</p> <p>Array of shape (..., k, ...), representing the result after soft selection along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.take","title":"<code>softjax.take(x: jax.Array, soft_indices: Float[Array, '...'], axis: int | None = None) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.take via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_indices</code>: A SoftIndex of shape (k, [n]) (positive Array which     sums to 1 over the last dimension).</li> <li><code>axis</code>: Axis along which to apply the soft index. If None, the input is     flattened. Defaults to None.</li> </ul> <p>Returns:</p> <p>Array of shape (..., k, ...) after soft selection.</p>"},{"location":"api/soft_indices/#softjax.choose","title":"<code>softjax.choose(soft_indices: Float[Array, '...'], choices: jax.Array) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.choose via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>soft_indices</code>: A SoftIndex of shape (..., [n]) (positive Array which     sums to 1 over the last dimension). Represents the weights for each choice.</li> <li><code>choices</code>: Array of shape (n, ...) supplying the values to mix.</li> </ul> <p>Returns:</p> <p>Array of shape (..., ...) after softly selecting among <code>choices</code>.</p>"},{"location":"api/soft_indices/#softjax.dynamic_index_in_dim","title":"<code>softjax.dynamic_index_in_dim(x: jax.Array, soft_index: Float[Array, '...'], axis: int = 0, keepdims: bool = True) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.lax.dynamic_index_in_dim via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_indices</code>: A SoftIndex of shape ([n],) (positive Array which     sums to 1 over the last dimension).</li> <li><code>axis</code>: Axis along which to apply the soft index. Defaults to 0.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}.</li> </ul> <p>Returns:</p> <p>Array after soft indexing, shape (..., {1}, ...).</p>"},{"location":"api/soft_indices/#softjax.dynamic_slice_in_dim","title":"<code>softjax.dynamic_slice_in_dim(x: jax.Array, soft_start_index: Float[Array, '...'], slice_size: int, axis: int = 0) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.lax.dynamic_slice_in_dim via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_indices</code>: A SoftIndex of shape ([n],) (positive Array which     sums to 1 over the last dimension).</li> <li><code>slice_size</code>: Length of the slice to extract.</li> <li><code>axis</code>: Axis along which to apply the soft slice. Defaults to 0.</li> </ul> <p>Returns:</p> <p>Array of shape (..., slice_size, ...) after soft slicing.</p>"},{"location":"api/soft_indices/#softjax.dynamic_slice","title":"<code>softjax.dynamic_slice(x: jax.Array, soft_start_indices: Sequence[Float[Array, '...']], slice_sizes: Sequence[int]) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.lax.dynamic_slice via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (n_1, n_2, ..., n_k).</li> <li><code>soft_start_indices</code>: A list of SoftIndices of shape ([n_i],) (positive Arrays     which sums to 1). Sequence of SoftIndex distributions of shapes     ([n_1],), ([n_2],), ..., ([n_k]) each summing to 1.</li> <li><code>slice_sizes</code>: Sequence of slice lengths for each dimension.</li> </ul> <p>Returns:</p> <p>Array of shape (l_1, l_2, ..., l_k) after soft slicing.</p>"},{"location":"api/soft_indices/#examples-of-selection","title":"Examples of Selection","text":""},{"location":"api/soft_indices/#softjax.max","title":"<code>softjax.max(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.max of <code>x</code> along the specified axis. Implemented as <code>softjax.argmax</code> followed by <code>softjax.take_along_axis</code>, see respective documentations for details.</p> <p>Returns:</p> <p>Array of shape (..., {1}, ...) representing the soft maximum of <code>x</code> along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.min","title":"<code>softjax.min(x: jax.Array, axis: int | None = None, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', keepdims: bool = False) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.min of <code>x</code> along the specified axis. Implemented as -<code>softjax.max</code> on <code>-x</code>, see respective documentation for details.</p> <p>Returns:</p> <p>Array of shape (..., {1}, ...) representing the soft minimum of <code>x</code> along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.median","title":"<code>softjax.median(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 20) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jnp.median of <code>x</code> along the specified axis. Implemented as <code>softjax.argmedian</code> followed by <code>softjax.take_along_axis</code>, see respective documentations for details.</p> <p>Returns:</p> <p>An Array of shape (..., {1}, ...), representing the soft median values along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.sort","title":"<code>softjax.sort(x: jax.Array, axis: int | None = None, descending: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 20) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.sort of <code>x</code> along the specified axis. Implemented as <code>softjax.argsort</code> followed by <code>softjax.take_along_axis</code>, see respective documentations for details.</p> <p>Returns:</p> <p>Array of shape (..., n, ...) representing the soft sorted values of <code>x</code> along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.top_k","title":"<code>softjax.top_k(x: jax.Array, k: int, axis: int = -1, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 20) -&gt; tuple[jax.Array, Float[Array, '...']]</code> <code></code>","text":"<p>Performs a soft version of jax.lax.top_k of <code>x</code> along the specified axis. Implemented as <code>softjax.argtop_k</code> followed by <code>softjax.take_along_axis</code>, see respective documentations for details.</p> <p>Returns:</p> <ul> <li><code>soft_values</code>: Top-k values of <code>x</code>, shape (..., k, ...).</li> <li><code>soft_indices</code>: SoftIndex of shape (..., k, ..., [n]) (positive Array which sums     to 1 over the last dimension). Represents the soft indices of the top-k values.</li> </ul>"},{"location":"api/straight_through/","title":"Straight-through","text":""},{"location":"api/straight_through/#softjax.grad_replace","title":"<code>softjax.grad_replace(fn: typing.Callable) -&gt; typing.Callable</code> <code></code>","text":"<p>This decorator calls the decorated function twice: once with <code>forward=True</code> and once with <code>forward=False</code>. It returns the output from the forward pass, but uses the output from the backward pass to compute gradients.</p> <p>Arguments:</p> <ul> <li><code>fn</code>: The function to be wrapped. It should accept a <code>forward</code> argument     that specifies which computation to perform depending on forward or     backward pass.</li> </ul> <p>Returns:     A wrapped function that behaves like the <code>forward=True</code> version during the     forward pass, but computes gradients using the <code>forward=False</code> version     during the backward pass.</p>"},{"location":"api/straight_through/#softjax.st","title":"<code>softjax.st(fn: typing.Callable) -&gt; typing.Callable</code> <code></code>","text":"<p>This decorator calls the decorated function twice: once with <code>mode=\"hard\"</code> and once with the specified <code>mode</code>. It returns the output from the hard forward pass, but uses the output from the soft backward pass to compute gradients.</p> <p>Arguments:</p> <ul> <li><code>fn</code>: The function to be wrapped. It should accept a <code>mode</code> argument.</li> </ul> <p>Returns:     A wrapped function that behaves like the <code>mode=\"hard\"</code> version during the     forward pass, but computes gradients using the specified <code>mode</code> and <code>softness</code>     during the backward pass.</p>"},{"location":"api/straight_through/#softjax.argmax_st","title":"<code>softjax.argmax_st(*args, **kwargs)</code> <code></code>","text":"<p>Straight-through version of <code>softjax.argmax</code>.</p> <p>This function returns the hard <code>argmax</code> during the forward pass, but uses a soft relaxation (controlled by the <code>mode</code> argument) for the backward pass (i.e., gradients are computed through the soft version).</p> <p>Implemented using the <code>softjax.st</code> decorator as <code>st(softjax.argmax)</code>.</p>"},{"location":"api/straight_through/#softjax.argmin_st","title":"<code>softjax.argmin_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.argsort_st","title":"<code>softjax.argsort_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.clip_st","title":"<code>softjax.clip_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.equal_st","title":"<code>softjax.equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.greater_st","title":"<code>softjax.greater_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.greater_equal_st","title":"<code>softjax.greater_equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.heaviside_st","title":"<code>softjax.heaviside_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.isclose_st","title":"<code>softjax.isclose_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.less_st","title":"<code>softjax.less_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.less_equal_st","title":"<code>softjax.less_equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.max_st","title":"<code>softjax.max_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.median_st","title":"<code>softjax.median_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.median_newton_st","title":"<code>softjax.median_newton_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.min_st","title":"<code>softjax.min_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.not_equal_st","title":"<code>softjax.not_equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.ranking_st","title":"<code>softjax.ranking_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.relu_st","title":"<code>softjax.relu_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.round_st","title":"<code>softjax.round_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.sign_st","title":"<code>softjax.sign_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.sort_st","title":"<code>softjax.sort_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.top_k_st","title":"<code>softjax.top_k_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"testing/test/","title":"Test","text":"In\u00a0[1]: Copied! <pre>import os\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport softjax as sj\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom functools import partial\n\n\nos.environ[\"JAX_ENABLE_X64\"] = \"true\"\nos.environ[\"JAX_DEFAULT_MATMUL_PRECISION\"] = \"high\"\nos.environ[\"JAX_DEBUG_NANS\"] = \"false\"\nos.environ[\"JAX_COMPILATION_CACHE_DIR\"] = os.environ[\"JAX_CACHE_DIR\"]\nos.environ[\"JAX_PERSISTENT_CACHE_ENABLE_XLA_CACHES\"] = (\n    \"xla_gpu_per_fusion_autotune_cache_dir\"\n)\njax.config.update(\"jax_enable_x64\", True)\n</pre> import os  import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpy as np import softjax as sj from matplotlib.colors import LinearSegmentedColormap from functools import partial   os.environ[\"JAX_ENABLE_X64\"] = \"true\" os.environ[\"JAX_DEFAULT_MATMUL_PRECISION\"] = \"high\" os.environ[\"JAX_DEBUG_NANS\"] = \"false\" os.environ[\"JAX_COMPILATION_CACHE_DIR\"] = os.environ[\"JAX_CACHE_DIR\"] os.environ[\"JAX_PERSISTENT_CACHE_ENABLE_XLA_CACHES\"] = (     \"xla_gpu_per_fusion_autotune_cache_dir\" ) jax.config.update(\"jax_enable_x64\", True) In\u00a0[2]: Copied! <pre>softness = 0.1\nx = jnp.array([-0.1, 0.0, 0.7, 1.8])\ny = jnp.array([0.2, -0.5, 0.5, -1.0])\n\n# Elementwise functions\nprint(\"Hard ReLU:\", jax.nn.relu(x))\nprint(\"Soft ReLU:\", sj.relu(x, softness=softness))\nprint(\"Hard Clip:\", jnp.clip(x, -0.5, 0.5))\nprint(\"Soft Clip:\", sj.clip(x, -0.5, 0.5, softness=softness))\nprint(\"Hard Absolute:\", jnp.abs(x))\nprint(\"Soft Absolute:\", sj.abs(x, softness=softness))\nprint(\"Hard Sign:\", jnp.sign(x))\nprint(\"Soft Sign:\", sj.sign(x, softness=softness))\nprint(\"Hard round:\", jnp.round(x))\nprint(\"Soft round:\", sj.round(x, softness=softness))\n\n# Functions on arrays\nprint(\"Hard max:\", jnp.max(x))\nprint(\"Soft max:\", sj.max(x, softness=softness))\nprint(\"Hard min:\", jnp.min(x))\nprint(\"Soft min:\", sj.min(x, softness=softness))\nprint(\"Hard median:\", jnp.median(x))\nprint(\"Soft median:\", sj.median(x, softness=softness))\nprint(\"Hard top_k:\", jax.lax.top_k(x, k=3)[0])\nprint(\"Soft top_k:\", sj.top_k(x, k=3, softness=softness)[0])\nprint(\"Hard sort:\", jnp.sort(x))\nprint(\"Soft sort:\", sj.sort(x, softness=softness))\nprint(\"Hard ranking:\", jnp.argsort(jnp.argsort(x)))\nprint(\"Soft ranking:\", sj.ranking(x, softness=softness))\n\n# Straight-through estimation: Use hard function on forward and soft on backward\nprint(\"Straight-through sort:\", sj.sort_st(x, softness=softness))\n\n# Functions returning indices\nprint(\"Hard argmax:\", jnp.argmax(x))\nprint(\"Soft argmax:\", sj.argmax(x, softness=softness))\nprint(\"Hard argmin:\", jnp.argmin(x))\nprint(\"Soft argmin:\", sj.argmin(x, softness=softness))\nprint(\"Hard argmedian:\", \"Not implemented in standard JAX\")\nprint(\"Soft argmedian:\", sj.argmedian(x, softness=softness))\nprint(\"Hard argtop_k:\", jax.lax.top_k(x, k=3)[1])\nprint(\"Soft argtop_k:\", sj.top_k(x, k=3, softness=softness)[1])\nprint(\"Hard argsort:\", jnp.argsort(x))\nprint(\"Soft argsort:\", sj.argsort(x, softness=softness))\n\n## SoftBool generation\nprint(\"Hard heaviside:\", jnp.heaviside(x, 0.5))\nprint(\"Soft heaviside:\", sj.heaviside(x, softness=softness))\nprint(\"Hard greater:\", x &gt; y)\nprint(\"Soft greater:\", sj.greater(x, y, softness=softness))\nprint(\"Hard greater equal:\", x &gt;= y)\nprint(\"Soft greater equal:\", sj.greater_equal(x, y, softness=softness))\nprint(\"Hard less:\", x &lt; y)\nprint(\"Soft less:\", sj.less(x, y, softness=softness))\nprint(\"Hard less equal:\", x &lt;= y)\nprint(\"Soft less equal:\", sj.less_equal(x, y, softness=softness))\nprint(\"Hard equal:\", x == y)\nprint(\"Soft equal:\", sj.equal(x, y, softness=softness))\nprint(\"Hard not equal:\", x != y)\nprint(\"Soft not equal:\", sj.not_equal(x, y, softness=softness))\nprint(\"Hard isclose:\", jnp.isclose(x, y))\nprint(\"Soft isclose:\", sj.isclose(x, y, softness=softness))\n\n## SoftBool manipulation\nfuzzy_a = jnp.array([0.1, 0.2, 0.8, 1.0])\nfuzzy_b = jnp.array([0.7, 0.3, 0.1, 0.9])\nprint(\"Soft AND:\", sj.logical_and(fuzzy_a, fuzzy_b))\nprint(\"Soft OR:\", sj.logical_or(fuzzy_a, fuzzy_b))\nprint(\"Soft NOT:\", sj.logical_not(fuzzy_a))\nprint(\"Soft XOR:\", sj.logical_xor(fuzzy_a, fuzzy_b))\nprint(\"Soft ALL:\", sj.all(fuzzy_a))\nprint(\"Soft ANY:\", sj.any(fuzzy_a))\n\n## SoftBool selection\nprint(\"Where:\", sj.where(fuzzy_a, x, y))\n</pre> softness = 0.1 x = jnp.array([-0.1, 0.0, 0.7, 1.8]) y = jnp.array([0.2, -0.5, 0.5, -1.0])  # Elementwise functions print(\"Hard ReLU:\", jax.nn.relu(x)) print(\"Soft ReLU:\", sj.relu(x, softness=softness)) print(\"Hard Clip:\", jnp.clip(x, -0.5, 0.5)) print(\"Soft Clip:\", sj.clip(x, -0.5, 0.5, softness=softness)) print(\"Hard Absolute:\", jnp.abs(x)) print(\"Soft Absolute:\", sj.abs(x, softness=softness)) print(\"Hard Sign:\", jnp.sign(x)) print(\"Soft Sign:\", sj.sign(x, softness=softness)) print(\"Hard round:\", jnp.round(x)) print(\"Soft round:\", sj.round(x, softness=softness))  # Functions on arrays print(\"Hard max:\", jnp.max(x)) print(\"Soft max:\", sj.max(x, softness=softness)) print(\"Hard min:\", jnp.min(x)) print(\"Soft min:\", sj.min(x, softness=softness)) print(\"Hard median:\", jnp.median(x)) print(\"Soft median:\", sj.median(x, softness=softness)) print(\"Hard top_k:\", jax.lax.top_k(x, k=3)[0]) print(\"Soft top_k:\", sj.top_k(x, k=3, softness=softness)[0]) print(\"Hard sort:\", jnp.sort(x)) print(\"Soft sort:\", sj.sort(x, softness=softness)) print(\"Hard ranking:\", jnp.argsort(jnp.argsort(x))) print(\"Soft ranking:\", sj.ranking(x, softness=softness))  # Straight-through estimation: Use hard function on forward and soft on backward print(\"Straight-through sort:\", sj.sort_st(x, softness=softness))  # Functions returning indices print(\"Hard argmax:\", jnp.argmax(x)) print(\"Soft argmax:\", sj.argmax(x, softness=softness)) print(\"Hard argmin:\", jnp.argmin(x)) print(\"Soft argmin:\", sj.argmin(x, softness=softness)) print(\"Hard argmedian:\", \"Not implemented in standard JAX\") print(\"Soft argmedian:\", sj.argmedian(x, softness=softness)) print(\"Hard argtop_k:\", jax.lax.top_k(x, k=3)[1]) print(\"Soft argtop_k:\", sj.top_k(x, k=3, softness=softness)[1]) print(\"Hard argsort:\", jnp.argsort(x)) print(\"Soft argsort:\", sj.argsort(x, softness=softness))  ## SoftBool generation print(\"Hard heaviside:\", jnp.heaviside(x, 0.5)) print(\"Soft heaviside:\", sj.heaviside(x, softness=softness)) print(\"Hard greater:\", x &gt; y) print(\"Soft greater:\", sj.greater(x, y, softness=softness)) print(\"Hard greater equal:\", x &gt;= y) print(\"Soft greater equal:\", sj.greater_equal(x, y, softness=softness)) print(\"Hard less:\", x &lt; y) print(\"Soft less:\", sj.less(x, y, softness=softness)) print(\"Hard less equal:\", x &lt;= y) print(\"Soft less equal:\", sj.less_equal(x, y, softness=softness)) print(\"Hard equal:\", x == y) print(\"Soft equal:\", sj.equal(x, y, softness=softness)) print(\"Hard not equal:\", x != y) print(\"Soft not equal:\", sj.not_equal(x, y, softness=softness)) print(\"Hard isclose:\", jnp.isclose(x, y)) print(\"Soft isclose:\", sj.isclose(x, y, softness=softness))  ## SoftBool manipulation fuzzy_a = jnp.array([0.1, 0.2, 0.8, 1.0]) fuzzy_b = jnp.array([0.7, 0.3, 0.1, 0.9]) print(\"Soft AND:\", sj.logical_and(fuzzy_a, fuzzy_b)) print(\"Soft OR:\", sj.logical_or(fuzzy_a, fuzzy_b)) print(\"Soft NOT:\", sj.logical_not(fuzzy_a)) print(\"Soft XOR:\", sj.logical_xor(fuzzy_a, fuzzy_b)) print(\"Soft ALL:\", sj.all(fuzzy_a)) print(\"Soft ANY:\", sj.any(fuzzy_a))  ## SoftBool selection print(\"Where:\", sj.where(fuzzy_a, x, y)) <pre>Hard ReLU: [0.  0.  0.7 1.8]\n</pre> <pre>Soft ReLU: [0.03132617 0.06931472 0.70009115 1.8       ]\nHard Clip: [-0.1  0.   0.5  0.5]\nSoft Clip: [-9.84325757e-02 -3.46944695e-18  4.87307813e-01  4.99999774e-01]\n</pre> <pre>Hard Absolute: [0.1 0.  0.7 1.8]\nSoft Absolute: [0.04621172 0.         0.69872453 1.79999995]\nHard Sign: [-1.  0.  1.  1.]\nSoft Sign: [-0.46211716  0.          0.9981779   0.99999997]\nHard round: [-0.  0.  1.  2.]\n</pre> <pre>Soft round: [-0.09064511  0.          0.71513653  1.81513653]\nHard max: 1.8\n</pre> <pre>Soft max: 1.7999815903777097\nHard min: -0.1\nSoft min: -0.07291629800981214\n</pre> <pre>Hard median: 0.35\n</pre> <pre>Soft median: 0.24772037254528773\nHard top_k: [1.8 0.7 0. ]\n</pre> <pre>Soft top_k: [ 1.79998159  0.69911281 -0.02640987]\nHard sort: [-0.1  0.   0.7  1.8]\n</pre> <pre>Soft sort: [-0.0729163  -0.02640987  0.69911281  1.79998159]\nHard ranking: [0 1 2 3]\n</pre> <pre>Soft ranking: [2.73063414e+00 2.26809603e+00 1.00156413e+00 1.67486891e-05]\nStraight-through sort: [-0.1  0.   0.7  1.8]\nHard argmax: 3\nSoft argmax: [5.60270275e-09 1.52297251e-08 1.67014215e-05 9.99983278e-01]\n</pre> <pre>Hard argmin: 0\nSoft argmin: [7.30879333e-01 2.68875480e-01 2.45182702e-04 4.09496812e-09]\nHard argmedian: Not implemented in standard JAX\nSoft argmedian: [0.23233226 0.38305115 0.38305115 0.00156544]\nHard argtop_k: [3 2 1]\nSoft argtop_k: [[5.60270275e-09 1.52297251e-08 1.67014215e-05 9.99983278e-01]\n [3.35039123e-04 9.10730760e-04 9.98737550e-01 1.66806157e-05]\n [2.68762251e-01 7.30571543e-01 6.66195015e-04 1.11265898e-08]]\nHard argsort: [0 1 2 3]\nSoft argsort: [[7.30879333e-01 2.68875480e-01 2.45182702e-04 4.09496812e-09]\n [2.68762251e-01 7.30571543e-01 6.66195015e-04 1.11265898e-08]\n [3.35039123e-04 9.10730760e-04 9.98737550e-01 1.66806157e-05]\n [5.60270275e-09 1.52297251e-08 1.67014215e-05 9.99983278e-01]]\nHard heaviside: [0.  0.5 1.  1. ]\nSoft heaviside: [0.26894142 0.5        0.99908895 0.99999998]\nHard greater: [False  True  True  True]\nSoft greater: [0.04742587 0.99330715 0.88079708 1.        ]\nHard greater equal: [False  True  True  True]\nSoft greater equal: [0.04742587 0.99330715 0.88079708 1.        ]\n</pre> <pre>Hard less: [ True False False False]\nSoft less: [9.52574127e-01 6.69285092e-03 1.19202922e-01 6.91446900e-13]\nHard less equal: [ True False False False]\nSoft less equal: [9.52574127e-01 6.69285093e-03 1.19202922e-01 6.91446900e-13]\nHard equal: [False False False False]\nSoft equal: [4.74258732e-02 6.69285093e-03 1.19202922e-01 6.91446900e-13]\nHard not equal: [ True  True  True  True]\nSoft not equal: [0.95257413 0.99330715 0.88079708 1.        ]\n</pre> <pre>Hard isclose: [False False False False]\nSoft isclose: [4.74267813e-02 6.69318401e-03 1.19208182e-01 6.91446900e-13]\n</pre> <pre>Soft AND: [0.26457513 0.24494897 0.28284271 0.9486833 ]\nSoft OR: [0.48038476 0.25166852 0.57573593 0.99999684]\nSoft NOT: [0.9 0.8 0.2 0. ]\nSoft XOR: [0.58702688 0.43498731 0.63937484 0.17309871]\n</pre> <pre>Soft ALL: 0.3556558820077846\nSoft ANY: 0.9980519925071494\nWhere: [ 0.17 -0.4   0.66  1.8 ]\n</pre> In\u00a0[3]: Copied! <pre># max_iter = 100\n# fast = False\n\nmax_iter = 100\nfast = True\n</pre> # max_iter = 100 # fast = False  max_iter = 100 fast = True In\u00a0[4]: Copied! <pre># x = jnp.array([8, 2, 5, 9])\n# k = 3\n# softness_l = 0.1\n# softness_h = 10.0\n# axis = 0\n\n# # # Median\n# # median_fn = lambda x, mode, softness: sj.median(\n# #     x,\n# #     mode=mode,\n# #     softness=softness,\n# #     axis=axis,\n# #     max_iter=max_iter,\n# # )\n# # median_hard = median_fn(x, mode=\"hard\", softness=None)\n# # median_logcosh_l = median_fn(x, mode=\"logcosh\", softness=softness_l)\n# # median_logcosh_h = median_fn(x, mode=\"logcosh\", softness=softness_h)\n# # median_pseudo_huber_l = median_fn(x, mode=\"pseudo_huber\", softness=softness_l)\n# # median_pseudo_huber_h = median_fn(x, mode=\"pseudo_huber\", softness=softness_h)\n# # print(\"Hard median:\", median_hard)\n# # print(\"Log-Cosh median (low temp):\", median_logcosh_l)\n# # print(\"Log-Cosh median (high temp):\", median_logcosh_h)\n# # print(\"Pseudo-Huber median (low temp):\", median_pseudo_huber_l)\n# # print(\"Pseudo-Huber median (high temp):\", median_pseudo_huber_h)\n\n# # Sort\n# sort_fn = lambda x, mode, softness: sj.sort(\n#     x,\n#     mode=mode,\n#     softness=softness,\n#     axis=axis,\n#     max_iter=max_iter,\n#     fast=fast,\n# )\n# x_sorted_hard = sort_fn(x, mode=\"hard\", softness=None)\n# x_sorted_ent_l = sort_fn(x, mode=\"entropic\", softness=softness_l)\n# x_sorted_ent_h = sort_fn(x, mode=\"entropic\", softness=softness_h)\n# x_sorted_euc_l = sort_fn(x, mode=\"euclidean\", softness=softness_l)\n# x_sorted_euc_h = sort_fn(x, mode=\"euclidean\", softness=softness_h)\n# print(\"Hard sort:\", x_sorted_hard)\n# print(\"Entropic sort (low temp):\", x_sorted_ent_l)\n# print(\"Entropic sort (high temp):\", x_sorted_ent_h)\n# print(\"Euclidean sort (low temp):\", x_sorted_euc_l)\n# print(\"Euclidean sort (high temp):\", x_sorted_euc_h)\n\n# # Ranking\n# ranking_fn = lambda x, mode, softness: sj.ranking(\n#     x,\n#     mode=mode,\n#     softness=softness,\n#     axis=axis,\n#     max_iter=max_iter,\n#     fast=fast,\n# )\n# x_ranking_hard = ranking_fn(x, mode=\"hard\", softness=None)\n# x_ranking_ent_l = ranking_fn(x, mode=\"entropic\", softness=softness_l)\n# x_ranking_ent_h = ranking_fn(x, mode=\"entropic\", softness=softness_h)\n# x_ranking_euc_l = ranking_fn(x, mode=\"euclidean\", softness=softness_l)\n# x_ranking_euc_h = ranking_fn(x, mode=\"euclidean\", softness=softness_h)\n# print(\"Hard ranking:\", x_ranking_hard)\n# print(\"Entropic ranking (low temp):\", x_ranking_ent_l)\n# print(\"Entropic ranking (high temp):\", x_ranking_ent_h)\n# print(\"Euclidean ranking (low temp):\", x_ranking_euc_l)\n# print(\"Euclidean ranking (high temp):\", x_ranking_euc_h)\n\n# # Top-k\n# topk_fn = lambda x, mode, softness: sj.top_k(\n#     x,\n#     k=k,\n#     mode=mode,\n#     softness=softness,\n#     axis=axis,\n#     max_iter=max_iter,\n#     fast=fast,\n# )\n# x_topk_hard, x_argtopk_hard = topk_fn(x, mode=\"hard\", softness=None)\n# x_topk_ent_l, x_argtopk_l = topk_fn(x, mode=\"entropic\", softness=softness_l)\n# x_topk_ent_h, x_argtopk_h = topk_fn(x, mode=\"entropic\", softness=softness_h)\n# x_topk_euc_l, x_argtopk_euc_l = topk_fn(x, mode=\"euclidean\", softness=softness_l)\n# x_topk_euc_h, x_argtopk_euc_h = topk_fn(x, mode=\"euclidean\", softness=softness_h)\n\n# print(\"Hard top-k:\", x_topk_hard)\n# print(\"Entropic top-k (low temp):\", x_topk_ent_l)\n# print(\"Entropic top-k (high temp):\", x_topk_ent_h)\n# print(\"Euclidean top-k (low temp):\", x_topk_euc_l)\n# print(\"Euclidean top-k (high temp):\", x_topk_euc_h)\n\n# print(\"Hard argtop-k:\", x_argtopk_hard)\n# print(\"Entropic argtop-k (low temp):\", x_argtopk_l)\n# print(\"Entropic argtop-k (high temp):\", x_argtopk_h)\n# print(\"Euclidean argtop-k (low temp):\", x_argtopk_euc_l)\n# print(\"Euclidean argtop-k (high temp):\", x_argtopk_euc_h)\n</pre> # x = jnp.array([8, 2, 5, 9]) # k = 3 # softness_l = 0.1 # softness_h = 10.0 # axis = 0  # # # Median # # median_fn = lambda x, mode, softness: sj.median( # #     x, # #     mode=mode, # #     softness=softness, # #     axis=axis, # #     max_iter=max_iter, # # ) # # median_hard = median_fn(x, mode=\"hard\", softness=None) # # median_logcosh_l = median_fn(x, mode=\"logcosh\", softness=softness_l) # # median_logcosh_h = median_fn(x, mode=\"logcosh\", softness=softness_h) # # median_pseudo_huber_l = median_fn(x, mode=\"pseudo_huber\", softness=softness_l) # # median_pseudo_huber_h = median_fn(x, mode=\"pseudo_huber\", softness=softness_h) # # print(\"Hard median:\", median_hard) # # print(\"Log-Cosh median (low temp):\", median_logcosh_l) # # print(\"Log-Cosh median (high temp):\", median_logcosh_h) # # print(\"Pseudo-Huber median (low temp):\", median_pseudo_huber_l) # # print(\"Pseudo-Huber median (high temp):\", median_pseudo_huber_h)  # # Sort # sort_fn = lambda x, mode, softness: sj.sort( #     x, #     mode=mode, #     softness=softness, #     axis=axis, #     max_iter=max_iter, #     fast=fast, # ) # x_sorted_hard = sort_fn(x, mode=\"hard\", softness=None) # x_sorted_ent_l = sort_fn(x, mode=\"entropic\", softness=softness_l) # x_sorted_ent_h = sort_fn(x, mode=\"entropic\", softness=softness_h) # x_sorted_euc_l = sort_fn(x, mode=\"euclidean\", softness=softness_l) # x_sorted_euc_h = sort_fn(x, mode=\"euclidean\", softness=softness_h) # print(\"Hard sort:\", x_sorted_hard) # print(\"Entropic sort (low temp):\", x_sorted_ent_l) # print(\"Entropic sort (high temp):\", x_sorted_ent_h) # print(\"Euclidean sort (low temp):\", x_sorted_euc_l) # print(\"Euclidean sort (high temp):\", x_sorted_euc_h)  # # Ranking # ranking_fn = lambda x, mode, softness: sj.ranking( #     x, #     mode=mode, #     softness=softness, #     axis=axis, #     max_iter=max_iter, #     fast=fast, # ) # x_ranking_hard = ranking_fn(x, mode=\"hard\", softness=None) # x_ranking_ent_l = ranking_fn(x, mode=\"entropic\", softness=softness_l) # x_ranking_ent_h = ranking_fn(x, mode=\"entropic\", softness=softness_h) # x_ranking_euc_l = ranking_fn(x, mode=\"euclidean\", softness=softness_l) # x_ranking_euc_h = ranking_fn(x, mode=\"euclidean\", softness=softness_h) # print(\"Hard ranking:\", x_ranking_hard) # print(\"Entropic ranking (low temp):\", x_ranking_ent_l) # print(\"Entropic ranking (high temp):\", x_ranking_ent_h) # print(\"Euclidean ranking (low temp):\", x_ranking_euc_l) # print(\"Euclidean ranking (high temp):\", x_ranking_euc_h)  # # Top-k # topk_fn = lambda x, mode, softness: sj.top_k( #     x, #     k=k, #     mode=mode, #     softness=softness, #     axis=axis, #     max_iter=max_iter, #     fast=fast, # ) # x_topk_hard, x_argtopk_hard = topk_fn(x, mode=\"hard\", softness=None) # x_topk_ent_l, x_argtopk_l = topk_fn(x, mode=\"entropic\", softness=softness_l) # x_topk_ent_h, x_argtopk_h = topk_fn(x, mode=\"entropic\", softness=softness_h) # x_topk_euc_l, x_argtopk_euc_l = topk_fn(x, mode=\"euclidean\", softness=softness_l) # x_topk_euc_h, x_argtopk_euc_h = topk_fn(x, mode=\"euclidean\", softness=softness_h)  # print(\"Hard top-k:\", x_topk_hard) # print(\"Entropic top-k (low temp):\", x_topk_ent_l) # print(\"Entropic top-k (high temp):\", x_topk_ent_h) # print(\"Euclidean top-k (low temp):\", x_topk_euc_l) # print(\"Euclidean top-k (high temp):\", x_topk_euc_h)  # print(\"Hard argtop-k:\", x_argtopk_hard) # print(\"Entropic argtop-k (low temp):\", x_argtopk_l) # print(\"Entropic argtop-k (high temp):\", x_argtopk_h) # print(\"Euclidean argtop-k (low temp):\", x_argtopk_euc_l) # print(\"Euclidean argtop-k (high temp):\", x_argtopk_euc_h)  In\u00a0[5]: Copied! <pre># # Examples of sj.argmax\n# x = jnp.array([[5, 3, 4], [2, 7, 6]])\n\n# print(\"jnp:\", jnp.argmax(x, axis=1))\n# print(\"sj_hard:\", sj.argmax(x, mode=\"hard\", axis=1))\n# print(\"sj_entropic_low:\", sj.argmax(x, mode=\"entropic\", softness=0.01, axis=1))\n# print(\"sj_entropic_high:\", sj.argmax(x, mode=\"entropic\", softness=1.0, axis=1))\n# print(\"sj_euclidean_low:\", sj.argmax(x, mode=\"euclidean\", softness=0.01, axis=1))\n# print(\"sj_euclidean_high:\", sj.argmax(x, mode=\"euclidean\", softness=4.0, axis=1))\n</pre> # # Examples of sj.argmax # x = jnp.array([[5, 3, 4], [2, 7, 6]])  # print(\"jnp:\", jnp.argmax(x, axis=1)) # print(\"sj_hard:\", sj.argmax(x, mode=\"hard\", axis=1)) # print(\"sj_entropic_low:\", sj.argmax(x, mode=\"entropic\", softness=0.01, axis=1)) # print(\"sj_entropic_high:\", sj.argmax(x, mode=\"entropic\", softness=1.0, axis=1)) # print(\"sj_euclidean_low:\", sj.argmax(x, mode=\"euclidean\", softness=0.01, axis=1)) # print(\"sj_euclidean_high:\", sj.argmax(x, mode=\"euclidean\", softness=4.0, axis=1)) In\u00a0[6]: Copied! <pre># # Examples of sj.take_along_axis\n# x = jnp.array([[1, 2, 3], [4, 5, 6]])\n\n# indices = jnp.array([[0, 2], [1, 0]])\n# indices_onehot = jax.nn.one_hot(indices, x.shape[1])\n\n# print(jnp.take_along_axis(x, indices, axis=1))\n# print(sj.take_along_axis(x, indices_onehot, axis=1))\n\n# x = jnp.array([[5, 3, 4], [2, 7, 6]])\n\n# indices = jnp.argsort(x, axis=1)\n# indices_onehot = sj.argsort(x, axis=1, mode=\"hard\")\n# indices_soft = sj.argsort(x, axis=1, mode=\"entropic\", softness=1.0)\n\n# print(\"sorted_jnp:\", jnp.take_along_axis(x, indices, axis=1))\n# print(\"sorted_sj_hard:\", sj.take_along_axis(x, indices_onehot, axis=1))\n# print(\"sorted_sj_soft:\", sj.take_along_axis(x, indices_soft, axis=1))\n\n# indices = jnp.argmin(x, axis=1, keepdims=True)\n# indices_onehot = sj.argmin(x, axis=1, mode=\"hard\", keepdims=True)\n# indices_soft = sj.argmin(x, axis=1, mode=\"entropic\", softness=1.0, keepdims=True)\n\n# print(\"argmin_jnp:\", jnp.take_along_axis(x, indices, axis=1))\n# print(\"argmin_val_onehot:\", sj.take_along_axis(x, indices_onehot, axis=1))\n# print(\"argmin_val_soft:\", sj.take_along_axis(x, indices_soft, axis=1))\n</pre> # # Examples of sj.take_along_axis # x = jnp.array([[1, 2, 3], [4, 5, 6]])  # indices = jnp.array([[0, 2], [1, 0]]) # indices_onehot = jax.nn.one_hot(indices, x.shape[1])  # print(jnp.take_along_axis(x, indices, axis=1)) # print(sj.take_along_axis(x, indices_onehot, axis=1))  # x = jnp.array([[5, 3, 4], [2, 7, 6]])  # indices = jnp.argsort(x, axis=1) # indices_onehot = sj.argsort(x, axis=1, mode=\"hard\") # indices_soft = sj.argsort(x, axis=1, mode=\"entropic\", softness=1.0)  # print(\"sorted_jnp:\", jnp.take_along_axis(x, indices, axis=1)) # print(\"sorted_sj_hard:\", sj.take_along_axis(x, indices_onehot, axis=1)) # print(\"sorted_sj_soft:\", sj.take_along_axis(x, indices_soft, axis=1))  # indices = jnp.argmin(x, axis=1, keepdims=True) # indices_onehot = sj.argmin(x, axis=1, mode=\"hard\", keepdims=True) # indices_soft = sj.argmin(x, axis=1, mode=\"entropic\", softness=1.0, keepdims=True)  # print(\"argmin_jnp:\", jnp.take_along_axis(x, indices, axis=1)) # print(\"argmin_val_onehot:\", sj.take_along_axis(x, indices_onehot, axis=1)) # print(\"argmin_val_soft:\", sj.take_along_axis(x, indices_soft, axis=1)) In\u00a0[7]: Copied! <pre># # choose\n# choices = jnp.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n\n# indices = jnp.array([2, 0, 1, 0])\n# chosen = jnp.choose(indices, choices)\n# print(\"chosen:\", chosen)\n\n# indices_onehot = jax.nn.one_hot(indices, len(choices))\n# chosen_onehot = sj.choose(indices_onehot, choices)\n# print(\"chosen_onehot:\", chosen_onehot)\n\n# indices_soft = jax.nn.softmax(indices_onehot * 5, axis=-1)\n# chosen_soft = sj.choose(indices_soft, choices)\n# print(\"chosen_soft:\", chosen_soft)\n</pre> # # choose # choices = jnp.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])  # indices = jnp.array([2, 0, 1, 0]) # chosen = jnp.choose(indices, choices) # print(\"chosen:\", chosen)  # indices_onehot = jax.nn.one_hot(indices, len(choices)) # chosen_onehot = sj.choose(indices_onehot, choices) # print(\"chosen_onehot:\", chosen_onehot)  # indices_soft = jax.nn.softmax(indices_onehot * 5, axis=-1) # chosen_soft = sj.choose(indices_soft, choices) # print(\"chosen_soft:\", chosen_soft) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[8]: Copied! <pre>def plot(\n    fn,\n    modes,\n    title,\n    softnesses=[0.1, 0.3, 1.0],\n    xs=jnp.linspace(-2, 2, 1001),\n    # fn, modes, title, softnesss=[5.0, 2.0, 1.0, 0.5, 0.2], xs=jnp.linspace(-2, 2, 1001)\n):\n    xs = jnp.array(xs)\n\n    blue_red = LinearSegmentedColormap.from_list(\"blue_red\", [\"blue\", \"red\"])\n    colors = blue_red(jnp.array(softnesses) / max(softnesses))\n\n    # Create plotting canvas\n    fig, axes = plt.subplots(\n        2,\n        len(modes),\n        figsize=(7 * len(modes), 8),\n        sharex=True,\n        sharey=\"row\",\n        squeeze=False,\n    )\n\n    for col_idx, mode in enumerate(modes):\n        ax_f = axes[0][col_idx]\n        ax_g = axes[1][col_idx]\n\n        ys, grad_vals = jax.vmap(\n            lambda x: jax.value_and_grad(fn)(x, mode=\"hard\", softness=None)\n        )(xs)\n        ax_f.plot(xs, ys, label=\"hard\", linewidth=2, linestyle=\"--\", color=\"black\")\n        ax_g.plot(\n            xs,\n            grad_vals,\n            label=\"hard\",\n            linewidth=2,\n            linestyle=\"--\",\n            color=\"black\",\n        )\n\n        for softness, color in zip(softnesses, colors):\n            # if mode == \"entropic\":\n            #     _softness = softness * 0.1  # Adjust softness for entropic mode\n            # else:\n            #     _softness = softness\n            ys, grad_vals = jax.vmap(\n                lambda x: jax.value_and_grad(fn)(x, mode=mode, softness=softness)\n            )(xs)\n            ax_f.plot(xs, ys, label=f\"{softness}\", linewidth=2, color=color)\n            ax_g.plot(\n                xs,\n                np.array(grad_vals),\n                label=f\"{softness}\",\n                linewidth=2,\n                color=color,\n            )\n\n        ax_f.set_title(f\"vals [{mode}]\", fontsize=14)\n        ax_g.set_title(f\"grads [{mode}]\", fontsize=14)\n\n        for ax in (ax_f, ax_g):\n            ax.grid(True, linestyle=\"--\", alpha=0.6)\n            ax.axhline(0, color=\"black\", linewidth=0.5, alpha=0.7)\n            ax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.7)\n            ax.legend()\n\n    # Label axes\n    for ax in axes[-1]:\n        ax.set_xlabel(\"x\", fontsize=12)\n\n    fig.suptitle(title, fontsize=16)\n\n    # fig.suptitle(\"Function Outputs\", fontsize=16)\n    fig.tight_layout(rect=(0, 0, 1, 0.96))\n    plt.show()\n</pre> def plot(     fn,     modes,     title,     softnesses=[0.1, 0.3, 1.0],     xs=jnp.linspace(-2, 2, 1001),     # fn, modes, title, softnesss=[5.0, 2.0, 1.0, 0.5, 0.2], xs=jnp.linspace(-2, 2, 1001) ):     xs = jnp.array(xs)      blue_red = LinearSegmentedColormap.from_list(\"blue_red\", [\"blue\", \"red\"])     colors = blue_red(jnp.array(softnesses) / max(softnesses))      # Create plotting canvas     fig, axes = plt.subplots(         2,         len(modes),         figsize=(7 * len(modes), 8),         sharex=True,         sharey=\"row\",         squeeze=False,     )      for col_idx, mode in enumerate(modes):         ax_f = axes[0][col_idx]         ax_g = axes[1][col_idx]          ys, grad_vals = jax.vmap(             lambda x: jax.value_and_grad(fn)(x, mode=\"hard\", softness=None)         )(xs)         ax_f.plot(xs, ys, label=\"hard\", linewidth=2, linestyle=\"--\", color=\"black\")         ax_g.plot(             xs,             grad_vals,             label=\"hard\",             linewidth=2,             linestyle=\"--\",             color=\"black\",         )          for softness, color in zip(softnesses, colors):             # if mode == \"entropic\":             #     _softness = softness * 0.1  # Adjust softness for entropic mode             # else:             #     _softness = softness             ys, grad_vals = jax.vmap(                 lambda x: jax.value_and_grad(fn)(x, mode=mode, softness=softness)             )(xs)             ax_f.plot(xs, ys, label=f\"{softness}\", linewidth=2, color=color)             ax_g.plot(                 xs,                 np.array(grad_vals),                 label=f\"{softness}\",                 linewidth=2,                 color=color,             )          ax_f.set_title(f\"vals [{mode}]\", fontsize=14)         ax_g.set_title(f\"grads [{mode}]\", fontsize=14)          for ax in (ax_f, ax_g):             ax.grid(True, linestyle=\"--\", alpha=0.6)             ax.axhline(0, color=\"black\", linewidth=0.5, alpha=0.7)             ax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.7)             ax.legend()      # Label axes     for ax in axes[-1]:         ax.set_xlabel(\"x\", fontsize=12)      fig.suptitle(title, fontsize=16)      # fig.suptitle(\"Function Outputs\", fontsize=16)     fig.tight_layout(rect=(0, 0, 1, 0.96))     plt.show()  In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[9]: Copied! <pre>y0 = 0.0\ny1 = 0.5\ny2 = 1.0\n# zs = [-0.5, 0.5]\n# zs = [-0.5, 0.0, 0.5]\n# zs = [-1.5, -0.5, 0.5]\nzs = [-2.5, -1.5, -0.5, 0.5]\n# zs = [-3.0, -2.5, -1.5, -0.5, 0.5]\n# zs = [-1.0, -0.5, 0.0, 0.5]\n\n# y0 = 0.0\n# y1 = 1.0\n# y2 = 0.0\n# zs = [-1.0, 0.5, 0.5]\n\n# y0 = 0.0\n# y1 = 0.0\n# y2 = 0.0\n# zs = [0.0, 0.0, 0.0]\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_greater(x, mode, softness):\n    return sj.greater(x, jnp.array(y0), mode=mode, softness=softness)\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_greater_equal(x, mode, softness):\n    return sj.greater_equal(x, jnp.array(y0), mode=mode, softness=softness)\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_equal(x, mode, softness):\n    return sj.equal(x, jnp.array(y0), mode=mode, softness=softness)\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_not_equal(x, mode, softness):\n    return sj.not_equal(x, jnp.array(y0), mode=mode, softness=softness)\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_isclose(x, mode, softness):\n    return sj.isclose(x, jnp.array(y0), mode=mode, softness=softness)\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_where_greater(x, mode, softness):\n    return sj.where(\n        sj.greater(x, jnp.array(y0), mode=mode, softness=softness),\n        jnp.array(y1),\n        jnp.array(y2),\n    )\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_clip(x, mode, softness):\n    return sj.clip(x, y1, y2, mode=mode, softness=softness)\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_argmax(x, mode, softness):\n    soft_indices = sj.argmax(\n        x=jnp.stack([x, *zs], axis=-1),\n        axis=-1,\n        mode=mode,\n        softness=softness,\n    )\n    return soft_indices[0]\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_max(x, mode, softness):\n    return sj.max(\n        x=jnp.stack([x, *zs], axis=-1),\n        axis=-1,\n        mode=mode,\n        softness=softness,\n    )\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_argtop_k(x, mode, softness):\n    _, soft_indices = sj.top_k(\n        x=jnp.stack([x, *zs], axis=-1),\n        k=1,\n        axis=-1,\n        mode=mode,\n        softness=softness,\n        max_iter=max_iter,\n        fast=fast,\n    )  # shape: (1, 4)\n    return soft_indices[0, 0]\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_top_k(x, mode, softness):\n    values, _ = sj.top_k(\n        x=jnp.stack([x, *zs], axis=-1),\n        k=1,\n        axis=-1,\n        mode=mode,\n        softness=softness,\n        max_iter=max_iter,\n        fast=fast,\n    )  # shape: (1,)\n    return values[0]\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_argsort(x, mode, softness):\n    soft_indices = sj.argsort(\n        x=jnp.stack([x, *zs], axis=-1),\n        axis=-1,\n        mode=mode,\n        softness=softness,\n        max_iter=max_iter,\n        fast=fast,\n    )  # shape: (1, 4)\n    return soft_indices[1, 0]\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_sort(x, mode, softness):\n    values = sj.sort(\n        x=jnp.stack([x, *zs], axis=-1),\n        axis=-1,\n        mode=mode,\n        softness=softness,\n        max_iter=max_iter,\n        fast=fast,\n    )  # shape: (4,)\n    return values[1]\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_median(x, mode, softness):\n    return sj.median(\n        x=jnp.stack([x, *zs], axis=-1),\n        axis=-1,\n        mode=mode,\n        softness=softness,\n        max_iter=max_iter,\n    )\n\n\n# @partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\n# def my_median2(x, mode, softness):\n#     return sj.median2(\n#         x=jnp.stack([x, *zs], axis=-1),\n#         axis=-1,\n#         softness=softness,\n#         iters=max_iter,\n#     )\n\n\n@partial(jax.jit, static_argnames=(\"mode\", \"softness\"))\ndef my_ranking(x, mode, softness):\n    values = sj.ranking(\n        x=jnp.stack([x, *zs], axis=-1),\n        axis=-1,\n        mode=mode,\n        softness=softness,\n        max_iter=max_iter,\n        fast=fast,\n    )  # shape: (4,)\n    return values[0]\n</pre> y0 = 0.0 y1 = 0.5 y2 = 1.0 # zs = [-0.5, 0.5] # zs = [-0.5, 0.0, 0.5] # zs = [-1.5, -0.5, 0.5] zs = [-2.5, -1.5, -0.5, 0.5] # zs = [-3.0, -2.5, -1.5, -0.5, 0.5] # zs = [-1.0, -0.5, 0.0, 0.5]  # y0 = 0.0 # y1 = 1.0 # y2 = 0.0 # zs = [-1.0, 0.5, 0.5]  # y0 = 0.0 # y1 = 0.0 # y2 = 0.0 # zs = [0.0, 0.0, 0.0]   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_greater(x, mode, softness):     return sj.greater(x, jnp.array(y0), mode=mode, softness=softness)   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_greater_equal(x, mode, softness):     return sj.greater_equal(x, jnp.array(y0), mode=mode, softness=softness)   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_equal(x, mode, softness):     return sj.equal(x, jnp.array(y0), mode=mode, softness=softness)   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_not_equal(x, mode, softness):     return sj.not_equal(x, jnp.array(y0), mode=mode, softness=softness)   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_isclose(x, mode, softness):     return sj.isclose(x, jnp.array(y0), mode=mode, softness=softness)   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_where_greater(x, mode, softness):     return sj.where(         sj.greater(x, jnp.array(y0), mode=mode, softness=softness),         jnp.array(y1),         jnp.array(y2),     )   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_clip(x, mode, softness):     return sj.clip(x, y1, y2, mode=mode, softness=softness)   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_argmax(x, mode, softness):     soft_indices = sj.argmax(         x=jnp.stack([x, *zs], axis=-1),         axis=-1,         mode=mode,         softness=softness,     )     return soft_indices[0]   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_max(x, mode, softness):     return sj.max(         x=jnp.stack([x, *zs], axis=-1),         axis=-1,         mode=mode,         softness=softness,     )   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_argtop_k(x, mode, softness):     _, soft_indices = sj.top_k(         x=jnp.stack([x, *zs], axis=-1),         k=1,         axis=-1,         mode=mode,         softness=softness,         max_iter=max_iter,         fast=fast,     )  # shape: (1, 4)     return soft_indices[0, 0]   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_top_k(x, mode, softness):     values, _ = sj.top_k(         x=jnp.stack([x, *zs], axis=-1),         k=1,         axis=-1,         mode=mode,         softness=softness,         max_iter=max_iter,         fast=fast,     )  # shape: (1,)     return values[0]   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_argsort(x, mode, softness):     soft_indices = sj.argsort(         x=jnp.stack([x, *zs], axis=-1),         axis=-1,         mode=mode,         softness=softness,         max_iter=max_iter,         fast=fast,     )  # shape: (1, 4)     return soft_indices[1, 0]   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_sort(x, mode, softness):     values = sj.sort(         x=jnp.stack([x, *zs], axis=-1),         axis=-1,         mode=mode,         softness=softness,         max_iter=max_iter,         fast=fast,     )  # shape: (4,)     return values[1]   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_median(x, mode, softness):     return sj.median(         x=jnp.stack([x, *zs], axis=-1),         axis=-1,         mode=mode,         softness=softness,         max_iter=max_iter,     )   # @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) # def my_median2(x, mode, softness): #     return sj.median2( #         x=jnp.stack([x, *zs], axis=-1), #         axis=-1, #         softness=softness, #         iters=max_iter, #     )   @partial(jax.jit, static_argnames=(\"mode\", \"softness\")) def my_ranking(x, mode, softness):     values = sj.ranking(         x=jnp.stack([x, *zs], axis=-1),         axis=-1,         mode=mode,         softness=softness,         max_iter=max_iter,         fast=fast,     )  # shape: (4,)     return values[0] In\u00a0[10]: Copied! <pre># heaviside_modes = [\"sigmoid\", \"tanh\", \"pseudohuber\", \"linear\", \"cubic\", \"quintic\"]\n# heaviside_modes = [\"sigmoid\"]\nrelu_modes = [\n    \"softplus\",\n    \"silu\",\n    \"quadratic\",\n    \"quartic\",\n    # \"entropic\",\n    # \"euclidean\",\n]\nargmax_modes = [\"entropic\", \"euclidean\"]\ntopk_modes = [\"entropic\", \"euclidean\"]\n\n\n# plot(sj.heaviside, title=\"heaviside(x)\", modes=heaviside_modes)\n# plot(sj.sign, title=\"sign(x)\", modes=heaviside_modes)\n# plot(my_greater, title=f\"greater(x, {y0})\", modes=heaviside_modes)\n# plot(my_greater_equal, title=f\"greater_equal(x, {y0})\", modes=heaviside_modes)\n# plot(my_equal, title=f\"equal(x, {y0})\", modes=heaviside_modes)\n# plot(my_not_equal, title=f\"not_equal(x, {y0})\", modes=heaviside_modes)\n# plot(my_isclose, title=f\"isclose(x, {y0})\", modes=heaviside_modes)\n# plot(\n#     my_where_greater,\n#     title=f\"where(greater(x, {y0}), {y1}, {y2})\",\n#     modes=heaviside_modes,\n# )\n\n# plot(sj.relu, title=\"relu(x)\", modes=relu_modes)\n# plot(my_clip, title=f\"clip(x, {y1}, {y2})\", modes=relu_modes)\n\n# plot(sj.abs, title=\"abs(x)\", modes=heaviside_modes)\n# plot(my_argmax, title=f\"argmax(x, {zs})[0]\", modes=argmax_modes)\n# plot(my_max, title=f\"max(x, {zs})\", modes=argmax_modes)\n\n# plot(my_argtop_k, title=f\"argtop_k(x, {zs}, k=1)[0,0]\", modes=topk_modes)\n# plot(my_top_k, title=f\"top_k(x, {zs}, k=1)[0]\", modes=topk_modes)\n# plot(my_argsort, title=f\"argsort(x, {zs})[1,0]\", modes=topk_modes)\n# plot(my_sort, title=f\"sort(x, {zs})[1]\", modes=topk_modes)\n# plot(my_median, title=f\"median(x, {zs})\", modes=heaviside_modes)\n# plot(my_median2, title=f\"median2(x, {zs})\", modes=topk_modes)\n# plot(my_ranking, title=f\"ranking(x, {zs})[0]\", modes=topk_modes)\n\nfrom jax.scipy.special import gammaln\n\n\ndef normalized_bump(x, n: int):\n    # A_n = 2^n (n!)^2 / (2n)!\n    logA = n * jnp.log(2.0) + 2.0 * gammaln(n + 1.0) - gammaln(2.0 * n + 1.0)\n    A = jnp.exp(logA)\n    return A * (1.0 - jnp.cos(2.0 * jnp.pi * x)) ** n\n\n\nfrom jax.scipy.special import betainc\n\n\ndef bump_cdf(x, n: int):\n    a = n + 0.5\n    b = 0.5\n\n    x = jnp.asarray(x)\n    z1 = jnp.sin(jnp.pi * x) ** 2\n    z2 = jnp.sin(jnp.pi * (1.0 - x)) ** 2\n\n    F_left = 0.5 * betainc(a, b, z1)\n    F_right = 1.0 - 0.5 * betainc(a, b, z2)\n\n    return jnp.where(x &lt;= 0.5, F_left, F_right)\n\n\ndef my_round(x, mode, softness):\n    if mode == \"hard\":\n        return jnp.round(x)\n    elif mode == \"soft\":\n        soft_round_1 = x - softness * jnp.sin(2.0 * jnp.pi * x) / (2.0 * jnp.pi)\n\n        k = jnp.floor(x)\n        u = x - k  # fractional part in [0, 1)\n        a = softness + 0.5\n        b = 0.5\n        z = jnp.sin(jnp.pi * u) ** 2  # in [0, 1]\n\n        half_cdf = 0.5 * betainc(a, b, z)\n        F = jnp.where(u &lt;= 0.5, half_cdf, 1.0 - half_cdf)\n        soft_round_2 = k + F\n\n        soft_round = jnp.where(softness &lt; 1.0, soft_round_1, soft_round_2)\n        return soft_round\n\n        # return normalized_bump(x, n=2)\n        # return jnp.floor(x) + bump_cdf(x - jnp.floor(x), n=1.0)\n\n        # return x - jnp.sin(2.0 * jnp.pi * x) / (2.0 * jnp.pi)\n        # return 1 - jnp.cos(x * 2.0 * jnp.pi)\n        # return 1 + jnp.sin(x * 2.0 * jnp.pi - jnp.pi / 2)\n\n\nplot(\n    my_round,\n    title=\"round(x)\",\n    modes=[\"hard\", \"soft\"],\n    softnesses=[0.1, 0.5, 1.0, 2.0, 4.0, 10.0],\n)\n</pre> # heaviside_modes = [\"sigmoid\", \"tanh\", \"pseudohuber\", \"linear\", \"cubic\", \"quintic\"] # heaviside_modes = [\"sigmoid\"] relu_modes = [     \"softplus\",     \"silu\",     \"quadratic\",     \"quartic\",     # \"entropic\",     # \"euclidean\", ] argmax_modes = [\"entropic\", \"euclidean\"] topk_modes = [\"entropic\", \"euclidean\"]   # plot(sj.heaviside, title=\"heaviside(x)\", modes=heaviside_modes) # plot(sj.sign, title=\"sign(x)\", modes=heaviside_modes) # plot(my_greater, title=f\"greater(x, {y0})\", modes=heaviside_modes) # plot(my_greater_equal, title=f\"greater_equal(x, {y0})\", modes=heaviside_modes) # plot(my_equal, title=f\"equal(x, {y0})\", modes=heaviside_modes) # plot(my_not_equal, title=f\"not_equal(x, {y0})\", modes=heaviside_modes) # plot(my_isclose, title=f\"isclose(x, {y0})\", modes=heaviside_modes) # plot( #     my_where_greater, #     title=f\"where(greater(x, {y0}), {y1}, {y2})\", #     modes=heaviside_modes, # )  # plot(sj.relu, title=\"relu(x)\", modes=relu_modes) # plot(my_clip, title=f\"clip(x, {y1}, {y2})\", modes=relu_modes)  # plot(sj.abs, title=\"abs(x)\", modes=heaviside_modes) # plot(my_argmax, title=f\"argmax(x, {zs})[0]\", modes=argmax_modes) # plot(my_max, title=f\"max(x, {zs})\", modes=argmax_modes)  # plot(my_argtop_k, title=f\"argtop_k(x, {zs}, k=1)[0,0]\", modes=topk_modes) # plot(my_top_k, title=f\"top_k(x, {zs}, k=1)[0]\", modes=topk_modes) # plot(my_argsort, title=f\"argsort(x, {zs})[1,0]\", modes=topk_modes) # plot(my_sort, title=f\"sort(x, {zs})[1]\", modes=topk_modes) # plot(my_median, title=f\"median(x, {zs})\", modes=heaviside_modes) # plot(my_median2, title=f\"median2(x, {zs})\", modes=topk_modes) # plot(my_ranking, title=f\"ranking(x, {zs})[0]\", modes=topk_modes)  from jax.scipy.special import gammaln   def normalized_bump(x, n: int):     # A_n = 2^n (n!)^2 / (2n)!     logA = n * jnp.log(2.0) + 2.0 * gammaln(n + 1.0) - gammaln(2.0 * n + 1.0)     A = jnp.exp(logA)     return A * (1.0 - jnp.cos(2.0 * jnp.pi * x)) ** n   from jax.scipy.special import betainc   def bump_cdf(x, n: int):     a = n + 0.5     b = 0.5      x = jnp.asarray(x)     z1 = jnp.sin(jnp.pi * x) ** 2     z2 = jnp.sin(jnp.pi * (1.0 - x)) ** 2      F_left = 0.5 * betainc(a, b, z1)     F_right = 1.0 - 0.5 * betainc(a, b, z2)      return jnp.where(x &lt;= 0.5, F_left, F_right)   def my_round(x, mode, softness):     if mode == \"hard\":         return jnp.round(x)     elif mode == \"soft\":         soft_round_1 = x - softness * jnp.sin(2.0 * jnp.pi * x) / (2.0 * jnp.pi)          k = jnp.floor(x)         u = x - k  # fractional part in [0, 1)         a = softness + 0.5         b = 0.5         z = jnp.sin(jnp.pi * u) ** 2  # in [0, 1]          half_cdf = 0.5 * betainc(a, b, z)         F = jnp.where(u &lt;= 0.5, half_cdf, 1.0 - half_cdf)         soft_round_2 = k + F          soft_round = jnp.where(softness &lt; 1.0, soft_round_1, soft_round_2)         return soft_round          # return normalized_bump(x, n=2)         # return jnp.floor(x) + bump_cdf(x - jnp.floor(x), n=1.0)          # return x - jnp.sin(2.0 * jnp.pi * x) / (2.0 * jnp.pi)         # return 1 - jnp.cos(x * 2.0 * jnp.pi)         # return 1 + jnp.sin(x * 2.0 * jnp.pi - jnp.pi / 2)   plot(     my_round,     title=\"round(x)\",     modes=[\"hard\", \"soft\"],     softnesses=[0.1, 0.5, 1.0, 2.0, 4.0, 10.0], ) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[11]: Copied! <pre># # Define x values\n# # x_vals = np.linspace(-10, 10, 501)\n# x_vals = np.linspace(-2, 2, 501)\n# # x_vals = np.linspace(-0.1, 0.1, 501)\n# x_jnp = jnp.array(x_vals)\n\n# # Plot settings\n# # modes = [\"polynomial_0\", \"polynomial_1\", \"polynomial_2\", \"entropic\"]\n# modes = [\"polynomial_0\", \"polynomial_1\", \"entropic\"]\n# # modes = [\"entropic\"]\n# # softnesses = [10, 5.0, 2.0, 1.0, 0.5, 0.1]\n# softnesses = [2.0, 1.0, 0.5, 0.1]\n# # modes = [\"entropic\"]\n# # softnesses = [100, 10, 1]\n# blue_red = LinearSegmentedColormap.from_list(\"blue_red\", [\"blue\", \"red\"])\n# colors = blue_red(jnp.array(softnesses) / max(softnesses))\n\n\n# # Functions to plot\n# functions = {\n#     # \"argsort(-0.5, x, 0.5)[1, 1]\": my_argsort,\n#     # \"sort(-0.5, x, 0.5)[0]\": my_sort_0,\n#     # \"sort(-0.5, x, 0.5)[1]\": my_sort_1,\n#     # \"sort(-0.5, x, 0.5)[2]\": my_sort_2,\n#     # \"argtop_k(0, x, 0.5, k=1)[1]\": my_argtop_k,\n#     # \"top_k(0, x, 0.5, k=1)\": my_top_k,\n#     # \"argmax(0, x)[1]\": my_argmax,\n#     # \"max(0, x)\": my_max,\n#     # \"max(0, x, 0.5)\": my_max_2,\n#     # \"argmin(0, x)[1]\": my_argmin,\n#     # \"min(0, x)\": my_min,\n#     \"relu(x)\": sj.relu,\n#     \"clip(x, 0, 1)\": my_clip,\n#     \"heaviside(x)\": sj.heaviside,\n#     \"greater(x, 0)\": my_greater,\n#     \"greater_equal(x, 0)\": my_greater_equal,\n#     \"less(x, 0)\": my_less,\n#     \"less_equal(x, 0)\": my_less_equal,\n#     \"equal(x, 0)\": my_equal,\n#     \"not_equal(x, 0)\": my_not_equal,\n#     \"isclose(x, 0)\": my_isclose,\n#     \"where(greater(x, 0), 2, -1)\": my_where_greater,\n# }\n\n\n# # Create plotting canvas\n# def make_figure(title_suffix):\n#     return plt.subplots(\n#         len(functions),\n#         len(modes),\n#         figsize=(7 * len(modes), 4 * len(functions)),\n#         sharex=True,\n#         sharey=\"row\",\n#         squeeze=False,\n#     )\n\n\n# # Plot function outputs\n# fig_f, axes_f = make_figure(\"Function\")\n\n# # Plot gradients\n# fig_g, axes_g = make_figure(\"Gradient\")\n\n# for row_idx, (func_name, func) in enumerate(functions.items()):\n#     for col_idx, mode in enumerate(modes):\n#         ax_f = axes_f[row_idx][col_idx]\n#         ax_g = axes_g[row_idx][col_idx]\n\n#         y_vals, grad_vals = jax.vmap(\n#             lambda x: jax.value_and_grad(func)(x, mode=\"hard\", softness=0.0)\n#         )(x_jnp)\n#         ax_f.plot(\n#             x_vals, y_vals, label=\"hard\", linewidth=2, linestyle=\"--\", color=\"black\"\n#         )\n#         ax_g.plot(\n#             x_vals, grad_vals, label=\"hard\", linewidth=2, linestyle=\"--\", color=\"black\"\n#         )\n\n#         for softness, color in zip(softnesses, colors):\n#             if mode == \"entropic\":\n#                    _softness = softness * 0.1  # Adjust softness for entropic mode\n#             else:\n#                 _softness = softness\n#             y_vals, grad_vals = jax.vmap(\n#                 lambda x: jax.value_and_grad(func)(x, mode=mode, softness=_softness)\n#             )(x_jnp)\n#             ax_f.plot(x_vals, y_vals, label=f\"{softness}\", linewidth=2, color=color)\n#             ax_g.plot(\n#                 x_vals,\n#                 np.array(grad_vals),\n#                 label=f\"{softness}\",\n#                 linewidth=2,\n#                 color=color,\n#             )\n\n#         ax_f.set_title(f\"{func_name} [{mode}]\", fontsize=14)\n#         ax_g.set_title(f\"\u2207{func_name} [{mode}]\", fontsize=14)\n\n#         for ax in (ax_f, ax_g):\n#             ax.grid(True, linestyle=\"--\", alpha=0.6)\n#             ax.axhline(0, color=\"black\", linewidth=0.5, alpha=0.7)\n#             ax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.7)\n#             ax.legend()\n\n# # Label axes\n# for ax in axes_f[-1]:\n#     ax.set_xlabel(\"x\", fontsize=12)\n# for ax in axes_g[-1]:\n#     ax.set_xlabel(\"x\", fontsize=12)\n\n# for ax, name in zip(axes_f[:, 0], functions.keys()):\n#     ax.set_ylabel(f\"{name}\", fontsize=12)\n# for ax, name in zip(axes_g[:, 0], functions.keys()):\n#     ax.set_ylabel(f\"d{name}/dx\", fontsize=12)\n\n# fig_f.suptitle(\"Function Outputs\", fontsize=16)\n# fig_g.suptitle(\"Function Gradients\", fontsize=16)\n# fig_f.tight_layout(rect=(0, 0, 1, 0.96))\n# fig_g.tight_layout(rect=(0, 0, 1, 0.96))\n# plt.show()\n</pre> # # Define x values # # x_vals = np.linspace(-10, 10, 501) # x_vals = np.linspace(-2, 2, 501) # # x_vals = np.linspace(-0.1, 0.1, 501) # x_jnp = jnp.array(x_vals)  # # Plot settings # # modes = [\"polynomial_0\", \"polynomial_1\", \"polynomial_2\", \"entropic\"] # modes = [\"polynomial_0\", \"polynomial_1\", \"entropic\"] # # modes = [\"entropic\"] # # softnesses = [10, 5.0, 2.0, 1.0, 0.5, 0.1] # softnesses = [2.0, 1.0, 0.5, 0.1] # # modes = [\"entropic\"] # # softnesses = [100, 10, 1] # blue_red = LinearSegmentedColormap.from_list(\"blue_red\", [\"blue\", \"red\"]) # colors = blue_red(jnp.array(softnesses) / max(softnesses))   # # Functions to plot # functions = { #     # \"argsort(-0.5, x, 0.5)[1, 1]\": my_argsort, #     # \"sort(-0.5, x, 0.5)[0]\": my_sort_0, #     # \"sort(-0.5, x, 0.5)[1]\": my_sort_1, #     # \"sort(-0.5, x, 0.5)[2]\": my_sort_2, #     # \"argtop_k(0, x, 0.5, k=1)[1]\": my_argtop_k, #     # \"top_k(0, x, 0.5, k=1)\": my_top_k, #     # \"argmax(0, x)[1]\": my_argmax, #     # \"max(0, x)\": my_max, #     # \"max(0, x, 0.5)\": my_max_2, #     # \"argmin(0, x)[1]\": my_argmin, #     # \"min(0, x)\": my_min, #     \"relu(x)\": sj.relu, #     \"clip(x, 0, 1)\": my_clip, #     \"heaviside(x)\": sj.heaviside, #     \"greater(x, 0)\": my_greater, #     \"greater_equal(x, 0)\": my_greater_equal, #     \"less(x, 0)\": my_less, #     \"less_equal(x, 0)\": my_less_equal, #     \"equal(x, 0)\": my_equal, #     \"not_equal(x, 0)\": my_not_equal, #     \"isclose(x, 0)\": my_isclose, #     \"where(greater(x, 0), 2, -1)\": my_where_greater, # }   # # Create plotting canvas # def make_figure(title_suffix): #     return plt.subplots( #         len(functions), #         len(modes), #         figsize=(7 * len(modes), 4 * len(functions)), #         sharex=True, #         sharey=\"row\", #         squeeze=False, #     )   # # Plot function outputs # fig_f, axes_f = make_figure(\"Function\")  # # Plot gradients # fig_g, axes_g = make_figure(\"Gradient\")  # for row_idx, (func_name, func) in enumerate(functions.items()): #     for col_idx, mode in enumerate(modes): #         ax_f = axes_f[row_idx][col_idx] #         ax_g = axes_g[row_idx][col_idx]  #         y_vals, grad_vals = jax.vmap( #             lambda x: jax.value_and_grad(func)(x, mode=\"hard\", softness=0.0) #         )(x_jnp) #         ax_f.plot( #             x_vals, y_vals, label=\"hard\", linewidth=2, linestyle=\"--\", color=\"black\" #         ) #         ax_g.plot( #             x_vals, grad_vals, label=\"hard\", linewidth=2, linestyle=\"--\", color=\"black\" #         )  #         for softness, color in zip(softnesses, colors): #             if mode == \"entropic\": #                    _softness = softness * 0.1  # Adjust softness for entropic mode #             else: #                 _softness = softness #             y_vals, grad_vals = jax.vmap( #                 lambda x: jax.value_and_grad(func)(x, mode=mode, softness=_softness) #             )(x_jnp) #             ax_f.plot(x_vals, y_vals, label=f\"{softness}\", linewidth=2, color=color) #             ax_g.plot( #                 x_vals, #                 np.array(grad_vals), #                 label=f\"{softness}\", #                 linewidth=2, #                 color=color, #             )  #         ax_f.set_title(f\"{func_name} [{mode}]\", fontsize=14) #         ax_g.set_title(f\"\u2207{func_name} [{mode}]\", fontsize=14)  #         for ax in (ax_f, ax_g): #             ax.grid(True, linestyle=\"--\", alpha=0.6) #             ax.axhline(0, color=\"black\", linewidth=0.5, alpha=0.7) #             ax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.7) #             ax.legend()  # # Label axes # for ax in axes_f[-1]: #     ax.set_xlabel(\"x\", fontsize=12) # for ax in axes_g[-1]: #     ax.set_xlabel(\"x\", fontsize=12)  # for ax, name in zip(axes_f[:, 0], functions.keys()): #     ax.set_ylabel(f\"{name}\", fontsize=12) # for ax, name in zip(axes_g[:, 0], functions.keys()): #     ax.set_ylabel(f\"d{name}/dx\", fontsize=12)  # fig_f.suptitle(\"Function Outputs\", fontsize=16) # fig_g.suptitle(\"Function Gradients\", fontsize=16) # fig_f.tight_layout(rect=(0, 0, 1, 0.96)) # fig_g.tight_layout(rect=(0, 0, 1, 0.96)) # plt.show()"}]}