{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>Softjax provides soft differentiable drop-in replacements for traditionally non-differentiable functions in JAX, including</p> <ul> <li>simple elementwise functions: <code>abs</code>, <code>relu</code>, <code>clip</code>, <code>sign</code> and <code>round</code>;</li> <li>functions operating on arrays: <code>max</code>, <code>min</code>, <code>median</code>, <code>sort</code>, <code>ranking</code> and <code>top_k</code>;</li> <li>functions returning indices: <code>argmax</code>, <code>argmin</code>, <code>argmedian</code>, <code>argsort</code> and <code>argtop_k</code>;</li> <li>functions returning boolean values such as: <code>greater</code>, <code>equal</code> or <code>isclose</code>;</li> <li>functions for selection with indices such as: <code>take_along_axis</code>, <code>dynamic_index_in_dim</code> and <code>choose</code>;</li> <li>functions for logical manipulation such as: <code>logical_and</code>, <code>all</code> and <code>where</code>.</li> </ul> <p>Many functions offer multiple modes for softening, allowing for e.g. smoothness of the soft function or boundedness of the softened region, depending on the user needs. Moreover, we tightly integrate functionality for deploying functions using straight-through-estimation, where we use non-differentiable functions in the forward pass and their differentiable replacements in the backward pass.</p> <p>The Softjax library is designed to require minimal user effort, by simply replacing the non-differentiable JAX function with the Softjax counterparts. However, keep in mind that special care needs to be taken when using functions operating on indices, as we relax the notion of an index into a distribution over indices, thereby modifying the shape of returned/accepted values.</p>"},{"location":"#installation","title":"Installation","text":"<p>Requires Python 3.10+. <pre><code>pip install softjax\n</code></pre></p>"},{"location":"#quick-example","title":"Quick example","text":"<p><pre><code>import jax.numpy as jnp\nimport softjax as sj\n\nx = jnp.array([-0.2, -1.0, 0.3, 1.0])\ny = jnp.array([0.2, -0.5, 0.5, -1.0])\n\n# Elementwise functions\nprint(\"Hard ReLU:\", jax.nn.relu(x))\nprint(\"Soft ReLU:\", sj.relu(x))\nprint(\"Hard Clip:\", jnp.clip(x, -0.5, 0.5))\nprint(\"Soft Clip:\", sj.clip(x, -0.5, 0.5))\nprint(\"Hard Absolute:\", jnp.abs(x))\nprint(\"Soft Absolute:\", sj.abs(x))\nprint(\"Hard Sign:\", jnp.sign(x))\nprint(\"Soft Sign:\", sj.sign(x))\nprint(\"Hard round:\", jnp.round(x))\nprint(\"Soft round:\", sj.round(x))\nprint(\"Hard heaviside:\", jnp.heaviside(x, 0.5))\nprint(\"Soft heaviside:\", sj.heaviside(x))\n</code></pre> <pre><code>Hard ReLU: [0.  0.  0.3 1. ]\nSoft ReLU: [1.26928011e-02 4.53988992e-06 3.04858735e-01 1.00000454e+00]\nHard Clip: [-0.2 -0.5  0.3  0.5]\nSoft Clip: [-0.19523241 -0.4993285   0.28734074  0.4993285 ]\nHard Absolute: [0.2 1.  0.3 1. ]\nSoft Absolute: [0.15231883 0.9999092  0.27154448 0.9999092 ]\nHard Sign: [-1. -1.  1.  1.]\nSoft Sign: [-0.76159416 -0.9999092   0.90514825  0.9999092 ]\nHard round: [-0. -1.  0.  1.]\nSoft round: [-0.04651704 -1.          0.1188737   1.        ]\nHard heaviside: [0. 0. 1. 1.]\nSoft heaviside: [1.19202922e-01 4.53978687e-05 9.52574127e-01 9.99954602e-01]\n</code></pre></p> <p><pre><code># Functions on arrays\nprint(\"Hard max:\", jnp.max(x))\nprint(\"Soft max:\", sj.max(x))\nprint(\"Hard min:\", jnp.min(x))\nprint(\"Soft min:\", sj.min(x))\nprint(\"Hard median:\", jnp.median(x))\nprint(\"Soft median:\", sj.median(x))\nprint(\"Hard top_k:\", jax.lax.top_k(x, k=3)[0])\nprint(\"Soft top_k:\", sj.top_k(x, k=3)[0])\nprint(\"Hard sort:\", jnp.sort(x))\nprint(\"Soft sort:\", sj.sort(x))\nprint(\"Hard ranking:\", jnp.argsort(jnp.argsort(x)))\nprint(\"Soft ranking:\", sj.ranking(x, descending=False))\n</code></pre> <pre><code>Hard max: 1.0\nSoft max: 0.9993548976691374\nHard min: -1.0\nSoft min: -0.9997287789452775\nHard median: 0.04999999999999999\nSoft median: 0.05000033589501627\nHard top_k: [ 1.   0.3 -0.2]\nSoft top_k: [ 0.9993549   0.29728716 -0.19691387]\nHard sort: [-1.  -0.2  0.3  1. ]\nSoft sort: [-0.99972878 -0.19691387  0.29728716  0.9993549 ]\nHard ranking: [1 0 2 3]\nSoft ranking: [1.00636968e+00 3.39874686e-04 1.99421369e+00 2.99907667e+00]\n</code></pre></p> <p><pre><code># Functions returning indices\nprint(\"Hard argmax:\", jnp.argmax(x))\nprint(\"Soft argmax:\", sj.argmax(x))\nprint(\"Hard argmin:\", jnp.argmin(x))\nprint(\"Soft argmin:\", sj.argmin(x))\nprint(\"Hard argmedian:\", \"Not implemented in standard JAX\")\nprint(\"Soft argmedian:\", sj.argmedian(x))\nprint(\"Hard argtop_k:\", jax.lax.top_k(x, k=3)[1])\nprint(\"Soft argtop_k:\", sj.top_k(x, k=3)[1])\nprint(\"Hard argsort:\", jnp.argsort(x))\nprint(\"Soft argsort:\", sj.argsort(x))\n</code></pre> <pre><code>Hard argmax: 3\nSoft argmax: [6.13857697e-06 2.05926316e-09 9.11045600e-04 9.99082814e-01]\nHard argmin: 1\nSoft argmin: [3.35349372e-04 9.99662389e-01 2.25956629e-06 2.06045775e-09]\nHard argmedian: Not implemented in standard JAX\nSoft argmedian: [4.99999764e-01 5.62675608e-08 4.99999764e-01 4.15764163e-07]\nHard argtop_k: [3 2 0]\nSoft argtop_k: [[6.13857697e-06 2.05926316e-09 9.11045600e-04 9.99082814e-01]\n [6.68677917e-03 2.24316451e-06 9.92406021e-01 9.04957153e-04]\n [9.92970214e-01 3.33104397e-04 6.69058067e-03 6.10101985e-06]]\nHard argsort: [1 0 2 3]\nSoft argsort: [[3.35349372e-04 9.99662389e-01 2.25956629e-06 2.06045775e-09]\n [9.92970214e-01 3.33104397e-04 6.69058067e-03 6.10101985e-06]\n [6.68677917e-03 2.24316451e-06 9.92406021e-01 9.04957153e-04]\n [6.13857697e-06 2.05926316e-09 9.11045600e-04 9.99082814e-01]]\n</code></pre></p> <p><pre><code>## SoftBool generation\nprint(\"Hard greater:\", x &gt; y)\nprint(\"Soft greater:\", sj.greater(x, y))\nprint(\"Hard greater equal:\", x &gt;= y)\nprint(\"Soft greater equal:\", sj.greater_equal(x, y))\nprint(\"Hard less:\", x &lt; y)\nprint(\"Soft less:\", sj.less(x, y))\nprint(\"Hard less equal:\", x &lt;= y)\nprint(\"Soft less equal:\", sj.less_equal(x, y))\nprint(\"Hard equal:\", x == y)\nprint(\"Soft equal:\", sj.equal(x, y))\nprint(\"Hard not equal:\", x != y)\nprint(\"Soft not equal:\", sj.not_equal(x, y))\nprint(\"Hard isclose:\", jnp.isclose(x, y))\nprint(\"Soft isclose:\", sj.isclose(x, y))\n</code></pre> <pre><code>Hard greater: [False False False  True]\nSoft greater: [0.01798621 0.00669285 0.11920292 1.        ]\nHard greater equal: [False False False  True]\nSoft greater equal: [0.01798621 0.00669285 0.11920292 1.        ]\nHard less: [ True  True  True False]\nSoft less: [9.82013790e-01 9.93307149e-01 8.80797078e-01 2.06115369e-09]\nHard less equal: [ True  True  True False]\nSoft less equal: [9.82013790e-01 9.93307149e-01 8.80797078e-01 2.06115369e-09]\nHard equal: [False False False False]\nSoft equal: [1.79862100e-02 6.69285093e-03 1.19202922e-01 2.06115369e-09]\nHard not equal: [ True  True  True  True]\nSoft not equal: [0.98201379 0.99330715 0.88079708 1.        ]\nHard isclose: [False False False False]\nSoft isclose: [1.79865650e-02 6.69318401e-03 1.19208182e-01 2.06135997e-09]\n</code></pre></p> <p><pre><code>## SoftBool manipulation\nfuzzy_a = jnp.array([0.1, 0.2, 0.8, 1.0])\nfuzzy_b = jnp.array([0.7, 0.3, 0.1, 0.9])\nprint(\"Soft AND:\", sj.logical_and(fuzzy_a, fuzzy_b))\nprint(\"Soft OR:\", sj.logical_or(fuzzy_a, fuzzy_b))\nprint(\"Soft NOT:\", sj.logical_not(fuzzy_a))\nprint(\"Soft XOR:\", sj.logical_xor(fuzzy_a, fuzzy_b))\nprint(\"Soft ALL:\", sj.all(fuzzy_a))\nprint(\"Soft ANY:\", sj.any(fuzzy_a))\n\n## SoftBool selection\nprint(\"Where:\", sj.where(fuzzy_a, x, y))\n</code></pre> <pre><code>Soft AND: [0.26457513 0.24494897 0.28284271 0.9486833 ]\nSoft OR: [0.48038476 0.25166852 0.57573593 0.99999684]\nSoft NOT: [0.9 0.8 0.2 0. ]\nSoft XOR: [0.58702688 0.43498731 0.63937484 0.17309871]\nSoft ALL: 0.35565588200778464\nSoft ANY: 0.9980519925071494\nWhere: [ 0.16 -0.6   0.34  1.  ]\n</code></pre></p> <p><pre><code># Straight-through estimation: Use hard function on forward and soft on backward\nprint(\"Straight-through ReLU:\", sj.relu_st(x))\nprint(\"Straight-through sort:\", sj.sort_st(x))\nprint(\"Straight-through argtop_k:\", sj.top_k_st(x, k=3)[1])\nprint(\"Straight-through greater:\", sj.greater_st(x, y))\n</code></pre> <pre><code>Straight-through ReLU: [0.  0.  0.3 1. ]\nStraight-through sort: [-1.  -0.2  0.3  1. ]\nStraight-through argtop_k: [[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]]\nStraight-through greater: [0. 0. 0. 1.]\n</code></pre></p> <p>The outputs were generated in this notebook.</p>"},{"location":"#citation","title":"Citation","text":"<p>If this library helped your academic work, please consider citing:</p> <pre><code>@misc{Softjax2025,\n  author = {Paulus, Anselm and Geist, Ren\\'e and Martius, Georg},\n  title = {Softjax},\n  year = {2025},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/a-paulus/softjax}}\n}\n</code></pre> <p>Also consider starring the project on GitHub!</p> <p>Special thanks and credit go to Patrick Kidger for the awesome JAX repositories that served as the basis for the documentation of this project.</p>"},{"location":"#next-steps","title":"Next steps","text":"<p>Have a look at the All of Softjax page.</p>"},{"location":"#feedback","title":"Feedback","text":"<p>This project is still relatively young, if you have any suggestions for improvement or other feedback, please reach out or raise a GitHub issue!</p>"},{"location":"#see-also","title":"See also","text":""},{"location":"#other-libraries-in-the-jax-ecosystem","title":"Other libraries in the JAX ecosystem","text":"<p>Always useful Equinox: neural networks and everything not already in core JAX! jaxtyping: type annotations for shape/dtype of arrays.  </p> <p>Deep learning Optax: first-order gradient (SGD, Adam, ...) optimisers. Orbax: checkpointing (async/multi-host/multi-device). Levanter: scalable+reliable training of foundation models (e.g. LLMs). paramax: parameterizations and constraints for PyTrees.  </p> <p>Scientific computing Optimistix: root finding, minimisation, fixed points, and least squares. Lineax: linear solvers. BlackJAX: probabilistic+Bayesian sampling. sympy2jax: SymPy&lt;-&gt;JAX conversion; train symbolic expressions via gradient descent. PySR: symbolic regression. (Non-JAX honourable mention!)  </p> <p>Awesome JAX Awesome JAX: a longer list of other JAX projects.  </p>"},{"location":"#other-libraries-on-differentiable-programming","title":"Other libraries on differentiable programming","text":"<p>Differentiable sorting, top-k and ranking DiffSort: Differentiable sorting networks in PyTorch. DiffTopK: Differentiable top-k in PyTorch. FastSoftSort: Fast differentiable sorting and ranking in JAX. Differentiable Top-k with Optimal Transport in JAX. SoftSort: Differentiable argsort in PyTorch and TensorFlow.  </p> <p>Other DiffLogic: Differentiable logic gate networks in PyTorch. SmoothOT: Smooth and Sparse Optimal Transport. JaxOpt: Differentiable optimization in JAX.  </p>"},{"location":"#papers-on-differentiable-algorithms","title":"Papers on differentiable algorithms","text":"<p>Softjax builds on / implements various different algoithms for e.g. differentiable <code>argtop_k</code>, <code>sorting</code> and <code>ranking</code>, including:</p> <p>Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application Fast Differentiable Sorting and Ranking. Differentiable Ranks and Sorting using Optimal Transport Differentiable Top-k with Optimal Transport SoftSort: A Continuous Relaxation for the argsort Operator Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances Smooth and Sparse Optimal Transport </p> <p>Please check the API Documentation for implementation details.</p>"},{"location":"all-of-softjax/","title":"All of Softjax","text":"In\u00a0[3]: Copied! <pre>import softjax as sj\n\nplot(sj.relu, modes=[\"entropic\", \"gated_quintic\"])\n</pre> import softjax as sj  plot(sj.relu, modes=[\"entropic\", \"gated_quintic\"]) <p>SoftJAX provides function surrogates for many other function such as the absolute function.</p> In\u00a0[4]: Copied! <pre>plot(sj.abs, modes=[\"entropic\", \"quintic\"])\n</pre> plot(sj.abs, modes=[\"entropic\", \"quintic\"]) In\u00a0[5]: Copied! <pre>plot(sj.round, modes=[\"entropic\", \"quintic\"])\n</pre> plot(sj.round, modes=[\"entropic\", \"quintic\"]) In\u00a0[6]: Copied! <pre>soft_relu = sj.st(sj.relu)\n\nx = jnp.arange(-1, 1, 0.01)\nvalues, grads = jax.vmap(jax.value_and_grad(soft_relu))(x)\nplot_value_and_grad(x, values, grads, label_func=\"ReLU\", label_grad=\"Soft gradient\")\n</pre> soft_relu = sj.st(sj.relu)  x = jnp.arange(-1, 1, 0.01) values, grads = jax.vmap(jax.value_and_grad(soft_relu))(x) plot_value_and_grad(x, values, grads, label_func=\"ReLU\", label_grad=\"Soft gradient\") In\u00a0[7]: Copied! <pre>def relu_prod(x, y):\n    # Standard ReLU product, no softening\n    return jax.nn.relu(x) * jax.nn.relu(y)\n\n\nplot_value_grad_2D(relu_prod)\n</pre> def relu_prod(x, y):     # Standard ReLU product, no softening     return jax.nn.relu(x) * jax.nn.relu(y)   plot_value_grad_2D(relu_prod) <p>A naive approach would be to replace each relu with <code>sj.relu_st</code> independently. However, the resulting function will not provide informative gradients for every input. This is due to the chain rule, in which the gradient flowing through one relu is multiplid by the (forward) output of the other relu. As the forward pass is not smoothed, the gradient will sometimes be multiplied by zero, resulting in no informative gradient.</p> In\u00a0[8]: Copied! <pre>def soft_relu_prod_naive(x, y, mode=\"entropic\", softness=1.0):\n    # Naive straight-through implementation\n    return sj.relu_st(x, mode=mode, softness=softness) * sj.relu_st(\n        y, mode=mode, softness=softness\n    )\n\n\nplot_value_grad_2D(soft_relu_prod_naive)\n</pre> def soft_relu_prod_naive(x, y, mode=\"entropic\", softness=1.0):     # Naive straight-through implementation     return sj.relu_st(x, mode=mode, softness=softness) * sj.relu_st(         y, mode=mode, softness=softness     )   plot_value_grad_2D(soft_relu_prod_naive) <p>An alternative approach for softening this function is to apply the straight through trick on the outer level as illustrated below. When applied on the outer level, the forward pass computes the hard product of ReLUs as before, whereas the backward pass differentiates through the product of smooth relus.</p> <p>Notice that we use sj.relu instead of sj.relu_st here.</p> In\u00a0[9]: Copied! <pre>@sj.st\ndef soft_relu_prod_custom_st(x, y, mode=\"entropic\", softness=1.0):\n    # Custom straight-through implementation\n    return sj.relu(x, mode=mode, softness=softness) * sj.relu(\n        y, mode=mode, softness=softness\n    )\n\n\nplot_value_grad_2D(soft_relu_prod_custom_st)\n</pre> @sj.st def soft_relu_prod_custom_st(x, y, mode=\"entropic\", softness=1.0):     # Custom straight-through implementation     return sj.relu(x, mode=mode, softness=softness) * sj.relu(         y, mode=mode, softness=softness     )   plot_value_grad_2D(soft_relu_prod_custom_st) <p>We observe that as expected, this version of the function now also produces informative gradients in the third quadrant. In the simple above example, the only both <code>sj.relu</code> functions take the same parameters, therefore it was easy to just apply the <code>sj.st</code> decorator again.</p> <p>In general, we might want to define custom behavior. This can be implemented by using the <code>@sj.grad_replace</code> decorator, which allows custom control flow conditioned on a <code>forward</code> boolean variable. The function will then execute with <code>forward=True</code> on the forward pass and <code>forward=False</code> on the backward pass.</p> <pre>def grad_replace(fn: Callable) -&gt; Callable:\n    def wrapped(*args, **kwargs):\n        fw_y = fn(*args, **kwargs, forward=True)\n        bw_y = fn(*args, **kwargs, forward=False)\n        return jtu.tree_map(lambda fw, bw: jax.lax.stop_gradient(fw - bw) + bw, fw_y, bw_y)\n    return wrapped\n</pre> In\u00a0[10]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(2, 10))\nbool_array = jax.numpy.greater(x, 0.5)\n\n\ndef boolean_loss(x):\n    return jax.numpy.greater(x, 0.5).sum().astype(\"float32\")\n\n\nboolean_grads = jax.grad(boolean_loss)(x)\n\nplot_array(x, title=\"x\")\nplot_array(bool_array, title=\"jax.numpy.greater(x, 0.5)\")\nplot_array(boolean_grads, title=\"jax.grad(jax.numpy.greater(x, 0.5).sum())(x)\")\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(2, 10)) bool_array = jax.numpy.greater(x, 0.5)   def boolean_loss(x):     return jax.numpy.greater(x, 0.5).sum().astype(\"float32\")   boolean_grads = jax.grad(boolean_loss)(x)  plot_array(x, title=\"x\") plot_array(bool_array, title=\"jax.numpy.greater(x, 0.5)\") plot_array(boolean_grads, title=\"jax.grad(jax.numpy.greater(x, 0.5).sum())(x)\") <p>Example - <code>softjax.greater_st</code> yields useful gradients: Instead of <code>jax.numpy.greater</code>, let's use <code>softjax.greater_st</code> (straight_through variant of <code>softjax.greater</code>). As shown below, thanks to straight-through estimation, <code>softjax.greater_st</code> yields exact Booleans while the gradient of the Boolean loss points in informative directions.</p> In\u00a0[11]: Copied! <pre>def soft_boolean_loss(x):\n    return sj.greater_st(x, 0.5).sum()\n\n\nx = jax.random.uniform(jax.random.key(0), shape=(2, 10))\nbool_array = sj.greater_st(x, 0.5)\nboolean_grads = jax.grad(soft_boolean_loss)(x)\n\nplot_array(x, title=\"x\")\nplot_array(bool_array, title=\"soft_greater_st(x, 0.5)\")\nplot_array(boolean_grads, title=\"jax.grad(soft_greater_st(x, 0.5).sum())(x)\")\n</pre> def soft_boolean_loss(x):     return sj.greater_st(x, 0.5).sum()   x = jax.random.uniform(jax.random.key(0), shape=(2, 10)) bool_array = sj.greater_st(x, 0.5) boolean_grads = jax.grad(soft_boolean_loss)(x)  plot_array(x, title=\"x\") plot_array(bool_array, title=\"soft_greater_st(x, 0.5)\") plot_array(boolean_grads, title=\"jax.grad(soft_greater_st(x, 0.5).sum())(x)\") In\u00a0[12]: Copied! <pre>plot(sj.heaviside, modes=[\"entropic\", \"euclidean\", \"quintic\"])\n</pre> plot(sj.heaviside, modes=[\"entropic\", \"euclidean\", \"quintic\"]) <p>In the above case, the <code>linear</code> and <code>quintic</code> relaxations have the advantage of altering the original function only in a bounded region, a property that can be desireble in some cases.</p> <p>Given the concept of a <code>SoftBool</code>, a probabilistic surrogate for binary logical operations such as <code>jax.numpy.equal</code> and <code>jax.numpy.greater</code> is obtained by simply shifting the sigmoid.</p> <p>Example - Greater operator: <code>sj.greater(x,y)</code> corresponds to shifting <code>sj.heaviside</code> by <code>y</code> to the right. The output can be interpreted as the probability $P(x \\geq y)\\in[0,1]$ with $x\\in\\mathbb{R}$ and $y\\in\\mathbb{R}$.</p> In\u00a0[13]: Copied! <pre>def greater_than_1(x, mode=\"entropic\", softness=1.0):\n    return sj.greater(x, y=jnp.array(1.0), mode=mode, softness=softness)\n\n\nplot(greater_than_1, modes=[\"entropic\", \"quintic\"])\n</pre> def greater_than_1(x, mode=\"entropic\", softness=1.0):     return sj.greater(x, y=jnp.array(1.0), mode=mode, softness=softness)   plot(greater_than_1, modes=[\"entropic\", \"quintic\"]) In\u00a0[14]: Copied! <pre>def not_greater_st(x):\n    return sj.logical_not(sj.greater_st(x, y=0.5, mode=\"entropic\", softness=1.0))\n\n\nx = jnp.arange(-1, 1, 0.01)\nvalues, grads = jax.vmap(jax.value_and_grad(not_greater_st))(x)\nplot_value_and_grad(x, values, grads, label_func=\"not_greater_st\")\n</pre> def not_greater_st(x):     return sj.logical_not(sj.greater_st(x, y=0.5, mode=\"entropic\", softness=1.0))   x = jnp.arange(-1, 1, 0.01) values, grads = jax.vmap(jax.value_and_grad(not_greater_st))(x) plot_value_and_grad(x, values, grads, label_func=\"not_greater_st\") <p>Example - Logical AND: Given two <code>SoftBools</code> $P(A)$ and $P(B)$, the probability that both independent events occur is $P(A \\wedge B) = P(A) \\cdot P(B)$.</p> <pre>def logical_and(x: SoftBool, y: SoftBool) -&gt; SoftBool:\n    return x * y\n</pre> In\u00a0[15]: Copied! <pre>plot_softbool_operation(sj.logical_and)\n</pre> plot_softbool_operation(sj.logical_and) <p>Example - Logical XOR: Softjax computes other soft logic operators such as <code>sj.logical_xor</code> by combining <code>sj.logical_not</code> and <code>sj.logical_and</code>.</p> <pre>def sj.logical_xor(x: SoftBool, y: SoftBool) -&gt; SoftBool:\n    return logical_or(logical_and(x, logical_not(y)), logical_and(logical_not(x), y))\n</pre> In\u00a0[16]: Copied! <pre>plot_softbool_operation(sj.logical_xor)\n</pre> plot_softbool_operation(sj.logical_xor) In\u00a0[17]: Copied! <pre>greater = lambda x, y: sj.greater(x, y, mode=\"entropic\", softness=1.0)\nsoft_where = lambda x, y: sj.where(greater(x, y), x, y)\n\nx = jax.random.uniform(jax.random.key(0), shape=(2, 10))\ny = jax.random.uniform(jax.random.key(1), shape=(2, 10))\n\nplot_array(x, title=\"x\")\nplot_array(y, title=\"y\")\nplot_array(soft_where(x, y), title=\"soft_where(x&gt;y, x, y)\")\n</pre> greater = lambda x, y: sj.greater(x, y, mode=\"entropic\", softness=1.0) soft_where = lambda x, y: sj.where(greater(x, y), x, y)  x = jax.random.uniform(jax.random.key(0), shape=(2, 10)) y = jax.random.uniform(jax.random.key(1), shape=(2, 10))  plot_array(x, title=\"x\") plot_array(y, title=\"y\") plot_array(soft_where(x, y), title=\"soft_where(x&gt;y, x, y)\") In\u00a0[18]: Copied! <pre>x = jnp.array([1, 2, 3])\nprint(jnp.argmax(x))\n</pre> x = jnp.array([1, 2, 3]) print(jnp.argmax(x)) <pre>2\n</pre> <p>In comparison, SoftJAX computes a <code>SoftIndex</code> array. Each entry of a <code>SoftIndex</code> array contains the probability that the index is being selected.</p> In\u00a0[19]: Copied! <pre>x = jnp.array([1, 2, 3])\nprint(sj.argmax(x))\n</pre> x = jnp.array([1, 2, 3]) print(sj.argmax(x)) <pre>[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n</pre> <p>Example - Softmax: The \"softmax\" (or more precisely \"softargmax\") is a commonly used  differentiable surrogate for the <code>argmax</code> function (it is also the default softening mode in <code>sj.argmax</code>). The $\\text{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j\\exp(x_j)}$ returns a discrete probability distribution over indices (aka a <code>SoftIndex</code>). As shown in the plots below, the softmax is fully differentiable. It is commonly used for multi-class classification and in transformer networks.</p> <p>When <code>softness</code> is low, <code>sj.argmax</code> concentrates probability on the true maximum index (e.g., <code>[1.0, 0.0, 0.0]</code>), recovering the hard maximum. When <code>softness</code> is higher, the result smoothly interpolates between values, providing useful gradients for optimization.</p> In\u00a0[20]: Copied! <pre>def cross_entropy(x, class_target=5):\n    probs = sj.argmax(x, softness=10.0)\n    target_one_hot = jax.nn.one_hot(class_target, num_classes=x.shape[0])\n    log_probs = jnp.log(probs)\n    return -(target_one_hot * log_probs).mean()\n\n\nx = jax.random.normal(jax.random.key(0), shape=(10,))\nprobs = sj.argmax(x, softness=10.0)\nlossgrads = jax.grad(cross_entropy)(x)\n\nplot_softindices_1D(x, title=\"logits\")\nplot_softindices_1D(probs, title=\"index probabilities (softmax)\")\nplot_softindices_1D(lossgrads, title=\"gradients of cross entropy loss\")\n</pre> def cross_entropy(x, class_target=5):     probs = sj.argmax(x, softness=10.0)     target_one_hot = jax.nn.one_hot(class_target, num_classes=x.shape[0])     log_probs = jnp.log(probs)     return -(target_one_hot * log_probs).mean()   x = jax.random.normal(jax.random.key(0), shape=(10,)) probs = sj.argmax(x, softness=10.0) lossgrads = jax.grad(cross_entropy)(x)  plot_softindices_1D(x, title=\"logits\") plot_softindices_1D(probs, title=\"index probabilities (softmax)\") plot_softindices_1D(lossgrads, title=\"gradients of cross entropy loss\") <p>Note that while in a conventional array of indices, the index information is stored in the integer values, a <code>SoftIndex</code> stores the probabilities over possible indices in an extra dimension. By convention, we always put this additional dimension into the final axis. Except for this additional final dimension, the shape of the returned soft index matches that of the indices returned by standard JAX. Here are a few examples of this:</p> In\u00a0[21]: Copied! <pre>x = jnp.arange(12).reshape((3, 4))\nprint(\"x.shape:\", x.shape)\nprint(\"jnp.argmax(x, axis=1).shape:\", jnp.argmax(x, axis=1).shape)\nprint(\"sj.argmax(x, axis=1).shape:\", sj.argmax(x, axis=1).shape)\nprint(\"jnp.argmax(x, axis=0).shape:\", jnp.argmax(x, axis=0).shape)\nprint(\"sj.argmax(x, axis=0).shape:\", sj.argmax(x, axis=0).shape)\nprint(\n    \"jnp.argmax(x, axis=1, keepdims=True).shape:\",\n    jnp.argmax(x, axis=1, keepdims=True).shape,\n)\nprint(\n    \"sj.argmax(x, axis=1, keepdims=True).shape:\",\n    sj.argmax(x, axis=1, keepdims=True).shape,\n)\nprint(\n    \"jnp.argmax(x, axis=0, keepdims=True).shape:\",\n    jnp.argmax(x, axis=0, keepdims=True).shape,\n)\nprint(\n    \"sj.argmax(x, axis=0, keepdims=True).shape:\",\n    sj.argmax(x, axis=0, keepdims=True).shape,\n)\n</pre> x = jnp.arange(12).reshape((3, 4)) print(\"x.shape:\", x.shape) print(\"jnp.argmax(x, axis=1).shape:\", jnp.argmax(x, axis=1).shape) print(\"sj.argmax(x, axis=1).shape:\", sj.argmax(x, axis=1).shape) print(\"jnp.argmax(x, axis=0).shape:\", jnp.argmax(x, axis=0).shape) print(\"sj.argmax(x, axis=0).shape:\", sj.argmax(x, axis=0).shape) print(     \"jnp.argmax(x, axis=1, keepdims=True).shape:\",     jnp.argmax(x, axis=1, keepdims=True).shape, ) print(     \"sj.argmax(x, axis=1, keepdims=True).shape:\",     sj.argmax(x, axis=1, keepdims=True).shape, ) print(     \"jnp.argmax(x, axis=0, keepdims=True).shape:\",     jnp.argmax(x, axis=0, keepdims=True).shape, ) print(     \"sj.argmax(x, axis=0, keepdims=True).shape:\",     sj.argmax(x, axis=0, keepdims=True).shape, ) <pre>x.shape: (3, 4)\njnp.argmax(x, axis=1).shape: (3,)\nsj.argmax(x, axis=1).shape: (3, 4)\njnp.argmax(x, axis=0).shape: (4,)\nsj.argmax(x, axis=0).shape: (4, 3)\n</pre> <pre>jnp.argmax(x, axis=1, keepdims=True).shape: (3, 1)\nsj.argmax(x, axis=1, keepdims=True).shape: (3, 1, 4)\njnp.argmax(x, axis=0, keepdims=True).shape: (1, 4)\nsj.argmax(x, axis=0, keepdims=True).shape: (1, 4, 3)\n</pre> <p>We also offer soft versions of <code>argmedian</code>, <code>argtop_k</code> and <code>argsort</code>.</p> In\u00a0[22]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(4,))\nprint(\"x:\", x)\nprint(\"jnp.argmedian(x):\\n\", \"Not implemented in standard JAX\")\nprint(\"sj.argmedian(x):\\n\", sj.argmedian(x))\n\nprint(\"jax.lax.top_k(x, k=3)[1]:\\n\", jax.lax.top_k(x, k=3)[1])\nprint(\"sj.argtop_k(x, k=3):\\n\", sj.argtop_k(x, k=3))\n\nprint(\"jnp.argsort(x):\\n\", jnp.argsort(x))\nprint(\"sj.argsort(x):\\n\", sj.argsort(x))\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(4,)) print(\"x:\", x) print(\"jnp.argmedian(x):\\n\", \"Not implemented in standard JAX\") print(\"sj.argmedian(x):\\n\", sj.argmedian(x))  print(\"jax.lax.top_k(x, k=3)[1]:\\n\", jax.lax.top_k(x, k=3)[1]) print(\"sj.argtop_k(x, k=3):\\n\", sj.argtop_k(x, k=3))  print(\"jnp.argsort(x):\\n\", jnp.argsort(x)) print(\"sj.argsort(x):\\n\", sj.argsort(x)) <pre>x: [0.41845711 0.21629545 0.96532146 0.57450053]\njnp.argmedian(x):\n Not implemented in standard JAX\n</pre> <pre>sj.argmedian(x):\n [4.95553956e-01 8.69234800e-03 1.99739291e-04 4.95553956e-01]\njax.lax.top_k(x, k=3)[1]:\n [2 3 0]\n</pre> <pre>sj.argtop_k(x, k=3):\n [[4.11469085e-03 5.44954558e-04 9.75750772e-01 1.95895826e-02]\n [1.66975269e-01 2.21144035e-02 1.59597616e-02 7.94950566e-01]\n [7.42554231e-01 9.83447670e-02 3.13131302e-03 1.55969689e-01]]\njnp.argsort(x):\n [1 0 3 2]\nsj.argsort(x):\n [[1.14092958e-01 8.61461280e-01 4.81124140e-04 2.39646378e-02]\n [7.42554231e-01 9.83447670e-02 3.13131302e-03 1.55969689e-01]\n [1.66975269e-01 2.21144035e-02 1.59597616e-02 7.94950566e-01]\n [4.11469085e-03 5.44954558e-04 9.75750772e-01 1.95895826e-02]]\n</pre> <p>Again, the shape of the returned <code>SoftIndex</code> matches that of the normal index array, except for an additional dimension in the last axis that matches the size of the input array along the specified axis. A few examples:</p> In\u00a0[23]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(3, 4))\nprint(\"x.shape:\", x.shape)\n# standard JAX only added support for axis argument in jax.lax.top_k recently, normally uses last axis\nprint(\"jax.lax.top_k(x, k=2, axis=1)[1].shape:\", jax.lax.top_k(x, k=2)[1].shape)\nprint(\"sj.argtop_k(x, k=2, axis=1).shape:\", sj.argtop_k(x, k=2, axis=1).shape)\nprint(\"sj.argtop_k(x, k=2, axis=0).shape:\", sj.argtop_k(x, k=2, axis=0).shape)\nprint(\"jnp.argsort(x, axis=1).shape:\", jnp.argsort(x, axis=1).shape)\nprint(\"sj.argsort(x, axis=1).shape:\", sj.argsort(x, axis=1).shape)\nprint(\"jnp.argsort(x, axis=0).shape:\", jnp.argsort(x, axis=0).shape)\nprint(\"sj.argsort(x, axis=0).shape:\", sj.argsort(x, axis=0).shape)\n# standard JAX does not support argmedian\nprint(\"sj.argmedian(x, axis=1).shape:\", sj.argmedian(x, axis=1).shape)\nprint(\"sj.argmedian(x, axis=0).shape:\", sj.argmedian(x, axis=0).shape)\nprint(\n    \"sj.argmedian(x, axis=1, keepdims=True).shape:\",\n    sj.argmedian(x, axis=1, keepdims=True).shape,\n)\nprint(\n    \"sj.argmedian(x, axis=0, keepdims=True).shape:\",\n    sj.argmedian(x, axis=0, keepdims=True).shape,\n)\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(3, 4)) print(\"x.shape:\", x.shape) # standard JAX only added support for axis argument in jax.lax.top_k recently, normally uses last axis print(\"jax.lax.top_k(x, k=2, axis=1)[1].shape:\", jax.lax.top_k(x, k=2)[1].shape) print(\"sj.argtop_k(x, k=2, axis=1).shape:\", sj.argtop_k(x, k=2, axis=1).shape) print(\"sj.argtop_k(x, k=2, axis=0).shape:\", sj.argtop_k(x, k=2, axis=0).shape) print(\"jnp.argsort(x, axis=1).shape:\", jnp.argsort(x, axis=1).shape) print(\"sj.argsort(x, axis=1).shape:\", sj.argsort(x, axis=1).shape) print(\"jnp.argsort(x, axis=0).shape:\", jnp.argsort(x, axis=0).shape) print(\"sj.argsort(x, axis=0).shape:\", sj.argsort(x, axis=0).shape) # standard JAX does not support argmedian print(\"sj.argmedian(x, axis=1).shape:\", sj.argmedian(x, axis=1).shape) print(\"sj.argmedian(x, axis=0).shape:\", sj.argmedian(x, axis=0).shape) print(     \"sj.argmedian(x, axis=1, keepdims=True).shape:\",     sj.argmedian(x, axis=1, keepdims=True).shape, ) print(     \"sj.argmedian(x, axis=0, keepdims=True).shape:\",     sj.argmedian(x, axis=0, keepdims=True).shape, ) <pre>x.shape: (3, 4)\njax.lax.top_k(x, k=2, axis=1)[1].shape: (3, 2)\n</pre> <pre>sj.argtop_k(x, k=2, axis=1).shape: (3, 2, 4)\nsj.argtop_k(x, k=2, axis=0).shape: (2, 3, 4)\njnp.argsort(x, axis=1).shape: (3, 4)\nsj.argsort(x, axis=1).shape: (3, 4, 4)\n</pre> <pre>jnp.argsort(x, axis=0).shape: (3, 4)\n</pre> <pre>sj.argsort(x, axis=0).shape: (3, 4, 3)\nsj.argmedian(x, axis=1).shape: (3, 4)\n</pre> <pre>sj.argmedian(x, axis=0).shape: (4, 3)\nsj.argmedian(x, axis=1, keepdims=True).shape: (3, 1, 4)\nsj.argmedian(x, axis=0, keepdims=True).shape: (1, 4, 3)\n</pre> <p>Note: All of the functions in this section come with the three modes: <code>hard</code>, <code>entropic</code> and <code>euclidean</code>. <code>hard</code> mode produces one-hot soft indices and is mainly used in straight-through estimation. <code>entropic</code> is the recommended soft default and reduces all operations to either a softmax or an entropy-regularized optimal transport problem. Finally, <code>euclidean</code> reduces operations to L2 projection onto the unit simplex or the Birkhoff polytope, which can be used to produce sparse outputs. See the API documentation for details.</p> In\u00a0[24]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(2, 3))\nprint(\"x:\\n\", x)\n\nindices = jnp.argmin(x, axis=1, keepdims=True)\nprint(\"min_jnp:\\n\", jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = sj.argmin(x, axis=1, mode=\"hard\", keepdims=True)\nprint(\"min_sj_hard:\\n\", sj.take_along_axis(x, indices_onehot, axis=1))\n\nindices_soft = sj.argmin(x, axis=1, keepdims=True)\nprint(\"min_sj_soft:\\n\", sj.take_along_axis(x, indices_soft, axis=1))\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(2, 3)) print(\"x:\\n\", x)  indices = jnp.argmin(x, axis=1, keepdims=True) print(\"min_jnp:\\n\", jnp.take_along_axis(x, indices, axis=1))  indices_onehot = sj.argmin(x, axis=1, mode=\"hard\", keepdims=True) print(\"min_sj_hard:\\n\", sj.take_along_axis(x, indices_onehot, axis=1))  indices_soft = sj.argmin(x, axis=1, keepdims=True) print(\"min_sj_soft:\\n\", sj.take_along_axis(x, indices_soft, axis=1)) <pre>x:\n [[0.41845711 0.21629545 0.96532146]\n [0.57450053 0.53222649 0.35490518]]\n</pre> <pre>min_jnp:\n [[0.21629545]\n [0.35490518]]\n</pre> <pre>min_sj_hard:\n [[0.21629545]\n [0.35490518]]\nmin_sj_soft:\n [[0.24029622]\n [0.39747788]]\n</pre> <p>As a convenience, this combination of <code>sj.take_along_axis</code> with <code>SoftIndex</code>-generataing functions is already implemented ino Softjax's <code>max</code>, <code>median</code>, <code>top_k</code> and <code>sort</code> functions.</p> In\u00a0[25]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(4,))\nprint(\"x:\", x)\n\nprint(\"jnp.max(x):\", jnp.max(x))\nprint(\"sj.max(x, mode='hard'):\", sj.max(x, mode=\"hard\"))\nprint(\"sj.max(x):\", sj.max(x))\n\nprint(\"jnp.median(x):\", jnp.median(x))\nprint(\"sj.median(x, mode='hard'):\", sj.median(x, mode=\"hard\"))\nprint(\"sj.median(x):\", sj.median(x))\n\nprint(\"jax.lax.top_k(x, k=2)[0]:\", jax.lax.top_k(x, k=2)[0])\nprint(\"sj.top_k(x, k=2, mode='hard')[0]:\", sj.top_k(x, k=2, mode=\"hard\")[0])\nprint(\"sj.top_k(x, k=2)[0]:\", sj.top_k(x, k=2)[0])\n\nprint(\"jnp.sort(x):\", jnp.sort(x))\nprint(\"sj.sort(x, mode='hard'):\", sj.sort(x, mode=\"hard\"))\nprint(\"sj.sort(x):\", sj.sort(x))\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(4,)) print(\"x:\", x)  print(\"jnp.max(x):\", jnp.max(x)) print(\"sj.max(x, mode='hard'):\", sj.max(x, mode=\"hard\")) print(\"sj.max(x):\", sj.max(x))  print(\"jnp.median(x):\", jnp.median(x)) print(\"sj.median(x, mode='hard'):\", sj.median(x, mode=\"hard\")) print(\"sj.median(x):\", sj.median(x))  print(\"jax.lax.top_k(x, k=2)[0]:\", jax.lax.top_k(x, k=2)[0]) print(\"sj.top_k(x, k=2, mode='hard')[0]:\", sj.top_k(x, k=2, mode=\"hard\")[0]) print(\"sj.top_k(x, k=2)[0]:\", sj.top_k(x, k=2)[0])  print(\"jnp.sort(x):\", jnp.sort(x)) print(\"sj.sort(x, mode='hard'):\", sj.sort(x, mode=\"hard\")) print(\"sj.sort(x):\", sj.sort(x)) <pre>x: [0.41845711 0.21629545 0.96532146 0.57450053]\njnp.max(x): 0.9653214611189975\nsj.max(x, mode='hard'): 0.9653214611189975\n</pre> <pre>sj.max(x): 0.9550070794223621\n</pre> <pre>jnp.median(x): 0.49647882272194555\nsj.median(x, mode='hard'): 0.49647882272194555\nsj.median(x): 0.494137017678798\njax.lax.top_k(x, k=2)[0]: [0.96532146 0.57450053]\n</pre> <pre>sj.top_k(x, k=2, mode='hard')[0]: [0.96532146 0.57450053]\nsj.top_k(x, k=2)[0]: [0.95500708 0.54676106]\njnp.sort(x): [0.21629545 0.41845711 0.57450053 0.96532146]\nsj.sort(x, mode='hard'): [0.21629545 0.41845711 0.57450053 0.96532146]\nsj.sort(x): [0.24830531 0.42462602 0.54676106 0.95500708]\n</pre> <p>Finally, we also offer a soft <code>ranking</code> operation. While it does not return a <code>SoftIndex</code> (because its output is the same shape as the input), it relies on similar computations under the hood as e.g. <code>sort</code>, and also offers the same modes.</p> In\u00a0[26]: Copied! <pre>x = jax.random.uniform(jax.random.key(0), shape=(5,))\nprint(\"x:\\n\", x)\n# This computes the ranking operation\nprint(\"jnp.argsort(jnp.argsort(x)):\\n\", jnp.argsort(jnp.argsort(x)))\nprint(\n    \"sj.ranking(x, descending=False, mode='hard'):\\n\",\n    sj.ranking(x, descending=False, mode=\"hard\"),\n)\nprint(\"sj.ranking(x, descending=False):\\n\", sj.ranking(x, descending=False))\n</pre> x = jax.random.uniform(jax.random.key(0), shape=(5,)) print(\"x:\\n\", x) # This computes the ranking operation print(\"jnp.argsort(jnp.argsort(x)):\\n\", jnp.argsort(jnp.argsort(x))) print(     \"sj.ranking(x, descending=False, mode='hard'):\\n\",     sj.ranking(x, descending=False, mode=\"hard\"), ) print(\"sj.ranking(x, descending=False):\\n\", sj.ranking(x, descending=False)) <pre>x:\n [0.41845711 0.21629545 0.96532146 0.57450053 0.53222649]\njnp.argsort(jnp.argsort(x)):\n [1 0 4 3 2]\nsj.ranking(x, descending=False, mode='hard'):\n [1. 0. 4. 3. 2.]\n</pre> <pre>sj.ranking(x, descending=False):\n [1.37238141 0.25184717 3.94097212 2.40480632 2.13591075]\n</pre> <p>The naming of the modes stems from a reduction of simple higher-level functions like <code>sj.abs</code> to more complex lower-level functions like <code>sj.argmax</code>. Starting from the <code>argmax</code> function, we relax it with a projection onto the unit simplex. We offer two modes, <code>entropic</code> and <code>euclidean</code> (in fact, ALL functions in Softjax support at least these two and the <code>hard</code> mode), which determine the regularizer used in the relaxed optimization problem. The solution to the <code>euclidean</code> case is available in closed-form via the classic softmax function, the <code>euclidean</code> case is a simple L2-projection onto the unit simplex which boils down to a sort+cumsum operation. Given the <code>argmax</code> relaxation, we directly get a <code>max</code> relaxation by taking the inner product of the soft indices with the original vector. Now we can define a relaxation of the <code>heaviside</code> function from the softened <code>argmax</code> operation, by observing that $\\text{heaviside}(x)=\\text{argmax}([x,0])[0]$. This results in different S-shaped sigmoid functions, in fact the standard exponential-sigmoid is the closed-solution to the <code>entropic</code> mode, whereas a linear inteprolation between 0 and 1 is the closed-form solution to the <code>euclidean</code> mode. Besides these modes, we define additional heaviside modes like <code>cubic</code>, <code>quintic</code> and <code>pseudohuber</code>, which all define different sigmoidal functions with different properties. Our heaviside relaxation can now be used to define relaxations for e.g. the <code>sign</code>, <code>abs</code> and <code>round</code> function. Most importantly though, we can move up the ladder to even higher-level functions based on the <code>ReLU</code> function. We first observe that we can generate the ReLU function from the heaviside function in two ways:</p> <ul> <li>By integrating $\\text{heaviside}(x)$ from negative infinity to x.</li> <li>By taking $x \\cdot \\text{heaviside}(x)$ (a \"gating\"-mechanism).</li> </ul> <p>Therefore, for each of our heaviside relaxations we can define two ReLU relaxations, some of which are well known. For example, the <code>entropic</code> case leads to the classic <code>softplus</code> function when integrated, and to the <code>SiLU</code> function when \"gated\" (we refer to this relaxation mode as <code>gated_entropic</code>). Similarly, the <code>euclidean</code>, <code>cubic</code> and <code>quintic</code> are simple piecewise-polynomials that we can integrate in closed-form. We can now use the soft ReLU relaxation to define more relaxations like e.g. <code>clip</code>.</p>"},{"location":"all-of-softjax/#all-of-softjax","title":"All of Softjax\u00b6","text":"<p>Softjax provides easy-to-use differentiable function surrogates of non-differentiable functions and discrete logic operations in JAX. Softjax offers soft function surrogates operating on real values, Booleans, and indices as well as wrappers to use these functions for gradient computation via straight-through estimation.</p> <p>This pages guides you through all of Softjax key functionalities.</p>"},{"location":"all-of-softjax/#1-softening","title":"1. Softening\u00b6","text":"<p>Many functions are not particulary well suited for gradient-based optimization as their gradients are either zero or undefined at discontinuities. Therefore, a typical approach in machine learning is to use soft surrogates for these functions to enable gradient-based optimization via automatic differentiation. Softjax provides such soft surrogates for many operations in JAX.</p> <p>As shown below, Softjax provides two hyper-parameters to tune soft function surrogates:</p> <ul> <li><p>mode being the type of function used for obtaining a soft approximation.</p> </li> <li><p>softness defining how close the surrogate function approximates the original function.</p> </li> </ul>"},{"location":"all-of-softjax/#2-straight-through-estimation","title":"2. Straight-through estimation\u00b6","text":"<p>Soft function surrogates can be used to compute soft gradients (or more accurately: soft vector-jacobian-producs) without modifying the forward pass via straight-through estimation. Straight-through estimation uses JAX's automatic differentiation system to replace only the function's gradient with the gradient of the surrogate function.</p> <p>Note: Historically, straight-through estimators refer to the special case of treating a function as the identity operation on the backward pass. We use the term more generally to describe the case of replacing a function with smooth a surrogate on the backward pass.</p> <p>Example - ReLu activation: The rectified linear unit (aka <code>relu</code>)  is commonly used as activation function in neural networks. For $x&lt;0$ the gradient of the <code>relu</code> is zero. In turn, neural networks containing <code>relu</code> activations may suffer from the \"dying ReLu problem\" where the gradients computed via automatic differentiation become zero when a gradient-based optimizer adjusts the inputs of ReLU functions to $x&lt;0$. As pointed out in \"The Resurrection of the ReLU\", you can mitigate these problems by replacing its backward pass with a soft surrogate function. To do this with Softjax, simply replace the <code>relu</code> activation in your code with <code>sj.st(sj.relu)</code>, or equivalently directly use our wrapped primitives via <code>sj.relu_st</code> for convenience.</p>"},{"location":"all-of-softjax/#the-ste-trick","title":"The STE Trick\u00b6","text":"<p>Under the hood <code>sj.st()</code> uses the stop-gradient oepration to replace the gradient of a function.</p> <pre>def st(fn: Callable) -&gt; Callable:\n    sig = inspect.signature(fn)\n    mode_default = sig.parameters.get(\"mode\").default\n    def wrapped(*args, **kwargs):\n        mode = kwargs.pop(\"mode\", mode_default)\n        fw_y = fn(*args, **kwargs, mode=\"hard\")\n        bw_y = fn(*args, **kwargs, mode=mode)\n        return jtu.tree_map(lambda fw, bw: jax.lax.stop_gradient(fw - bw) + bw, fw_y, bw_y)\n    return wrapped\n</pre> <p>By adding and subtracting the backward function <code>bw_y</code> to the function call, it does not alter the function's forward pass <code>fw_y</code>. Due to <code>jax.lax.stop_gradient</code>, only the soft backward function <code>bw_y</code> is used in the gradient computation.</p>"},{"location":"all-of-softjax/#custom-differentiation-rules","title":"Custom differentiation rules\u00b6","text":"<p>The <code>@sj.st</code> decorator can also be used to define custom straight-through operations. This can be useful when combining multiple functions provided by the softjax library. For this, it is important to understand, that simply applying the straight-through trick to every non-smooth function does not always result in the intended behavior. Consider for example the case of multiplying the output of two <code>relu</code> functions together. This function only provides meaningful gradients in the first quadrant, we would like to change it such that we get a meaningful signal in the whole domain, as visualized below.</p> <p>Note: We normalize the plotted gradient vectors for reduced cluttering.</p>"},{"location":"all-of-softjax/#3-soft-bools","title":"3. Soft bools\u00b6","text":"<p>Softjax provides differentiable surrogates of JAX's Boolean operators. A Boolean (aka Bool) is a data type that takes one of two possible values either being <code>false</code> or <code>true</code> (aka 0 or 1). Many operations in JAX such as <code>greater</code> or <code>isclose</code> generate arrays containing Booleans, while other operations such as <code>logical_and</code> or <code>any</code> operate on such arrays.</p> <p>Example - <code>jax.numpy.greater</code> yields zero gradients: As shown below, <code>jax.grad</code> does not raise an error when called on its boolean operations. However, the returned gradients are zero for all array entries.</p>"},{"location":"all-of-softjax/#generating-soft-bools","title":"Generating soft bools\u00b6","text":"<p>How does Softjax make Boolean logic operations differentiable? A real number $x\\in \\mathbb{R}$ could be mapped to a <code>Bool</code> using the Heaviside function $$ H(x) = \\begin{cases} 1, &amp; x &gt; 0 \\\\ 0.5, &amp; x=0\\\\ 0, &amp; x &lt; 0 \\end{cases}. $$ The gradient of the Heaviside function (as implemented in JAX) is zero everywhere and hence unsuited for differentiable optimization. Instead of operating directly on Booleans, Softjax's differentiable logic operators resort to soft Booleans. A soft Boolean aka <code>SoftBool</code> can be interpreted as the probability of a Boolean being <code>True</code>.</p> <p>We replace the heaviside function with differentiable surrogate such as the sigmoid function $\\sigma(x) = \\frac{1}{1+e^{-x}}$. While the <code>sigmoid</code> is the canonical example for mapping a real number to a <code>SoftBool</code>, Softjax provides additional surrogates.</p>"},{"location":"all-of-softjax/#manipulating-soft-bools","title":"Manipulating soft bools\u00b6","text":"<p>Softjax replaces a Boolean with a <code>SoftBool</code>, in turn Boolean logic operators are replaced in Softjax with fuzzy logic operators that effectively compute the probabilities of Boolean events.</p> <p>Example - Logical NOT: Given a <code>SoftBool</code> $P(B)$ (being the probability that a Boolean event $B$ occurs), the probability of the event not occuring is $P(\\bar B) = 1 - P(B)$ as implemented in <code>sj.logical_not</code>.</p> <pre>def logical_not(x: SoftBool) -&gt; SoftBool:\n    return 1 - x\n</pre> <p>Given <code>sj.logical_not</code>, the probability that <code>x is not greater equal 0.5</code> is given by <code>sj.logical_not(sj.greater_st(x, 0.5))</code>. Due to the straight-through trick, the function <code>sj.logical_not(sj.greater_st(x, 0.5))</code> uses exact Boolean logic in the forward pass and the <code>SoftBool</code> probability computation in the backward pass.</p>"},{"location":"all-of-softjax/#selection-with-soft-bools","title":"Selection with soft bools\u00b6","text":"<p>Through the use of Fuzzy logic operators, Softjax provides a toolbox to make many non-differentiable functions of JAX differentiable.</p> <p>Example - sj.where(): The function <code>jax.numpy.where(condition, x, y)</code> selects elements of array <code>x</code> if <code>condition == True</code> and otherwise selects <code>y</code>. Softjax provides a differentiable surrogate for this function via <code>sj.where(P, x, y)</code> which effectively computes the expected value $\\mathbb{E}[X] = P \\cdot x + (1-P) \\cdot y$.</p>"},{"location":"all-of-softjax/#4-soft-indices","title":"4. Soft indices\u00b6","text":"<p>Softjax offers soft surrogates for functions that generate indices as outputs, such as <code>argmax</code>, <code>argmin</code>, <code>argtop_k</code>, <code>argmedian</code>, and <code>argsort</code>. The main mechanism here is to replace hard indices with distributions over indices (<code>SoftIndex</code>), allowing for informative gradients.</p> <p>Similar to how <code>SoftBool</code> required going from boolean logic to fuzzy logic, this now requires adjusting functions that do selection via indices. As such, we provide new versions of e.g. <code>take_along_axis</code>, <code>dynamic_index_in_dim</code>, and <code>choose</code>. Combining the soft index generation with the selection then allows to define surrogates for the corresponding <code>max</code>, <code>min</code>, <code>top_k</code>, <code>median</code> and <code>sort</code> functions.</p>"},{"location":"all-of-softjax/#generating-soft-indices","title":"Generating soft indices\u00b6","text":"<p>In JAX, functions like <code>jax.argmax</code> return integer indices as outputs, which can take values within {0, ..., len(x)-1}.</p>"},{"location":"all-of-softjax/#selection-with-softindices","title":"Selection with SoftIndices\u00b6","text":"<p>Given a <code>SoftIndex</code>, Softjax provides (differentiable) helper functions for selecting array elements, mirroring the non-differentiable indexing in standard JAX. Put simply, entries of an array are selected by computing the expected value:</p> <p>$$\\mathrm{E}(arr, p) = \\sum_{i} arr[i] \\cdot p[i] = arr^{\\top} \\cdot p$$</p> <p>where $p$ is the <code>SoftIndex</code>.</p> <p>Example <code>sj.take_along_axis</code>: The function <code>sj.take_along_axis</code> is central to this selection mechanism. It generalizes <code>jnp.take_along_axis</code> to work with probability distributions (SoftIndices) instead of just integer indices.</p> <p>The standard <code>jnp.take_along_axis(arr, indices, axis)</code> selects elements from <code>arr</code> using integer indices. Conceptually, it works by:</p> <ol> <li>Slicing along the specified axis to get 1D arrays</li> <li>Using the corresponding indices to select elements</li> <li>Assembling the results into the output array</li> </ol> <p>One of its main uses is to accept the index output of e.g. <code>jnp.argmax</code> and select the maximum values at the indexed locations. While <code>jnp.take_along_axis</code> uses integer indices <code>out_1d[j] = arr_1d[indices_1d[j]]</code>, <code>sj.take_along_axis</code> accepts a <code>SoftIndex</code> to compute the corresponding the weighted sum: <code>out_1d[j] = sum_i(arr_1d[i] * soft_indices_2d[j, i])</code>.</p>"},{"location":"all-of-softjax/#a-note-on-modes","title":"A note on modes\u00b6","text":""},{"location":"manifold_points/","title":"Manifold Points","text":"In\u00a0[1]: Copied! <pre>from dataclasses import dataclass\nfrom typing import Literal\n\nimport jax\nimport jax.numpy as jp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport softjax as sj\n\n\njax.config.update(\"jax_enable_x64\", True)\njax.config.update(\"jax_default_matmul_precision\", \"high\")\njax.config.update(\"jax_platforms\", \"cpu\")\n</pre> from dataclasses import dataclass from typing import Literal  import jax import jax.numpy as jp import matplotlib.pyplot as plt import numpy as np import softjax as sj   jax.config.update(\"jax_enable_x64\", True) jax.config.update(\"jax_default_matmul_precision\", \"high\") jax.config.update(\"jax_platforms\", \"cpu\")  In\u00a0[2]: Copied! <pre># Hard manifold selector mirrored from MJX (collision_convex.py)\ndef manifold_points_mjx(\n    poly: jp.ndarray, poly_mask: jp.ndarray, poly_norm: jp.ndarray\n) -&gt; jp.ndarray:\n    \"\"\"MJX hard manifold heuristic (returns 4 vertex indices).\"\"\"\n    dist_mask = jp.where(poly_mask, 0.0, -1e6)\n    # A: any unmasked vertex (in MJX this corresponds to the most penetrating one)\n    # Note: We add a small tie-breaker to ensure consistent results\n    a_idx = jp.argmax(dist_mask - 0.1 * jp.arange(poly.shape[0]))\n    a = poly[a_idx]\n    # B: farthest from A (largest squared distance)\n    b_idx = (((a - poly) ** 2).sum(axis=1) + dist_mask).argmax()\n    b = poly[b_idx]\n    # C: farthest from the AB line within the plane\n    ab = jp.cross(poly_norm, a - b)\n    ap = a - poly\n    c_idx = (jp.abs(ap.dot(ab)) + dist_mask).argmax()\n    c = poly[c_idx]\n    # D: farthest from edges AC and BC\n    ac = jp.cross(poly_norm, a - c)\n    bc = jp.cross(poly_norm, b - c)\n    bp = b - poly\n    dist_bp = jp.abs(bp.dot(bc)) + dist_mask\n    dist_ap = jp.abs(ap.dot(ac)) + dist_mask\n    d_idx = (dist_bp + dist_ap).argmax() % poly.shape[0]\n    return jp.array([a_idx, b_idx, c_idx, d_idx])\n</pre> # Hard manifold selector mirrored from MJX (collision_convex.py) def manifold_points_mjx(     poly: jp.ndarray, poly_mask: jp.ndarray, poly_norm: jp.ndarray ) -&gt; jp.ndarray:     \"\"\"MJX hard manifold heuristic (returns 4 vertex indices).\"\"\"     dist_mask = jp.where(poly_mask, 0.0, -1e6)     # A: any unmasked vertex (in MJX this corresponds to the most penetrating one)     # Note: We add a small tie-breaker to ensure consistent results     a_idx = jp.argmax(dist_mask - 0.1 * jp.arange(poly.shape[0]))     a = poly[a_idx]     # B: farthest from A (largest squared distance)     b_idx = (((a - poly) ** 2).sum(axis=1) + dist_mask).argmax()     b = poly[b_idx]     # C: farthest from the AB line within the plane     ab = jp.cross(poly_norm, a - b)     ap = a - poly     c_idx = (jp.abs(ap.dot(ab)) + dist_mask).argmax()     c = poly[c_idx]     # D: farthest from edges AC and BC     ac = jp.cross(poly_norm, a - c)     bc = jp.cross(poly_norm, b - c)     bp = b - poly     dist_bp = jp.abs(bp.dot(bc)) + dist_mask     dist_ap = jp.abs(ap.dot(ac)) + dist_mask     d_idx = (dist_bp + dist_ap).argmax() % poly.shape[0]     return jp.array([a_idx, b_idx, c_idx, d_idx])  In\u00a0[3]: Copied! <pre>def manifold_points_softjax(\n    poly: jp.ndarray,\n    poly_mask: jp.ndarray,\n    poly_norm: jp.ndarray,\n    *,\n    softness: float = 1.0,\n    mode: Literal[\"hard\", \"entropic\"] = \"entropic\",\n) -&gt; jp.ndarray:\n    \"\"\"Soft counterpart of `manifold_points_mjx`.\n\n    Returns 4 SoftIndex distributions (shape: 4 x n).\n    \"\"\"\n    dist_mask = jp.where(poly_mask, 0.0, -1e6)\n\n    def argmax_soft(x):\n        return sj.argmax(x, mode=mode, softness=softness, axis=0)\n\n    def abs_soft(x):\n        return sj.abs(x, mode=mode, softness=softness)\n\n    # A: soft argmax over masked distances\n    a_idx = argmax_soft(dist_mask - 0.1 * jp.arange(poly.shape[0]))\n    a = sj.dynamic_index_in_dim(poly, a_idx, axis=0, keepdims=False)\n    # B: soft argmax of distance from A\n    b_logits = (((a - poly) ** 2).sum(axis=1)) + dist_mask\n    b_idx = argmax_soft(b_logits)\n    b = sj.dynamic_index_in_dim(poly, b_idx, axis=0, keepdims=False)\n    # C: soft argmax farthest from AB line\n    ab = jp.cross(poly_norm, a - b)\n    ap = a - poly\n    c_logits = abs_soft(ap.dot(ab)) + dist_mask\n    c_idx = argmax_soft(c_logits)\n    c = sj.dynamic_index_in_dim(poly, c_idx, axis=0, keepdims=False)\n    # D: soft argmax farthest from AC and BC edges\n    ac = jp.cross(poly_norm, a - c)\n    bc = jp.cross(poly_norm, b - c)\n    bp = b - poly\n    dist_bp = abs_soft(bp.dot(bc)) + dist_mask\n    dist_ap = abs_soft(ap.dot(ac)) + dist_mask\n    d_logits = dist_bp + dist_ap\n    d_idx = argmax_soft(d_logits)\n    return jp.stack([a_idx, b_idx, c_idx, d_idx], axis=0)\n</pre> def manifold_points_softjax(     poly: jp.ndarray,     poly_mask: jp.ndarray,     poly_norm: jp.ndarray,     *,     softness: float = 1.0,     mode: Literal[\"hard\", \"entropic\"] = \"entropic\", ) -&gt; jp.ndarray:     \"\"\"Soft counterpart of `manifold_points_mjx`.      Returns 4 SoftIndex distributions (shape: 4 x n).     \"\"\"     dist_mask = jp.where(poly_mask, 0.0, -1e6)      def argmax_soft(x):         return sj.argmax(x, mode=mode, softness=softness, axis=0)      def abs_soft(x):         return sj.abs(x, mode=mode, softness=softness)      # A: soft argmax over masked distances     a_idx = argmax_soft(dist_mask - 0.1 * jp.arange(poly.shape[0]))     a = sj.dynamic_index_in_dim(poly, a_idx, axis=0, keepdims=False)     # B: soft argmax of distance from A     b_logits = (((a - poly) ** 2).sum(axis=1)) + dist_mask     b_idx = argmax_soft(b_logits)     b = sj.dynamic_index_in_dim(poly, b_idx, axis=0, keepdims=False)     # C: soft argmax farthest from AB line     ab = jp.cross(poly_norm, a - b)     ap = a - poly     c_logits = abs_soft(ap.dot(ab)) + dist_mask     c_idx = argmax_soft(c_logits)     c = sj.dynamic_index_in_dim(poly, c_idx, axis=0, keepdims=False)     # D: soft argmax farthest from AC and BC edges     ac = jp.cross(poly_norm, a - c)     bc = jp.cross(poly_norm, b - c)     bp = b - poly     dist_bp = abs_soft(bp.dot(bc)) + dist_mask     dist_ap = abs_soft(ap.dot(ac)) + dist_mask     d_logits = dist_bp + dist_ap     d_idx = argmax_soft(d_logits)     return jp.stack([a_idx, b_idx, c_idx, d_idx], axis=0)  In\u00a0[4]: Copied! <pre>@dataclass\nclass Selection:\n    poly: np.ndarray\n    hard_idx: np.ndarray\n    hard_pts: np.ndarray\n    soft_runs: list[tuple[float, np.ndarray]]\n\n\ndef make_planar_polygon(\n    n: int = 8, radius: float = 1.0, jitter: float = 0.05, seed: int = 0\n):\n    rng = np.random.RandomState(seed)\n    angles = np.linspace(0, 2 * np.pi, n, endpoint=False)\n    r = radius * (1.0 + jitter * rng.randn(n))\n    x = r * np.cos(angles)\n    y = r * np.sin(angles)\n    poly2d = np.stack([x, y], axis=1)\n    center = poly2d.mean(axis=0)\n    angles_ccw = np.arctan2(poly2d[:, 1] - center[1], poly2d[:, 0] - center[0])\n    order = np.argsort(angles_ccw)\n    poly2d = poly2d[order]\n    poly3d = np.concatenate([poly2d, np.zeros((n, 1))], axis=1)\n    return jp.array(poly3d)\n\n\ndef order_quad(points: np.ndarray):\n    center = points.mean(axis=0)\n    angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])\n    order = np.argsort(angles)\n    return points[order]\n\n\ndef run_selection(\n    n: int = 16,\n    softness: list[float] | tuple[float, ...] = (1.0,),\n    mode: str = \"entropic\",\n    seed: int = 0,\n) -&gt; Selection:\n    poly = make_planar_polygon(n=n, seed=seed)\n    mask = jp.ones((n,), dtype=bool)\n    normal = jp.array([0.0, 0.0, 1.0])\n\n    hard_idx = np.array(manifold_points_mjx(poly, mask, normal))\n    hard_pts = np.array(poly[hard_idx])\n\n    soft_runs = []\n    for w in softness:\n        soft_probs = manifold_points_softjax(\n            poly, mask, normal, softness=float(w), mode=mode\n        )\n        soft_pts = soft_probs @ np.array(poly)\n        soft_runs.append((w, soft_pts))\n\n    return Selection(\n        poly=np.array(poly), hard_idx=hard_idx, hard_pts=hard_pts, soft_runs=soft_runs\n    )\n\n\ndef plot_multi(sel: Selection, mode: str):\n    poly2d = sel.poly[:, :2]\n    hull = order_quad(poly2d)\n    hull_closed = np.vstack([hull, hull[0]])\n\n    fig, ax = plt.subplots(figsize=(8, 7))\n    ax.fill(\n        hull_closed[:, 0],\n        hull_closed[:, 1],\n        color=\"#dddddd\",\n        alpha=0.3,\n        label=\"poly hull\",\n    )\n    ax.plot(hull_closed[:, 0], hull_closed[:, 1], color=\"#999999\", lw=1.0)\n    ax.scatter(poly2d[:, 0], poly2d[:, 1], color=\"#444444\", s=40, label=\"verts\")\n\n    hard_xy = order_quad(sel.hard_pts[:, :2])\n    hard_closed = np.vstack([hard_xy, hard_xy[0]])\n    ax.plot(\n        hard_closed[:, 0], hard_closed[:, 1], color=\"#000000\", lw=2.0, label=\"hard_mjx\"\n    )\n    ax.scatter(hard_xy[:, 0], hard_xy[:, 1], color=\"#000000\", s=60, zorder=3)\n\n    cmap = plt.get_cmap(\"tab10\")\n    for i, (w, soft_pts) in enumerate(sel.soft_runs):\n        soft_xy = order_quad(np.array(soft_pts)[:, :2])\n        soft_closed = np.vstack([soft_xy, soft_xy[0]])\n        color = cmap(i % cmap.N)\n        ax.plot(\n            soft_closed[:, 0],\n            soft_closed[:, 1],\n            color=color,\n            lw=2.0,\n            ls=\"--\",\n            label=f\"soft w={w}\",\n        )\n        ax.scatter(soft_xy[:, 0], soft_xy[:, 1], color=color, s=55, zorder=3)\n\n    ax.set_aspect(\"equal\", adjustable=\"datalim\")\n    ax.set_title(f\"Hard vs soft manifold points (mode={mode})\")\n    ax.legend(loc=\"upper right\")\n    ax.grid(True, alpha=0.2)\n    plt.show()\n</pre> @dataclass class Selection:     poly: np.ndarray     hard_idx: np.ndarray     hard_pts: np.ndarray     soft_runs: list[tuple[float, np.ndarray]]   def make_planar_polygon(     n: int = 8, radius: float = 1.0, jitter: float = 0.05, seed: int = 0 ):     rng = np.random.RandomState(seed)     angles = np.linspace(0, 2 * np.pi, n, endpoint=False)     r = radius * (1.0 + jitter * rng.randn(n))     x = r * np.cos(angles)     y = r * np.sin(angles)     poly2d = np.stack([x, y], axis=1)     center = poly2d.mean(axis=0)     angles_ccw = np.arctan2(poly2d[:, 1] - center[1], poly2d[:, 0] - center[0])     order = np.argsort(angles_ccw)     poly2d = poly2d[order]     poly3d = np.concatenate([poly2d, np.zeros((n, 1))], axis=1)     return jp.array(poly3d)   def order_quad(points: np.ndarray):     center = points.mean(axis=0)     angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])     order = np.argsort(angles)     return points[order]   def run_selection(     n: int = 16,     softness: list[float] | tuple[float, ...] = (1.0,),     mode: str = \"entropic\",     seed: int = 0, ) -&gt; Selection:     poly = make_planar_polygon(n=n, seed=seed)     mask = jp.ones((n,), dtype=bool)     normal = jp.array([0.0, 0.0, 1.0])      hard_idx = np.array(manifold_points_mjx(poly, mask, normal))     hard_pts = np.array(poly[hard_idx])      soft_runs = []     for w in softness:         soft_probs = manifold_points_softjax(             poly, mask, normal, softness=float(w), mode=mode         )         soft_pts = soft_probs @ np.array(poly)         soft_runs.append((w, soft_pts))      return Selection(         poly=np.array(poly), hard_idx=hard_idx, hard_pts=hard_pts, soft_runs=soft_runs     )   def plot_multi(sel: Selection, mode: str):     poly2d = sel.poly[:, :2]     hull = order_quad(poly2d)     hull_closed = np.vstack([hull, hull[0]])      fig, ax = plt.subplots(figsize=(8, 7))     ax.fill(         hull_closed[:, 0],         hull_closed[:, 1],         color=\"#dddddd\",         alpha=0.3,         label=\"poly hull\",     )     ax.plot(hull_closed[:, 0], hull_closed[:, 1], color=\"#999999\", lw=1.0)     ax.scatter(poly2d[:, 0], poly2d[:, 1], color=\"#444444\", s=40, label=\"verts\")      hard_xy = order_quad(sel.hard_pts[:, :2])     hard_closed = np.vstack([hard_xy, hard_xy[0]])     ax.plot(         hard_closed[:, 0], hard_closed[:, 1], color=\"#000000\", lw=2.0, label=\"hard_mjx\"     )     ax.scatter(hard_xy[:, 0], hard_xy[:, 1], color=\"#000000\", s=60, zorder=3)      cmap = plt.get_cmap(\"tab10\")     for i, (w, soft_pts) in enumerate(sel.soft_runs):         soft_xy = order_quad(np.array(soft_pts)[:, :2])         soft_closed = np.vstack([soft_xy, soft_xy[0]])         color = cmap(i % cmap.N)         ax.plot(             soft_closed[:, 0],             soft_closed[:, 1],             color=color,             lw=2.0,             ls=\"--\",             label=f\"soft w={w}\",         )         ax.scatter(soft_xy[:, 0], soft_xy[:, 1], color=color, s=55, zorder=3)      ax.set_aspect(\"equal\", adjustable=\"datalim\")     ax.set_title(f\"Hard vs soft manifold points (mode={mode})\")     ax.legend(loc=\"upper right\")     ax.grid(True, alpha=0.2)     plt.show()  <p>We first check, that in <code>hard</code> mode our softjax version matches the original mjx selection.</p> In\u00a0[5]: Copied! <pre>sel = run_selection(softness=(0.0,), mode=\"hard\", seed=0)\nplot_multi(sel, mode=\"hard\")\n</pre> sel = run_selection(softness=(0.0,), mode=\"hard\", seed=0) plot_multi(sel, mode=\"hard\") <p>Next, we visualize the relaxations for two different modes.</p> In\u00a0[6]: Copied! <pre>sel = run_selection(softness=[0.5, 1.0, 1.5, 2.0], mode=\"entropic\", seed=0)\nplot_multi(sel, mode=\"entropic\")\n</pre> sel = run_selection(softness=[0.5, 1.0, 1.5, 2.0], mode=\"entropic\", seed=0) plot_multi(sel, mode=\"entropic\") In\u00a0[7]: Copied! <pre>from softjax.straight_through import st\n\nmanifold_points_softjax_st = st(manifold_points_softjax)\n</pre> from softjax.straight_through import st  manifold_points_softjax_st = st(manifold_points_softjax) <p>The <code>manifold_points_softjax_st</code> now still has the discrete one-hot outputs of the hard function, but provides the gradients of the relaxed version.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"manifold_points/#manifold-points","title":"Manifold Points\u00b6","text":"<p>This notebook provides an example of how to translate a hard jax function into a softjax function in practice.</p> <p>As the worked example, we consider Mujoco MJX's convex collision detection algorithm, which has a subroutine that chooses four vertices (A, B, C, D) that roughly maximize the contact patch area.</p>"},{"location":"manifold_points/#original-function","title":"Original function\u00b6","text":"<p>The steps of the selection algorithm are</p> <ol> <li>Pick A: any unmasked vertex (in MJX this is the most penetrating one).</li> <li>Pick B: the vertex farthest from A (long base edge).</li> <li>Pick C: the vertex farthest from line AB within the plane (opens the area).</li> <li>Pick D: the vertex farthest from both edges AC and BC.</li> </ol>"},{"location":"manifold_points/#softjax-version","title":"SoftJax version\u00b6","text":"<p>To make this algorithm smoothly differentiable we convert it to softjax. For this, we:</p> <ul> <li>replace discrete <code>argmax</code> with <code>sj.argmax</code>, which returns a <code>SoftIndex</code> distribution,</li> <li>replace the hard indexing with <code>sj.dynamic_index_in_dim</code>, which uses the <code>SoftIndex</code> as input,</li> <li>replace <code>abs</code> with <code>sj.abs</code>.</li> </ul> <p>Note that we now return four <code>SoftIndex</code> distributions (shape: 4 x n) instead of the hard indices. The remaining code of the collision detection therefore would need to be adjusted accordingly.</p>"},{"location":"manifold_points/#visualization","title":"Visualization\u00b6","text":"<p>We generate a perturbed planar polygon for visualization.</p>"},{"location":"manifold_points/#straight-through-estimation","title":"Straight-through estimation\u00b6","text":"<p>We could now directly use one of the relaxations as a differentiable proxy. However, sometimes it is desired to not alter the forward pass, e.g. in simulation we do not want to relaxt the forward physics. In these cases, we can resort to the straight-through trick, which means to replace only the gradient of a hard function on forward with the gradeint of a relaxed/soft function on backward pass. <code>softjax.straight_through.st</code> implements this behavior by wrapping a function so that the forward pass uses a hard definition while the backward pass uses a soft surrogate. For convenience, we all ready provide wrapped versions of all our primitives, allowing the user to just use e.g. <code>sj.argmax_st</code> instead of <code>sj.argmax</code>.</p> <p>However, as described in the all-of-softjax notebook, this can still lead to uninformative gradients due to the interaction of the straight-through trick with the chain rule. A way to avoid this issue is to apply the straight-through trick on the outer level of the downstream function, which calls the whole function twice instead of each of the primitives. This is illustrated below.</p>"},{"location":"plots/","title":"Plots","text":"In\u00a0[3]: Copied! <pre>sigmoid_modes = [\"entropic\", \"euclidean\", \"pseudohuber\", \"cubic\", \"quintic\"]\n</pre> sigmoid_modes = [\"entropic\", \"euclidean\", \"pseudohuber\", \"cubic\", \"quintic\"] In\u00a0[4]: Copied! <pre>def median_newton(x, **kwargs):\n    return sj.median_newton(jnp.array([x, -0.5, 0.5]), **kwargs)\n\n\ndef greater_than_1(x, **kwargs):\n    return sj.greater(x, jnp.array(1.0), **kwargs)\n\n\ndef less_than_1(x, **kwargs):\n    return sj.less(x, jnp.array(1.0), **kwargs)\n\n\ndef equal_to_0(x, **kwargs):\n    return sj.equal(x, jnp.array(0.0), **kwargs)\n\n\ndef not_equal_to_0(x, **kwargs):\n    return sj.not_equal(x, jnp.array(0.0), **kwargs)\n\n\ndef isclose_to_0(x, **kwargs):\n    return sj.not_equal(x, jnp.array(0.0), **kwargs)\n\n\nplot(sj.heaviside, modes=sigmoid_modes)\nplot(sj.abs, modes=sigmoid_modes)\nplot(sj.sign, modes=sigmoid_modes)\nplot(sj.round, modes=sigmoid_modes)\nplot(median_newton, modes=sigmoid_modes)\nplot(greater_than_1, modes=sigmoid_modes)\nplot(less_than_1, modes=sigmoid_modes)\nplot(equal_to_0, modes=sigmoid_modes)\nplot(not_equal_to_0, modes=sigmoid_modes)\nplot(isclose_to_0, modes=sigmoid_modes)\n</pre> def median_newton(x, **kwargs):     return sj.median_newton(jnp.array([x, -0.5, 0.5]), **kwargs)   def greater_than_1(x, **kwargs):     return sj.greater(x, jnp.array(1.0), **kwargs)   def less_than_1(x, **kwargs):     return sj.less(x, jnp.array(1.0), **kwargs)   def equal_to_0(x, **kwargs):     return sj.equal(x, jnp.array(0.0), **kwargs)   def not_equal_to_0(x, **kwargs):     return sj.not_equal(x, jnp.array(0.0), **kwargs)   def isclose_to_0(x, **kwargs):     return sj.not_equal(x, jnp.array(0.0), **kwargs)   plot(sj.heaviside, modes=sigmoid_modes) plot(sj.abs, modes=sigmoid_modes) plot(sj.sign, modes=sigmoid_modes) plot(sj.round, modes=sigmoid_modes) plot(median_newton, modes=sigmoid_modes) plot(greater_than_1, modes=sigmoid_modes) plot(less_than_1, modes=sigmoid_modes) plot(equal_to_0, modes=sigmoid_modes) plot(not_equal_to_0, modes=sigmoid_modes) plot(isclose_to_0, modes=sigmoid_modes) In\u00a0[5]: Copied! <pre>softplus_modes = [\"entropic\", \"euclidean\", \"quartic\"]\nsoftplus_modes_gated = [\n    \"gated_entropic\",\n    \"gated_euclidean\",\n    \"gated_cubic\",\n    \"gated_quintic\",\n    \"gated_pseudohuber\",\n]\n</pre> softplus_modes = [\"entropic\", \"euclidean\", \"quartic\"] softplus_modes_gated = [     \"gated_entropic\",     \"gated_euclidean\",     \"gated_cubic\",     \"gated_quintic\",     \"gated_pseudohuber\", ] In\u00a0[6]: Copied! <pre>def clip_between_0_and_1(x, **kwargs):\n    return sj.clip(x, jnp.array(0.0), jnp.array(1.0), **kwargs)\n\n\nplot(sj.relu, modes=softplus_modes)\nplot(sj.relu, modes=softplus_modes_gated)\nplot(clip_between_0_and_1, modes=softplus_modes)\nplot(clip_between_0_and_1, modes=softplus_modes_gated)\n</pre> def clip_between_0_and_1(x, **kwargs):     return sj.clip(x, jnp.array(0.0), jnp.array(1.0), **kwargs)   plot(sj.relu, modes=softplus_modes) plot(sj.relu, modes=softplus_modes_gated) plot(clip_between_0_and_1, modes=softplus_modes) plot(clip_between_0_and_1, modes=softplus_modes_gated) In\u00a0[7]: Copied! <pre>projection_modes = [\"entropic\", \"euclidean\"]\n</pre> projection_modes = [\"entropic\", \"euclidean\"] In\u00a0[8]: Copied! <pre>def sj_max(x, **kwargs):\n    return sj.max(jnp.array([x, 0.5]), **kwargs)\n\n\ndef sj_min(x, **kwargs):\n    return sj.min(jnp.array([x, 0.5]), **kwargs)\n\n\ndef sj_median(x, **kwargs):\n    return sj.median(jnp.array([x, -1.0, 1.0, -2.0, 2.0]), **kwargs)\n\n\ndef sj_sort(x, **kwargs):\n    return sj.sort(jnp.array([x, -1.0, 1.0]), **kwargs)[1]\n\n\ndef sj_top_k(x, **kwargs):\n    return sj.top_k(jnp.array([x, -1.0, 1.0]), k=2, **kwargs)[0][1]\n\n\nplot(sj_max, modes=projection_modes)\nplot(sj_min, modes=projection_modes)\nplot(sj_median, modes=projection_modes)\nplot(sj_median, modes=projection_modes, fast=False)\nplot(sj_sort, modes=projection_modes)\nplot(sj_sort, modes=projection_modes, fast=False)\nplot(sj_top_k, modes=projection_modes)\nplot(sj_top_k, modes=projection_modes, fast=False)\n</pre> def sj_max(x, **kwargs):     return sj.max(jnp.array([x, 0.5]), **kwargs)   def sj_min(x, **kwargs):     return sj.min(jnp.array([x, 0.5]), **kwargs)   def sj_median(x, **kwargs):     return sj.median(jnp.array([x, -1.0, 1.0, -2.0, 2.0]), **kwargs)   def sj_sort(x, **kwargs):     return sj.sort(jnp.array([x, -1.0, 1.0]), **kwargs)[1]   def sj_top_k(x, **kwargs):     return sj.top_k(jnp.array([x, -1.0, 1.0]), k=2, **kwargs)[0][1]   plot(sj_max, modes=projection_modes) plot(sj_min, modes=projection_modes) plot(sj_median, modes=projection_modes) plot(sj_median, modes=projection_modes, fast=False) plot(sj_sort, modes=projection_modes) plot(sj_sort, modes=projection_modes, fast=False) plot(sj_top_k, modes=projection_modes) plot(sj_top_k, modes=projection_modes, fast=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"plots/#plots","title":"Plots\u00b6","text":"<p>Below we plot many of the Softjax functions for different modes and softnesses to give an idea of how they look like. All functions are designed to be soft when used on inputs in an interval [0,1] when setting default softness of 1.0. When inputs in other ranges are used, softness can be scaled accrdingly, e.g. on inputs distributed in the inteval [0, 100] a softness of 100.0 should result in a soft behavior.</p>"},{"location":"plots/#sigmoid-based","title":"Sigmoid-based\u00b6","text":""},{"location":"plots/#softplus-based","title":"Softplus-based\u00b6","text":""},{"location":"plots/#simplex-projection-optimal-transport-based","title":"Simplex-projection / optimal transport based\u00b6","text":""},{"location":"quick-example/","title":"Quick Example","text":"In\u00a0[1]: Copied! <pre>import jax\nimport jax.numpy as jnp\nimport softjax as sj\n\n\njax.config.update(\"jax_enable_x64\", True)\njax.config.update(\"jax_default_matmul_precision\", \"high\")\njax.config.update(\"jax_platforms\", \"cpu\")\n</pre> import jax import jax.numpy as jnp import softjax as sj   jax.config.update(\"jax_enable_x64\", True) jax.config.update(\"jax_default_matmul_precision\", \"high\") jax.config.update(\"jax_platforms\", \"cpu\") <p>The default mode of all our softjax functions is <code>mode=entropic</code>. This uses some well-known exponential-function-based relaxations, e.g. an exponential sigmoid as a soft heaviside function and the <code>softplus</code> function as a soft ReLU function.</p> <p>Note that all Softjax functions taking a <code>mode</code> argument also take a <code>softness</code> argument, which determines how soft the relaxation is. Note that while the limit of softness going to zero should always recover the hard function, setting <code>softness=0</code> is not allowed. Instead, use the <code>mode=hard</code> setting.</p> In\u00a0[2]: Copied! <pre>x = jnp.array([-0.2, -1.0, 0.3, 1.0])\ny = jnp.array([0.2, -0.5, 0.5, -1.0])\n\n# Elementwise functions\nprint(\"Hard ReLU:\", jax.nn.relu(x))\nprint(\"Soft ReLU:\", sj.relu(x))\nprint(\"Hard Clip:\", jnp.clip(x, -0.5, 0.5))\nprint(\"Soft Clip:\", sj.clip(x, -0.5, 0.5))\nprint(\"Hard Absolute:\", jnp.abs(x))\nprint(\"Soft Absolute:\", sj.abs(x))\nprint(\"Hard Sign:\", jnp.sign(x))\nprint(\"Soft Sign:\", sj.sign(x))\nprint(\"Hard round:\", jnp.round(x))\nprint(\"Soft round:\", sj.round(x))\nprint(\"Hard heaviside:\", jnp.heaviside(x, 0.5))\nprint(\"Soft heaviside:\", sj.heaviside(x))\n</pre> x = jnp.array([-0.2, -1.0, 0.3, 1.0]) y = jnp.array([0.2, -0.5, 0.5, -1.0])  # Elementwise functions print(\"Hard ReLU:\", jax.nn.relu(x)) print(\"Soft ReLU:\", sj.relu(x)) print(\"Hard Clip:\", jnp.clip(x, -0.5, 0.5)) print(\"Soft Clip:\", sj.clip(x, -0.5, 0.5)) print(\"Hard Absolute:\", jnp.abs(x)) print(\"Soft Absolute:\", sj.abs(x)) print(\"Hard Sign:\", jnp.sign(x)) print(\"Soft Sign:\", sj.sign(x)) print(\"Hard round:\", jnp.round(x)) print(\"Soft round:\", sj.round(x)) print(\"Hard heaviside:\", jnp.heaviside(x, 0.5)) print(\"Soft heaviside:\", sj.heaviside(x)) <pre>Hard ReLU: [0.  0.  0.3 1. ]\nSoft ReLU: [1.26928011e-02 4.53988992e-06 3.04858735e-01 1.00000454e+00]\nHard Clip: [-0.2 -0.5  0.3  0.5]\nSoft Clip: [-0.19523241 -0.4993285   0.28734074  0.4993285 ]\nHard Absolute: [0.2 1.  0.3 1. ]\n</pre> <pre>Soft Absolute: [0.15231883 0.9999092  0.27154448 0.9999092 ]\nHard Sign: [-1. -1.  1.  1.]\nSoft Sign: [-0.76159416 -0.9999092   0.90514825  0.9999092 ]\nHard round: [-0. -1.  0.  1.]\n</pre> <pre>Soft round: [-0.04651704 -1.          0.1188737   1.        ]\nHard heaviside: [0. 0. 1. 1.]\nSoft heaviside: [1.19202922e-01 4.53978687e-05 9.52574127e-01 9.99954602e-01]\n</pre> In\u00a0[3]: Copied! <pre># Functions on arrays\nprint(\"Hard max:\", jnp.max(x))\nprint(\"Soft max:\", sj.max(x))\nprint(\"Hard min:\", jnp.min(x))\nprint(\"Soft min:\", sj.min(x))\nprint(\"Hard median:\", jnp.median(x))\nprint(\"Soft median:\", sj.median(x))\nprint(\"Hard top_k:\", jax.lax.top_k(x, k=3)[0])\nprint(\"Soft top_k:\", sj.top_k(x, k=3)[0])\nprint(\"Hard sort:\", jnp.sort(x))\nprint(\"Soft sort:\", sj.sort(x))\nprint(\"Hard ranking:\", jnp.argsort(jnp.argsort(x)))\nprint(\"Soft ranking:\", sj.ranking(x, descending=False))\n</pre> # Functions on arrays print(\"Hard max:\", jnp.max(x)) print(\"Soft max:\", sj.max(x)) print(\"Hard min:\", jnp.min(x)) print(\"Soft min:\", sj.min(x)) print(\"Hard median:\", jnp.median(x)) print(\"Soft median:\", sj.median(x)) print(\"Hard top_k:\", jax.lax.top_k(x, k=3)[0]) print(\"Soft top_k:\", sj.top_k(x, k=3)[0]) print(\"Hard sort:\", jnp.sort(x)) print(\"Soft sort:\", sj.sort(x)) print(\"Hard ranking:\", jnp.argsort(jnp.argsort(x))) print(\"Soft ranking:\", sj.ranking(x, descending=False)) <pre>Hard max: 1.0\n</pre> <pre>Soft max: 0.9993548976691374\nHard min: -1.0\n</pre> <pre>Soft min: -0.9997287789452775\nHard median: 0.04999999999999999\n</pre> <pre>Soft median: 0.05000033589501627\nHard top_k: [ 1.   0.3 -0.2]\n</pre> <pre>Soft top_k: [ 0.9993549   0.29728716 -0.19691387]\nHard sort: [-1.  -0.2  0.3  1. ]\nSoft sort: [-0.99972878 -0.19691387  0.29728716  0.9993549 ]\nHard ranking: [1 0 2 3]\n</pre> <pre>Soft ranking: [1.00636968e+00 3.39874686e-04 1.99421369e+00 2.99907667e+00]\n</pre> In\u00a0[4]: Copied! <pre># Functions returning indices\nprint(\"Hard argmax:\", jnp.argmax(x))\nprint(\"Soft argmax:\", sj.argmax(x))\nprint(\"Hard argmin:\", jnp.argmin(x))\nprint(\"Soft argmin:\", sj.argmin(x))\nprint(\"Hard argmedian:\", \"Not implemented in standard JAX\")\nprint(\"Soft argmedian:\", sj.argmedian(x))\nprint(\"Hard argtop_k:\", jax.lax.top_k(x, k=3)[1])\nprint(\"Soft argtop_k:\", sj.top_k(x, k=3)[1])\nprint(\"Hard argsort:\", jnp.argsort(x))\nprint(\"Soft argsort:\", sj.argsort(x))\n</pre> # Functions returning indices print(\"Hard argmax:\", jnp.argmax(x)) print(\"Soft argmax:\", sj.argmax(x)) print(\"Hard argmin:\", jnp.argmin(x)) print(\"Soft argmin:\", sj.argmin(x)) print(\"Hard argmedian:\", \"Not implemented in standard JAX\") print(\"Soft argmedian:\", sj.argmedian(x)) print(\"Hard argtop_k:\", jax.lax.top_k(x, k=3)[1]) print(\"Soft argtop_k:\", sj.top_k(x, k=3)[1]) print(\"Hard argsort:\", jnp.argsort(x)) print(\"Soft argsort:\", sj.argsort(x)) <pre>Hard argmax: 3\nSoft argmax: [6.13857697e-06 2.05926316e-09 9.11045600e-04 9.99082814e-01]\nHard argmin: 1\nSoft argmin: [3.35349372e-04 9.99662389e-01 2.25956629e-06 2.06045775e-09]\nHard argmedian: Not implemented in standard JAX\nSoft argmedian: [4.99999764e-01 5.62675608e-08 4.99999764e-01 4.15764163e-07]\nHard argtop_k: [3 2 0]\nSoft argtop_k: [[6.13857697e-06 2.05926316e-09 9.11045600e-04 9.99082814e-01]\n [6.68677917e-03 2.24316451e-06 9.92406021e-01 9.04957153e-04]\n [9.92970214e-01 3.33104397e-04 6.69058067e-03 6.10101985e-06]]\nHard argsort: [1 0 2 3]\nSoft argsort: [[3.35349372e-04 9.99662389e-01 2.25956629e-06 2.06045775e-09]\n [9.92970214e-01 3.33104397e-04 6.69058067e-03 6.10101985e-06]\n [6.68677917e-03 2.24316451e-06 9.92406021e-01 9.04957153e-04]\n [6.13857697e-06 2.05926316e-09 9.11045600e-04 9.99082814e-01]]\n</pre> In\u00a0[5]: Copied! <pre>## SoftBool generation\nprint(\"Hard greater:\", x &gt; y)\nprint(\"Soft greater:\", sj.greater(x, y))\nprint(\"Hard greater equal:\", x &gt;= y)\nprint(\"Soft greater equal:\", sj.greater_equal(x, y))\nprint(\"Hard less:\", x &lt; y)\nprint(\"Soft less:\", sj.less(x, y))\nprint(\"Hard less equal:\", x &lt;= y)\nprint(\"Soft less equal:\", sj.less_equal(x, y))\nprint(\"Hard equal:\", x == y)\nprint(\"Soft equal:\", sj.equal(x, y))\nprint(\"Hard not equal:\", x != y)\nprint(\"Soft not equal:\", sj.not_equal(x, y))\nprint(\"Hard isclose:\", jnp.isclose(x, y))\nprint(\"Soft isclose:\", sj.isclose(x, y))\n</pre> ## SoftBool generation print(\"Hard greater:\", x &gt; y) print(\"Soft greater:\", sj.greater(x, y)) print(\"Hard greater equal:\", x &gt;= y) print(\"Soft greater equal:\", sj.greater_equal(x, y)) print(\"Hard less:\", x &lt; y) print(\"Soft less:\", sj.less(x, y)) print(\"Hard less equal:\", x &lt;= y) print(\"Soft less equal:\", sj.less_equal(x, y)) print(\"Hard equal:\", x == y) print(\"Soft equal:\", sj.equal(x, y)) print(\"Hard not equal:\", x != y) print(\"Soft not equal:\", sj.not_equal(x, y)) print(\"Hard isclose:\", jnp.isclose(x, y)) print(\"Soft isclose:\", sj.isclose(x, y)) <pre>Hard greater: [False False False  True]\nSoft greater: [0.01798621 0.00669285 0.11920292 1.        ]\nHard greater equal: [False False False  True]\nSoft greater equal: [0.01798621 0.00669285 0.11920292 1.        ]\n</pre> <pre>Hard less: [ True  True  True False]\nSoft less: [9.82013790e-01 9.93307149e-01 8.80797078e-01 2.06115369e-09]\n</pre> <pre>Hard less equal: [ True  True  True False]\nSoft less equal: [9.82013790e-01 9.93307149e-01 8.80797078e-01 2.06115369e-09]\nHard equal: [False False False False]\n</pre> <pre>Soft equal: [1.79862100e-02 6.69285093e-03 1.19202922e-01 2.06115369e-09]\nHard not equal: [ True  True  True  True]\nSoft not equal: [0.98201379 0.99330715 0.88079708 1.        ]\nHard isclose: [False False False False]\n</pre> <pre>Soft isclose: [1.79865650e-02 6.69318401e-03 1.19208182e-01 2.06135997e-09]\n</pre> In\u00a0[6]: Copied! <pre>## SoftBool manipulation\nfuzzy_a = jnp.array([0.1, 0.2, 0.8, 1.0])\nfuzzy_b = jnp.array([0.7, 0.3, 0.1, 0.9])\nprint(\"Soft AND:\", sj.logical_and(fuzzy_a, fuzzy_b))\nprint(\"Soft OR:\", sj.logical_or(fuzzy_a, fuzzy_b))\nprint(\"Soft NOT:\", sj.logical_not(fuzzy_a))\nprint(\"Soft XOR:\", sj.logical_xor(fuzzy_a, fuzzy_b))\nprint(\"Soft ALL:\", sj.all(fuzzy_a))\nprint(\"Soft ANY:\", sj.any(fuzzy_a))\n\n## SoftBool selection\nprint(\"Where:\", sj.where(fuzzy_a, x, y))\n</pre> ## SoftBool manipulation fuzzy_a = jnp.array([0.1, 0.2, 0.8, 1.0]) fuzzy_b = jnp.array([0.7, 0.3, 0.1, 0.9]) print(\"Soft AND:\", sj.logical_and(fuzzy_a, fuzzy_b)) print(\"Soft OR:\", sj.logical_or(fuzzy_a, fuzzy_b)) print(\"Soft NOT:\", sj.logical_not(fuzzy_a)) print(\"Soft XOR:\", sj.logical_xor(fuzzy_a, fuzzy_b)) print(\"Soft ALL:\", sj.all(fuzzy_a)) print(\"Soft ANY:\", sj.any(fuzzy_a))  ## SoftBool selection print(\"Where:\", sj.where(fuzzy_a, x, y)) <pre>Soft AND: [0.26457513 0.24494897 0.28284271 0.9486833 ]\nSoft OR: [0.48038476 0.25166852 0.57573593 0.99999684]\nSoft NOT: [0.9 0.8 0.2 0. ]\nSoft XOR: [0.58702688 0.43498731 0.63937484 0.17309871]\n</pre> <pre>Soft ALL: 0.35565588200778464\nSoft ANY: 0.9980519925071494\nWhere: [ 0.16 -0.6   0.34  1.  ]\n</pre> In\u00a0[7]: Copied! <pre># Straight-through estimation: Use hard function on forward and soft on backward\nprint(\"Straight-through ReLU:\", sj.relu_st(x))\nprint(\"Straight-through sort:\", sj.sort_st(x))\nprint(\"Straight-through argtop_k:\", sj.top_k_st(x, k=3)[1])\nprint(\"Straight-through greater:\", sj.greater_st(x, y))\n</pre> # Straight-through estimation: Use hard function on forward and soft on backward print(\"Straight-through ReLU:\", sj.relu_st(x)) print(\"Straight-through sort:\", sj.sort_st(x)) print(\"Straight-through argtop_k:\", sj.top_k_st(x, k=3)[1]) print(\"Straight-through greater:\", sj.greater_st(x, y)) <pre>Straight-through ReLU: [0.  0.  0.3 1. ]\n</pre> <pre>Straight-through sort: [-1.  -0.2  0.3  1. ]\n</pre> <pre>Straight-through argtop_k: [[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]]\n</pre> <pre>Straight-through greater: [0. 0. 0. 1.]\n</pre> <p>We can replicate the original JAX function outputs (plus some one-hot generation for index functions) by setting <code>mode=hard</code>. This is especially useful for straight-through estimators, where we want to easily switch between hard and soft modes.</p> In\u00a0[8]: Copied! <pre>x = jnp.array([-0.2, -1.0, 0.3, 1.0])\ny = jnp.array([0.2, -0.5, 0.5, -1.0])\n\n# Elementwise functions\nprint(\"Jax ReLU:    \", jax.nn.relu(x))\nprint(\"Softjax ReLU:\", sj.relu(x, mode=\"hard\"))\nprint(\"Jax Clip:    \", jnp.clip(x, -0.5, 0.5))\nprint(\n    \"Softjax Clip:\",\n    sj.clip(\n        x,\n        -0.5,\n        0.5,\n        mode=\"hard\",\n    ),\n)\nprint(\"Jax Absolute:    \", jnp.abs(x))\nprint(\"Softjax Absolute:\", sj.abs(x, mode=\"hard\"))\nprint(\"Jax Sign:    \", jnp.sign(x))\nprint(\"Softjax Sign:\", sj.sign(x, mode=\"hard\"))\nprint(\"Jax round:    \", jnp.round(x))\nprint(\"Softjax round:\", sj.round(x, mode=\"hard\"))\n</pre> x = jnp.array([-0.2, -1.0, 0.3, 1.0]) y = jnp.array([0.2, -0.5, 0.5, -1.0])  # Elementwise functions print(\"Jax ReLU:    \", jax.nn.relu(x)) print(\"Softjax ReLU:\", sj.relu(x, mode=\"hard\")) print(\"Jax Clip:    \", jnp.clip(x, -0.5, 0.5)) print(     \"Softjax Clip:\",     sj.clip(         x,         -0.5,         0.5,         mode=\"hard\",     ), ) print(\"Jax Absolute:    \", jnp.abs(x)) print(\"Softjax Absolute:\", sj.abs(x, mode=\"hard\")) print(\"Jax Sign:    \", jnp.sign(x)) print(\"Softjax Sign:\", sj.sign(x, mode=\"hard\")) print(\"Jax round:    \", jnp.round(x)) print(\"Softjax round:\", sj.round(x, mode=\"hard\")) <pre>Jax ReLU:     [0.  0.  0.3 1. ]\nSoftjax ReLU: [0.  0.  0.3 1. ]\nJax Clip:     [-0.2 -0.5  0.3  0.5]\nSoftjax Clip: [-0.2 -0.5  0.3  0.5]\nJax Absolute:     [0.2 1.  0.3 1. ]\nSoftjax Absolute: [0.2 1.  0.3 1. ]\nJax Sign:     [-1. -1.  1.  1.]\nSoftjax Sign: [-1. -1.  1.  1.]\nJax round:     [-0. -1.  0.  1.]\nSoftjax round: [-0. -1.  0.  1.]\n</pre> In\u00a0[9]: Copied! <pre># Functions on arrays\nprint(\"Jax max:    \", jnp.max(x))\nprint(\"Softjax max:\", sj.max(x, mode=\"hard\"))\nprint(\"Jax min:    \", jnp.min(x))\nprint(\"Softjax min:\", sj.min(x, mode=\"hard\"))\nprint(\"Jax median:    \", jnp.median(x))\nprint(\"Softjax median:\", sj.median(x, mode=\"hard\"))\nprint(\"Jax top_k:    \", jax.lax.top_k(x, k=3)[0])\nprint(\n    \"Softjax top_k:\",\n    sj.top_k(\n        x,\n        k=3,\n        mode=\"hard\",\n    )[0],\n)\nprint(\"Jax sort:    \", jnp.sort(x))\nprint(\"Softjax sort:\", sj.sort(x, mode=\"hard\"))\nprint(\"Jax ranking:    \", jnp.argsort(jnp.argsort(x)))\nprint(\"Softjax ranking:\", sj.ranking(x, mode=\"hard\", descending=False))\n</pre> # Functions on arrays print(\"Jax max:    \", jnp.max(x)) print(\"Softjax max:\", sj.max(x, mode=\"hard\")) print(\"Jax min:    \", jnp.min(x)) print(\"Softjax min:\", sj.min(x, mode=\"hard\")) print(\"Jax median:    \", jnp.median(x)) print(\"Softjax median:\", sj.median(x, mode=\"hard\")) print(\"Jax top_k:    \", jax.lax.top_k(x, k=3)[0]) print(     \"Softjax top_k:\",     sj.top_k(         x,         k=3,         mode=\"hard\",     )[0], ) print(\"Jax sort:    \", jnp.sort(x)) print(\"Softjax sort:\", sj.sort(x, mode=\"hard\")) print(\"Jax ranking:    \", jnp.argsort(jnp.argsort(x))) print(\"Softjax ranking:\", sj.ranking(x, mode=\"hard\", descending=False)) <pre>Jax max:     1.0\nSoftjax max: 1.0\nJax min:     -1.0\nSoftjax min: -1.0\nJax median:     0.04999999999999999\nSoftjax median: 0.04999999999999999\nJax top_k:     [ 1.   0.3 -0.2]\nSoftjax top_k: [ 1.   0.3 -0.2]\nJax sort:     [-1.  -0.2  0.3  1. ]\nSoftjax sort: [-1.  -0.2  0.3  1. ]\nJax ranking:     [1 0 2 3]\nSoftjax ranking: [1. 0. 2. 3.]\n</pre> In\u00a0[10]: Copied! <pre># Functions returning indices\nprint(\"Jax argmax:    \", jnp.argmax(x))\nprint(\"Softjax argmax:\", sj.argmax(x, mode=\"hard\"))\nprint(\"Jax argmin:    \", jnp.argmin(x))\nprint(\"Softjax argmin:\", sj.argmin(x, mode=\"hard\"))\nprint(\"Jax argmedian:    \", \"Not implemented in standard JAX\")\nprint(\"Softjax argmedian:\", sj.argmedian(x, mode=\"hard\"))\nprint(\"Jax argtop_k:    \", jax.lax.top_k(x, k=3)[1])\nprint(\"Softjax argtop_k:\", sj.top_k(x, k=3, mode=\"hard\")[1])\nprint(\"Jax argsort:    \", jnp.argsort(x))\nprint(\"Softjax argsort:\", sj.argsort(x, mode=\"hard\"))\n</pre> # Functions returning indices print(\"Jax argmax:    \", jnp.argmax(x)) print(\"Softjax argmax:\", sj.argmax(x, mode=\"hard\")) print(\"Jax argmin:    \", jnp.argmin(x)) print(\"Softjax argmin:\", sj.argmin(x, mode=\"hard\")) print(\"Jax argmedian:    \", \"Not implemented in standard JAX\") print(\"Softjax argmedian:\", sj.argmedian(x, mode=\"hard\")) print(\"Jax argtop_k:    \", jax.lax.top_k(x, k=3)[1]) print(\"Softjax argtop_k:\", sj.top_k(x, k=3, mode=\"hard\")[1]) print(\"Jax argsort:    \", jnp.argsort(x)) print(\"Softjax argsort:\", sj.argsort(x, mode=\"hard\")) <pre>Jax argmax:     3\n</pre> <pre>Softjax argmax: [0. 0. 0. 1.]\nJax argmin:     1\nSoftjax argmin: [0. 1. 0. 0.]\nJax argmedian:     Not implemented in standard JAX\nSoftjax argmedian: [0.5 0.  0.5 0. ]\nJax argtop_k:     [3 2 0]\nSoftjax argtop_k: [[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]]\nJax argsort:     [1 0 2 3]\nSoftjax argsort: [[0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]]\n</pre> In\u00a0[11]: Copied! <pre>## SoftBool generation\nprint(\"Jax heaviside:    \", jnp.heaviside(x, 0.5))\nprint(\"Softjax heaviside:\", sj.heaviside(x, mode=\"hard\"))\nprint(\"Jax greater:    \", x &gt; y)\nprint(\"Softjax greater:\", sj.greater(x, y, mode=\"hard\"))\nprint(\"Jax greater equal:    \", x &gt;= y)\nprint(\"Softjax greater equal:\", sj.greater_equal(x, y, mode=\"hard\"))\nprint(\"Jax less:    \", x &lt; y)\nprint(\"Softjax less:\", sj.less(x, y, mode=\"hard\"))\nprint(\"Jax less equal:    \", x &lt;= y)\nprint(\"Softjax less equal:\", sj.less_equal(x, y, mode=\"hard\"))\nprint(\"Jax equal:    \", x == y)\nprint(\"Softjax equal:\", sj.equal(x, y, mode=\"hard\"))\nprint(\"Jax not equal:    \", x != y)\nprint(\"Softjax not equal:\", sj.not_equal(x, y, mode=\"hard\"))\nprint(\"Jax isclose:    \", jnp.isclose(x, y))\nprint(\"Softjax isclose:\", sj.isclose(x, y, mode=\"hard\"))\n</pre> ## SoftBool generation print(\"Jax heaviside:    \", jnp.heaviside(x, 0.5)) print(\"Softjax heaviside:\", sj.heaviside(x, mode=\"hard\")) print(\"Jax greater:    \", x &gt; y) print(\"Softjax greater:\", sj.greater(x, y, mode=\"hard\")) print(\"Jax greater equal:    \", x &gt;= y) print(\"Softjax greater equal:\", sj.greater_equal(x, y, mode=\"hard\")) print(\"Jax less:    \", x &lt; y) print(\"Softjax less:\", sj.less(x, y, mode=\"hard\")) print(\"Jax less equal:    \", x &lt;= y) print(\"Softjax less equal:\", sj.less_equal(x, y, mode=\"hard\")) print(\"Jax equal:    \", x == y) print(\"Softjax equal:\", sj.equal(x, y, mode=\"hard\")) print(\"Jax not equal:    \", x != y) print(\"Softjax not equal:\", sj.not_equal(x, y, mode=\"hard\")) print(\"Jax isclose:    \", jnp.isclose(x, y)) print(\"Softjax isclose:\", sj.isclose(x, y, mode=\"hard\")) <pre>Jax heaviside:     [0. 0. 1. 1.]\n</pre> <pre>Softjax heaviside: [0. 0. 1. 1.]\nJax greater:     [False False False  True]\nSoftjax greater: [0. 0. 0. 1.]\nJax greater equal:     [False False False  True]\nSoftjax greater equal: [0. 0. 0. 1.]\nJax less:     [ True  True  True False]\nSoftjax less: [1. 1. 1. 0.]\nJax less equal:     [ True  True  True False]\nSoftjax less equal: [1. 1. 1. 0.]\nJax equal:     [False False False False]\nSoftjax equal: [0. 0. 0. 0.]\nJax not equal:     [ True  True  True  True]\nSoftjax not equal: [1. 1. 1. 1.]\nJax isclose:     [False False False False]\nSoftjax isclose: [0. 0. 0. 0.]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"quick-example/#quick-example","title":"Quick Example\u00b6","text":"<p>This notebook contains the quick examples from the Readme.</p>"},{"location":"quick-example/#default-entropic-mode-of-softjax","title":"Default \"entropic\" mode of Softjax\u00b6","text":""},{"location":"quick-example/#hard-mode-of-softjax","title":"\"Hard\" mode of Softjax\u00b6","text":""},{"location":"_static/Readme/","title":"Readme","text":"<p>The favicon is adapted from <code>cosine-wave</code> from https://materialdesignicons.com, found by way of https://pictogrammers.com. Specifically it has been adapted by filling in the integral with black. (Originally it has 100% alpha.)</p>"},{"location":"api/soft_bools/","title":"SoftBools","text":""},{"location":"api/soft_bools/#generating-softbools","title":"Generating SoftBools","text":""},{"location":"api/soft_bools/#softjax.heaviside","title":"<code>softjax.heaviside(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic') -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.heaviside(x,0.5).</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact Heaviside step. Otherwise uses     \"entropic\", \"euclidean\", \"cubic\", or \"quintic\" relaxations. Defaults to \"entropic\".</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> (Array with values in [0, 1]), relaxing the elementwise Heaviside step function.</p>"},{"location":"api/soft_bools/#softjax.greater","title":"<code>softjax.greater(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &gt; y</code>. Uses a Heaviside relaxation so the output approaches 0 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, greater returns 0 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &gt; y</code>.</p>"},{"location":"api/soft_bools/#softjax.greater_equal","title":"<code>softjax.greater_equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &gt;= y</code>. Uses a Heaviside relaxation so the output approaches 1 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, greater_equal returns 1 at     equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &gt;= y</code>.</p>"},{"location":"api/soft_bools/#softjax.less","title":"<code>softjax.less(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &lt; y</code>. Uses a Heaviside relaxation so the output approaches 0 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, less returns 0 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &lt; y</code>.</p>"},{"location":"api/soft_bools/#softjax.less_equal","title":"<code>softjax.less_equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x &lt;= y</code>. Uses a Heaviside relaxation so the output approaches 1 at equality.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, less_equal returns 1 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x &lt;= y</code>.</p>"},{"location":"api/soft_bools/#softjax.equal","title":"<code>softjax.equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x == y</code>. Implemented as a soft <code>abs(x - y) &lt;= 0</code> comparison.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, equal returns 1 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x == y</code>.</p>"},{"location":"api/soft_bools/#softjax.not_equal","title":"<code>softjax.not_equal(x: jax.Array, y: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to elementwise <code>x != y</code>. Implemented as a soft <code>abs(x - y) &gt; 0</code> comparison.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, not_equal returns 0 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>x != y</code>.</p>"},{"location":"api/soft_bools/#softjax.isclose","title":"<code>softjax.isclose(x: jax.Array, y: jax.Array, softness: float = 1.0, rtol: float = 1e-05, atol: float = 1e-08, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes a soft approximation to <code>jnp.isclose</code> for elementwise comparison. Implemented as a soft <code>abs(x - y) &lt;= atol + rtol * abs(y)</code> comparison.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input Array.</li> <li><code>y</code>: Second input Array, same shape as <code>x</code>.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>rtol</code>: Relative tolerance. Defaults to 1e-5.</li> <li><code>atol</code>: Absolute tolerance. Defaults to 1e-8.</li> <li><code>mode</code>: If \"hard\", returns the exact comparison. Otherwise uses a soft     Heaviside: \"entropic\", \"euclidean\", \"cubic\" spline, or \"quintic\" spline.     Defaults to \"entropic\".</li> <li><code>epsilon</code>: Small offset so that as softness-&gt;0, isclose returns 1 at equality.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise <code>isclose(x, y)</code>.</p>"},{"location":"api/soft_bools/#manipulating-softbools","title":"Manipulating SoftBools","text":""},{"location":"api/soft_bools/#softjax.logical_and","title":"<code>softjax.logical_and(x: Float[Array, '...'], y: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical AND between two SoftBool Arrays. Fuzzy logic implemented as <code>all(stack([x, y], axis=-1), axis=-1)</code>.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First SoftBool input Array.</li> <li><code>y</code>: Second SoftBool input Array.</li> </ul> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise logical AND.</p>"},{"location":"api/soft_bools/#softjax.logical_not","title":"<code>softjax.logical_not(x: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical NOT of a SoftBool Array. Fuzzy logic implemented as <code>1.0 - x</code>.</p> <p>Arguments: - <code>x</code>: SoftBool input Array.</p> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> (Array with values in [0, 1]), relaxing the elementwise logical NOT.</p>"},{"location":"api/soft_bools/#softjax.logical_or","title":"<code>softjax.logical_or(x: Float[Array, '...'], y: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical OR between two SoftBool Arrays. Fuzzy logic implemented as <code>any(stack([x, y], axis=-1), axis=-1)</code>.</p> <p>Arguments: - <code>x</code>: First SoftBool input Array. - <code>y</code>: Second SoftBool input Array.</p> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise logical OR.</p>"},{"location":"api/soft_bools/#softjax.logical_xor","title":"<code>softjax.logical_xor(x: Float[Array, '...'], y: Float[Array, '...']) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical XOR between two SoftBool Arrays.</p> <p>Arguments: - <code>x</code>: First SoftBool input Array. - <code>y</code>: Second SoftBool input Array.</p> <p>Returns:</p> <p>SoftBool of same shape as <code>x</code> and <code>y</code> (Array with values in [0, 1]), relaxing the elementwise logical XOR.</p>"},{"location":"api/soft_bools/#softjax.all","title":"<code>softjax.all(x: Float[Array, '...'], axis: int = -1, epsilon: float = 1e-10) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical AND across a specified axis. Fuzzy logic implemented as the geometric mean along the axis.</p> <p>Arguments: - <code>x</code>: SoftBool input Array. - <code>axis</code>: Axis along which to compute the logical AND. Default is -1 (last axis). - <code>epsilon</code>: Minimum value for numerical stability inside the log.</p> <p>Returns:</p> <p>SoftBool (Array with values in [0, 1]) with the specified axis reduced, relaxing the logical ALL along that axis.</p>"},{"location":"api/soft_bools/#softjax.any","title":"<code>softjax.any(x: Float[Array, '...'], axis: int = -1) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes soft elementwise logical OR across a specified axis. Fuzzy logic implemented as <code>1.0 - all(logical_not(x), axis=axis)</code>.</p> <p>Arguments: - <code>x</code>: SoftBool input Array. - <code>axis</code>: Axis along which to compute the logical OR. Default is -1 (last axis).</p> <p>Returns:</p> <p>SoftBool (Array with values in [0, 1]) with the specified axis reduced, relaxing t he logical ANY along that axis.</p>"},{"location":"api/soft_bools/#selection-with-softbools","title":"Selection with SoftBools","text":""},{"location":"api/soft_bools/#softjax.where","title":"<code>softjax.where(condition: Float[Array, '...'], x: jax.Array, y: jax.Array) -&gt; jax.Array</code> <code></code>","text":"<p>Computes a soft elementwise selection between two Arrays based on a SoftBool condition. Fuzzy logic implemented as <code>x * condition + y * (1.0 - condition)</code>.</p> <p>Arguments: - <code>condition</code>: SoftBool condition Array, same shape as <code>x</code> and <code>y</code>. - <code>x</code>: First input Array, same shape as <code>condition</code>. - <code>y</code>: Second input Array, same shape as <code>condition</code>.</p> <p>Returns:</p> <p>Array of the same shape as <code>x</code> and <code>y</code>, interpolating between <code>x</code> and <code>y</code> according to <code>condition</code> in [0, 1].</p>"},{"location":"api/soft_functions/","title":"SoftFunctions","text":""},{"location":"api/soft_functions/#softjax.relu","title":"<code>softjax.relu(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'quartic', 'gated_entropic', 'gated_euclidean', 'gated_cubic', 'gated_quintic', 'gated_pseudohuber'] = 'entropic') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.nn.relu.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", applies <code>jax.nn.relu</code>. Otherwise uses \"entropic\", \"euclidean\",     \"quartic\", \"gated_entropic\", \"gated_euclidean\", \"gated_cubic\", \"gated_quintic\",     or \"gated_pseudohuber\" relaxations. Defaults to \"entropic\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise ReLU to <code>x</code>.</p>"},{"location":"api/soft_functions/#softjax.clip","title":"<code>softjax.clip(x: jax.Array, a: jax.Array, b: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'quartic', 'gated_entropic', 'gated_euclidean', 'gated_cubic', 'gated_quintic', 'gated_pseudohuber'] = 'entropic') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.clip.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>a</code>: Lower bound scalar.</li> <li><code>b</code>: Upper bound scalar.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", applies <code>jnp.clip</code>. Otherwise uses \"entropic\", \"euclidean\",     \"quartic\", \"gated_entropic\", \"gated_euclidean\", \"gated_cubic\", \"gated_quintic\",     or \"gated_pseudohuber\" relaxations. Defaults to \"entropic\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise clipping to <code>x</code>.</p>"},{"location":"api/soft_functions/#softjax.abs","title":"<code>softjax.abs(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.abs.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: Projection mode. \"hard\" returns the exact absolute value, otherwise uses     \"entropic\", \"pseudohuber\", \"euclidean\", \"cubic\", or \"quintic\" relaxations.     Defaults to \"entropic\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise absolute value to <code>x</code>.</p>"},{"location":"api/soft_functions/#softjax.sign","title":"<code>softjax.sign(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.sign.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", returns <code>jnp.sign</code>. Otherwise smooths via \"entropic\", \"euclidean\",     \"cubic\", or \"quintic\" relaxations. Defaults to \"entropic\".</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise sign to <code>x</code>.</p>"},{"location":"api/soft_functions/#softjax.round","title":"<code>softjax.round(x: jax.Array, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean', 'pseudohuber', 'cubic', 'quintic'] = 'entropic', neighbor_radius: int = 5) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.round.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of any shape.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: If \"hard\", applies <code>jnp.round</code>. Otherwise uses a sigmoid-based relaxation     based on the algorithm described in https://arxiv.org/pdf/2504.19026v1.     This function thereby inherits the different sigmoid modes \"entropic\",     \"euclidean\", \"pseudohuber\", \"cubic\", or \"quintic\".     Defaults to \"entropic\".</li> <li><code>neighbor_radius</code>: Number of neighbors on each side of the floor value to     consider for the soft rounding. Defaults to 5.</li> </ul> <p>Returns:</p> <p>Result of applying soft elementwise rounding to <code>x</code>.</p>"},{"location":"api/soft_functions/#softjax.median_newton","title":"<code>softjax.median_newton(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'pseudohuber', 'euclidean', 'cubic', 'quintic'] = 'entropic', max_iter: int = 8, eps: float = 1e-12) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.median of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: Axis along which to compute the median. If None, the input is flattened.     Defaults to None.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}. Defaults to     False.</li> <li><code>softness</code>: Softness of the score function, should be larger than zero. Defaults     to 1.0.</li> <li><code>mode</code>: Smooth score choice:<ul> <li><code>hard</code>: Returns <code>jnp.median</code>.</li> <li><code>sigmoid</code>, <code>pseudohuber</code>, <code>linear</code>, <code>cubic</code>, <code>quintic</code>: Smooth     relaxations for the M-estimator using Newton steps. Defaults to <code>sigmoid</code>.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of Newton iterations in the M-estimator.</li> <li><code>eps</code>: Small constant added to the derivative to avoid division by zero.</li> </ul> <p>Returns:</p> <p>Array of shape (..., {1}, ...) representing the soft median of <code>x</code> along the specified axis.</p>"},{"location":"api/soft_indices/","title":"SoftIndices","text":""},{"location":"api/soft_indices/#generating-softindices","title":"Generating SoftIndices","text":""},{"location":"api/soft_indices/#softjax.argmax","title":"<code>softjax.argmax(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic') -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.argmax of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the argmax. If None, the input Array is     flattened before computing the argmax. Defaults to None.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code>: Controls the type of softening:<ul> <li><code>hard</code>: Returns the result of jnp.argmax with a one-hot encoding of     the indices.</li> <li><code>entropic</code>: Returns a softmax-based relaxation of the argmax.</li> <li><code>euclidean</code>: Returns an L2-projection-based relaxation of the argmax.</li> </ul> </li> </ul> <p>Returns:</p> <p>A SoftIndex of shape (..., {1}, ..., [n]) (positive Array which sums to 1 over the last dimension). Represents the probability of an index corresponding to the argmax along the specified axis.</p> <p>Usage</p> <p>This function can be used as a differentiable relaxation to <code>softjax.argmax</code>, enabling backpropagation through index selection steps in neural networks or optimization routines. However, note that the output is not a discrete index but a <code>SoftIndex</code>, which is a distribution over indices. Therefore, functions which operate on indices have to be adjusted accordingly to accept a SoftIndex, see e.g. <code>softjax.max</code> for an example of using <code>softjax.take_along_axis</code> to retrieve the soft maximum value via the <code>SoftIndex</code>.</p> <p>Difference to jax.nn.softmax</p> <p>Note that <code>softjax.argmax</code> in <code>entropic</code> mode is not fully equivalent to jax.nn.softmax because it moves the probability dimension into the last axis (this is a convention in the <code>SoftIndex</code> data type).</p> Example <pre><code>x = jnp.array([[5, 3, 4], [2, 7, 6]])\n\n# Hard\nprint(\"jnp:\", jnp.argmax(x, axis=1))\nprint(\"sj_hard:\", sj.argmax(x, mode=\"hard\", axis=1))\n\n# Entropic (Softmax projection)\nprint(\"sj_entropic_low:\", sj.argmax(x, mode=\"entropic\", softness=0.01, axis=1))\nprint(\"sj_entropic_high:\", sj.argmax(x, mode=\"entropic\", softness=1.0, axis=1))\n\n# Euclidean (L2 projection)\nprint(\"sj_euclidean_low:\", sj.argmax(x, mode=\"euclidean\", softness=0.01,\n    axis=1))\nprint(\"sj_euclidean_high:\", sj.argmax(x, mode=\"euclidean\", softness=4.0,\n    axis=1))\n</code></pre> <pre><code>jnp: [0 1]\nsj_hard: [[1. 0. 0.]\n          [0. 1. 0.]]\nsj_entropic_low: [[1.00000000e+000 1.38389653e-087 3.72007598e-044]\n                  [7.12457641e-218 1.00000000e+000 3.72007598e-044]]\nsj_entropic_high: [[0.66524096 0.09003057 0.24472847]\n                   [0.00490169 0.72747516 0.26762315]]\nsj_euclidean_low: [[1. 0. 0.]\n                   [0. 1. 0.]]\nsj_euclidean_high: [[0.58333333 0.08333333 0.33333333]\n                    [0.         0.625      0.375     ]]\n</code></pre>"},{"location":"api/soft_indices/#softjax.argmin","title":"<code>softjax.argmin(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic') -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.argmin of <code>x</code> along the specified axis. Implemented as <code>softjax.argmax</code> on <code>-x</code>, see respective documentation for details.</p>"},{"location":"api/soft_indices/#softjax.argsort","title":"<code>softjax.argsort(x: jax.Array, axis: int | None = None, descending: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.argsort of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the argsort operation. If None, the input     Array is flattened before computing the argsort. Defaults to None.</li> <li><code>descending</code>: If True, sorts in descending order. Defaults to False (ascending).</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the type of softening:<ul> <li><code>mode=\"hard\"</code>: Returns the result of jnp.argsort with a one-hot encoding of     the indices.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations) as in     Differentiable Ranks and Sorting using Optimal Transport.     Intuition: The sorted elements are selected by specifying n \"anchors\"     and then transporting the ith-largest value to the ith-largest anchor.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via LBFGS projection onto Birkhoff polytope) as     in Fast Differentiable Sorting and Ranking.</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: Uses the \"SoftSort\" operator proposed in     SoftSort: A Continuous Relaxation for the argsort Operator.     This initializes the cost matrix based on the absolute difference of <code>x</code> to     the sorted values and then applies a single row normalization (instead of     full Sinkhorn in OT).     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax. To the best of our     knowledge this variant is novel.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <p>A SoftIndex of shape (..., n, ..., [n]) (positive Array which sums to 1 over the last dimension). The elements in (..., i, ..., [n]) represent a distribution over values in x for the ith smallest element along the specified axis.</p> <p>Computing the expectation</p> <p>Computing the soft sorted values means taking the expectation of <code>x</code> under the SoftIndex distribution. Similar to how with normal indices you would do     <pre><code>sorted_x = jnp.take_along_axis(x, indices, axis=axis)\n</code></pre> we offer the equivalent soft version via     <pre><code>soft_sorted_x = sj.take_along_axis(x, soft_indices, axis=axis)\n</code></pre> This is what is done in <code>softjax.sort</code>.</p>"},{"location":"api/soft_indices/#softjax.argtop_k","title":"<code>softjax.argtop_k(x: jax.Array, k: int, axis: int = -1, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes the soft argtop_k operation of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>k</code>: The number of top elements to select.</li> <li><code>axis</code>: The axis along which to compute the top_k operation. Defaults to -1.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the type of softening:<ul> <li><code>mode=\"hard\"</code>: Returns the result of jax.lax.top_k with a one-hot encoding of     the indices.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations) as in     Differentiable Top-k with Optimal Transport.     Intuition: The top-k elements are selected by specifying k+1 \"anchors\"     and then transporting the top_k values to the top k anchors, and the     remaining (n-k) values to the last anchor.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via projection onto Birkhoff polytope).     This version combines the approaches in Fast Differentiable Sorting and Ranking     (L2 regularizer for sorting) and Differentiable Top-k with Optimal Transport     (entropic regularizer for top-k).</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: Uses the \"SoftSort\" operator proposed  in     SoftSort: A Continuous Relaxation for the argsort Operator.     This initializes the cost matrix based on the absolute difference of <code>x</code> to     the sorted values and then applies a single row normalization (instead of     full Sinkhorn in OT).     Because this is very fast we do a full soft argsort and then take the top-k     elements.     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax. To the best of our     knowledge this variant is novel.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <p>A SoftIndex of shape (..., k, ..., [n]) (positive Array which sums to 1 over the last dimension). The elements in (..., i, ..., [n]) represent a distribution over values in x for the ith largest element along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.ranking","title":"<code>softjax.ranking(x: jax.Array, axis: int | None = None, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000, descending: bool = True) -&gt; jax.Array</code> <code></code>","text":"<p>Computes the soft rankings of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the ranking operation. If None, the input     Array is flattened before computing the ranking. Defaults to None.</li> <li><code>descending</code>: If True, larger inputs receive smaller ranks (best rank is 0). If     False, ranks increase with the input values.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the behavior of the ranking     operation:<ul> <li><code>mode=\"hard\"</code>: Returns ranking computed as two jnp.argsort calls.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations) as in     Differentiable Ranks and Sorting using Optimal Transport.     Intuition: We can use the transportation plan obtained in soft sorting for     ranking by transporting the sorted ranks (0, 1, ..., n-1) back to the     ranks of the original values.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via projection onto Birkhoff polytope) as in     Fast Differentiable Sorting and Ranking.</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: Uses an adaptation of the \"SoftSort\"     operator proposed in SoftSort: A Continuous Relaxation for the argsort Operator.     This initializes the cost matrix based on the absolute difference of <code>x</code> to     the sorted values and then we crucially apply a single column     normalization (instead of of row normalization in the original paper).     This makes the resulting matrix a unimodal column stochastic matrix which is     better suited for soft ranking.     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax. To the best of our     knowledge this variant is novel.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <p>A positive Array of shape (..., n, ...) with values in [0, n-1]. The elements in (..., i, ...) represent the soft rank of the ith element along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.argmedian","title":"<code>softjax.argmedian(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000) -&gt; Float[Array, '...']</code> <code></code>","text":"<p>Computes the soft argmedian of <code>x</code> along the specified axis.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>axis</code>: The axis along which to compute the median. If None, the input Array is     flattened before computing the median. Defaults to None.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}.</li> <li><code>softness</code>: Softness of the function, should be larger than zero. Defaults to 1.</li> <li><code>mode</code> and <code>fast</code>: These two arguments control the behavior of the median:<ul> <li><code>mode=\"hard\"</code>: Returns the result of jnp.median with a one-hot encoding of     the indices. On ties, it returns a uniform distribution over all median     indices.</li> <li><code>fast=False</code> and <code>mode=\"entropic\"</code>: Uses entropy-regularized optimal     transport (implemented via Sinkhorn iterations).     We adapt the approach in Differentiable Ranks and Sorting     using Optimal Transport and     Differentiable Top-k with Optimal Transport     to the median operation by carefully adjusting the cost matrix and     marginals.     Intuition: There are three \"anchors\", the median is transported onto one     anchor, and all the larger and smaller elements are transported to the other     two anchors, respectively.     Can be slow for large <code>max_iter</code>.</li> <li><code>fast=False</code> and <code>mode=\"euclidean\"</code>: Similar to entropic case, but using an     L2-regularizer (implemented via projection onto Birkhoff polytope).</li> <li><code>fast=True</code> and <code>mode=\"entropic\"</code>: This formulation a well-known soft median     operation based on the interpretation of the median as the minimizer of     absolute deviations. The softening is then achieved by replacing the argmax     operator with a softmax. Note, that this also has close ties to the     \"SoftSort\" operator from SoftSort: A Continuous Relaxation for the argsort Operator.     Note: Fast mode introduces gradient discontinuities when elements in <code>x</code> are     not unique, but is much faster.</li> <li><code>fast=True</code> and <code>mode=\"euclidean\"</code>: Similar to entropic fast case, but using     a euclidean unit-simplex projection instead of softmax.</li> </ul> </li> <li><code>max_iter</code>: Maximum number of iterations for the Sinkhorn algorithm if <code>mode</code> is     \"entropic\", or for the projection onto the Birkhoff polytope if     <code>mode</code> is \"euclidean\". Unused if <code>fast=True</code>.</li> </ul> <p>Returns:</p> <p>A SoftIndex of shape (..., {1}, ..., [n]) (positive Array which sums to 1 over the last dimension). The elements in (..., 0, ...) represent a distribution over values in x being the median along the specified axis.</p>"},{"location":"api/soft_indices/#selection-with-softindices","title":"Selection with SoftIndices","text":""},{"location":"api/soft_indices/#softjax.take_along_axis","title":"<code>softjax.take_along_axis(x: jax.Array, soft_indices: Float[Array, '...'], axis: int = -1) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.take_along_axis via a weighted dot product.</p> <p>Relation to <code>jnp.take_along_axis</code></p> <pre><code>x = jnp.array([[1, 2, 3], [4, 5, 6]])\n\nindices = jnp.array([[0, 2], [1, 0]])\nprint(jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = jax.nn.one_hot(indices, x.shape[1])\nprint(sj.take_along_axis(x, indices_onehot, axis=1))\n</code></pre> <pre><code>[[1. 3.]\n [5. 4.]]\n[[1. 3.]\n [5. 4.]]\n</code></pre> Interaction with <code>softjax.argmax</code> <pre><code>x = jnp.array([[5, 3, 4], [2, 7, 6]])\n\nindices = jnp.argmin(x, axis=1, keepdims=True)\nprint(\"argmin_jnp:\", jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = sj.argmin(x, axis=1, mode=\"hard\", keepdims=True)\nprint(\"argmin_val_onehot:\", sj.take_along_axis(x, indices_onehot, axis=1))\n\nindices_soft = sj.argmin(x, axis=1, mode=\"entropic\", softness=1.0,\n    keepdims=True)\nprint(\"argmin_val_soft:\", sj.take_along_axis(x, indices_soft, axis=1))\n</code></pre> <pre><code>argmin_jnp: [[3]\n             [2]]\nargmin_val_onehot: [[3.]\n                    [2.]]\nargmin_val_soft: [[3.42478962]\n                  [2.10433824]]\n</code></pre> Interaction with <code>softjax.argsort</code> <pre><code>x = jnp.array([[5, 3, 4], [2, 7, 6]])\n\nindices = jnp.argsort(x, axis=1)\nprint(\"sorted_jnp:\", jnp.take_along_axis(x, indices, axis=1))\n\nindices_onehot = sj.argsort(x, axis=1, mode=\"hard\")\nprint(\"sorted_sj_hard:\", sj.take_along_axis(x, indices_onehot, axis=1))\n\nindices_soft = sj.argsort(x, axis=1, mode=\"entropic\", softness=1.0)\nprint(\"sorted_sj_soft:\", sj.take_along_axis(x, indices_soft, axis=1))\n</code></pre> <pre><code>sorted_jnp: [[3 4 5]\n             [2 6 7]]\nsorted_sj_hard: [[3. 4. 5.]\n                 [2. 6. 7.]]\nsorted_sj_soft: [[3.2918137  4.         4.7081863 ]\n                 [2.00000045 6.26894107 6.73105858]]\n</code></pre> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_indices</code>: A SoftIndex of shape (..., k, ..., [n]) (positive Array which     sums to 1 over the last dimension).</li> <li><code>axis</code>: Axis along which to apply the soft index. Defaults to -1.</li> </ul> <p>Returns:</p> <p>Array of shape (..., k, ...), representing the result after soft selection along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.take","title":"<code>softjax.take(x: jax.Array, soft_indices: Float[Array, '...'], axis: int | None = None) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.take via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_indices</code>: A SoftIndex of shape (k, [n]) (positive Array which     sums to 1 over the last dimension).</li> <li><code>axis</code>: Axis along which to apply the soft index. If None, the input is     flattened. Defaults to None.</li> </ul> <p>Returns:</p> <p>Array of shape (..., k, ...) after soft selection.</p>"},{"location":"api/soft_indices/#softjax.choose","title":"<code>softjax.choose(soft_indices: Float[Array, '...'], choices: jax.Array) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.choose via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>soft_indices</code>: A SoftIndex of shape (..., [n]) (positive Array which     sums to 1 over the last dimension). Represents the weights for each choice.</li> <li><code>choices</code>: Array of shape (n, ...) supplying the values to mix.</li> </ul> <p>Returns:</p> <p>Array of shape (..., ...) after softly selecting among <code>choices</code>.</p>"},{"location":"api/soft_indices/#softjax.dynamic_index_in_dim","title":"<code>softjax.dynamic_index_in_dim(x: jax.Array, soft_index: Float[Array, '...'], axis: int = 0, keepdims: bool = True) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.lax.dynamic_index_in_dim via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_indices</code>: A SoftIndex of shape ([n],) (positive Array which     sums to 1 over the last dimension).</li> <li><code>axis</code>: Axis along which to apply the soft index. Defaults to 0.</li> <li><code>keepdims</code>: If True, keeps the reduced dimension as a singleton {1}.</li> </ul> <p>Returns:</p> <p>Array after soft indexing, shape (..., {1}, ...).</p>"},{"location":"api/soft_indices/#softjax.dynamic_slice_in_dim","title":"<code>softjax.dynamic_slice_in_dim(x: jax.Array, soft_start_index: Float[Array, '...'], slice_size: int, axis: int = 0) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.lax.dynamic_slice_in_dim via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (..., n, ...).</li> <li><code>soft_indices</code>: A SoftIndex of shape ([n],) (positive Array which     sums to 1 over the last dimension).</li> <li><code>slice_size</code>: Length of the slice to extract.</li> <li><code>axis</code>: Axis along which to apply the soft slice. Defaults to 0.</li> </ul> <p>Returns:</p> <p>Array of shape (..., slice_size, ...) after soft slicing.</p>"},{"location":"api/soft_indices/#softjax.dynamic_slice","title":"<code>softjax.dynamic_slice(x: jax.Array, soft_start_indices: Sequence[Float[Array, '...']], slice_sizes: Sequence[int]) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.lax.dynamic_slice via a weighted dot product.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Input Array of shape (n_1, n_2, ..., n_k).</li> <li><code>soft_start_indices</code>: A list of SoftIndices of shape ([n_i],) (positive Arrays     which sums to 1). Sequence of SoftIndex distributions of shapes     ([n_1],), ([n_2],), ..., ([n_k]) each summing to 1.</li> <li><code>slice_sizes</code>: Sequence of slice lengths for each dimension.</li> </ul> <p>Returns:</p> <p>Array of shape (l_1, l_2, ..., l_k) after soft slicing.</p>"},{"location":"api/soft_indices/#examples-of-selection","title":"Examples of Selection","text":""},{"location":"api/soft_indices/#softjax.max","title":"<code>softjax.max(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic') -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.max of <code>x</code> along the specified axis. Implemented as <code>softjax.argmax</code> followed by <code>softjax.take_along_axis</code>, see respective documentations for details.</p> <p>Returns:</p> <p>Array of shape (..., {1}, ...) representing the soft maximum of <code>x</code> along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.min","title":"<code>softjax.min(x: jax.Array, axis: int | None = None, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', keepdims: bool = False) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.min of <code>x</code> along the specified axis. Implemented as -<code>softjax.max</code> on <code>-x</code>, see respective documentation for details.</p> <p>Returns:</p> <p>Array of shape (..., {1}, ...) representing the soft minimum of <code>x</code> along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.median","title":"<code>softjax.median(x: jax.Array, axis: int | None = None, keepdims: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jnp.median of <code>x</code> along the specified axis. Implemented as <code>softjax.argmedian</code> followed by <code>softjax.take_along_axis</code>, see respective documentations for details.</p> <p>Returns:</p> <p>An Array of shape (..., {1}, ...), representing the soft median values along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.sort","title":"<code>softjax.sort(x: jax.Array, axis: int | None = None, descending: bool = False, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000) -&gt; jax.Array</code> <code></code>","text":"<p>Performs a soft version of jax.numpy.sort of <code>x</code> along the specified axis. Implemented as <code>softjax.argsort</code> followed by <code>softjax.take_along_axis</code>, see respective documentations for details.</p> <p>Returns:</p> <p>Array of shape (..., n, ...) representing the soft sorted values of <code>x</code> along the specified axis.</p>"},{"location":"api/soft_indices/#softjax.top_k","title":"<code>softjax.top_k(x: jax.Array, k: int, axis: int = -1, softness: float = 1.0, mode: Literal['hard', 'entropic', 'euclidean'] = 'entropic', fast: bool = True, max_iter: int = 1000) -&gt; tuple[jax.Array, Float[Array, '...']]</code> <code></code>","text":"<p>Performs a soft version of jax.lax.top_k of <code>x</code> along the specified axis. Implemented as <code>softjax.argtop_k</code> followed by <code>softjax.take_along_axis</code>, see respective documentations for details.</p> <p>Returns:</p> <ul> <li><code>soft_values</code>: Top-k values of <code>x</code>, shape (..., k, ...).</li> <li><code>soft_indices</code>: SoftIndex of shape (..., k, ..., [n]) (positive Array which sums     to 1 over the last dimension). Represents the soft indices of the top-k values.</li> </ul>"},{"location":"api/straight_through/","title":"Straight-through","text":""},{"location":"api/straight_through/#softjax.grad_replace","title":"<code>softjax.grad_replace(fn: typing.Callable) -&gt; typing.Callable</code> <code></code>","text":"<p>This decorator calls the decorated function twice: once with <code>forward=True</code> and once with <code>forward=False</code>. It returns the output from the forward pass, but uses the output from the backward pass to compute gradients.</p> <p>Arguments:</p> <ul> <li><code>fn</code>: The function to be wrapped. It should accept a <code>forward</code> argument     that specifies which computation to perform depending on forward or     backward pass.</li> </ul> <p>Returns:     A wrapped function that behaves like the <code>forward=True</code> version during the     forward pass, but computes gradients using the <code>forward=False</code> version     during the backward pass.</p>"},{"location":"api/straight_through/#softjax.st","title":"<code>softjax.st(fn: typing.Callable) -&gt; typing.Callable</code> <code></code>","text":"<p>This decorator calls the decorated function twice: once with <code>mode=\"hard\"</code> and once with the specified <code>mode</code>. It returns the output from the hard forward pass, but uses the output from the soft backward pass to compute gradients.</p> <p>Arguments:</p> <ul> <li><code>fn</code>: The function to be wrapped. It should accept a <code>mode</code> argument.</li> </ul> <p>Returns:     A wrapped function that behaves like the <code>mode=\"hard\"</code> version during the     forward pass, but computes gradients using the specified <code>mode</code> and <code>softness</code>     during the backward pass.</p>"},{"location":"api/straight_through/#softjax.argmax_st","title":"<code>softjax.argmax_st(*args, **kwargs)</code> <code></code>","text":"<p>Straight-through version of <code>softjax.argmax</code>.</p> <p>This function returns the hard <code>argmax</code> during the forward pass, but uses a soft relaxation (controlled by the <code>mode</code> argument) for the backward pass (i.e., gradients are computed through the soft version).</p> <p>Implemented using the <code>softjax.st</code> decorator as <code>st(softjax.argmax)</code>.</p>"},{"location":"api/straight_through/#softjax.argmin_st","title":"<code>softjax.argmin_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.argsort_st","title":"<code>softjax.argsort_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.clip_st","title":"<code>softjax.clip_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.equal_st","title":"<code>softjax.equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.greater_st","title":"<code>softjax.greater_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.greater_equal_st","title":"<code>softjax.greater_equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.heaviside_st","title":"<code>softjax.heaviside_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.isclose_st","title":"<code>softjax.isclose_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.less_st","title":"<code>softjax.less_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.less_equal_st","title":"<code>softjax.less_equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.max_st","title":"<code>softjax.max_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.median_st","title":"<code>softjax.median_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.median_newton_st","title":"<code>softjax.median_newton_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.min_st","title":"<code>softjax.min_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.not_equal_st","title":"<code>softjax.not_equal_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.ranking_st","title":"<code>softjax.ranking_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.relu_st","title":"<code>softjax.relu_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.round_st","title":"<code>softjax.round_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.sign_st","title":"<code>softjax.sign_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.sort_st","title":"<code>softjax.sort_st(*args, **kwargs)</code> <code></code>","text":""},{"location":"api/straight_through/#softjax.top_k_st","title":"<code>softjax.top_k_st(*args, **kwargs)</code> <code></code>","text":""}]}